{
  "classification": {
    "primaryType": "API/Library",
    "confidence": 1.0,
    "characteristics": [
      "Concurrent Task Execution",
      "Generic Resource Management",
      "Rate-Based Dynamic Worker Scaling",
      "Priority Queues",
      "Resource Pooling",
      "Customizable Health Checks & Retries",
      "Graceful & Immediate Shutdown",
      "Real-time Performance Metrics",
      "Custom Logging"
    ],
    "adaptations": [
      "Focus on function signatures and usage patterns for library consumption.",
      "Emphasize resource lifecycle management for diverse external dependencies.",
      "Highlight configuration for dynamic scaling and error recovery.",
      "Provide Go-specific examples and import paths."
    ]
  },
  "introduction": "Tasker is a powerful and flexible Go library designed for efficient management of concurrent tasks. It provides a highly customizable worker pool, dynamic scaling (bursting), priority queuing, and robust resource lifecycle management. This makes it an ideal solution for processing background jobs, handling I/O-bound operations, or managing CPU-intensive computations with controlled concurrency. Tasker abstracts away the complexities of goroutine management, worker pools, and resource lifecycles, allowing developers to define tasks that operate on specific resources and queue them for execution while the library handles the underlying concurrency, scaling, and error recovery.",
  "sections": [
    {
      "title": "Getting Started",
      "path": "getting-started/overview-core-concepts.md",
      "content": "### Overview and Core Concepts\nTasker operates around several core concepts that enable its powerful concurrency features:\n\n*   **Resource (`R`)**: This is a generic type representing any dependency or object your tasks require for execution (e.g., a database connection, an HTTP client, a custom compute unit). Tasker manages the creation and destruction of these resources.\n*   **Task Result (`E`)**: A generic type for the value returned by a task upon successful completion. This ensures type safety for diverse task outputs.\n*   **Task Function**: Your application's specific logic, defined as a `func(resource R) (result E, err error)`. This function is executed by a worker, receiving an `R` resource instance.\n*   **`tasker.Config[R]`**: The configuration struct used to initialize the `TaskManager`. It defines resource lifecycle functions (`OnCreate`, `OnDestroy`), worker counts, scaling parameters, health check logic, and optional custom logging/metrics.\n*   **`tasker.Manager[R, E]`**: The concrete implementation of the `TaskManager[R, E]` interface. It's the central orchestrator for task management, handling queues, worker synchronization, and resource lifecycles.\n*   **`Task[R, E]` (Internal)**: An internal struct encapsulating a task's executable function, channels for its result and errors, a retry counter, and a timestamp for when it was queued.\n*   **`TaskStats`**: A snapshot struct providing real-time operational statistics of the `TaskManager`, including worker counts and queued tasks.\n*   **`TaskMetrics`**: A comprehensive struct offering aggregated performance metrics such as task arrival/completion rates, execution time percentiles (P95, P99), and success/failure rates.\n*   **`Logger` Interface**: An interface allowing integration with custom logging solutions.\n*   **`MetricsCollector` Interface**: An interface for integrating with external monitoring and observability systems for detailed performance metrics.",
      "agentGuidance": {
        "decisionPoints": [],
        "verificationSteps": [],
        "quickPatterns": [],
        "diagnosticPaths": []
      }
    },
    {
      "title": "Getting Started",
      "path": "getting-started/quick-setup-guide.md",
      "content": "### Quick Setup Guide\n\nTo begin using Tasker, ensure you have Go 1.24.3 or higher installed. Then, follow these steps to add it to your project and run a basic example.\n\n#### Prerequisites\n* Go 1.24.3 or higher\n\n#### Installation\n```bash\ngo get github.com/asaidimu/tasker\n```\n\n#### Basic Example: Simple Calculator\nThis example demonstrates how to set up a `TaskManager` with a `CalculatorResource` and queue basic arithmetic tasks.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype CalculatorResource struct{}\n\nfunc createCalcResource() (*CalculatorResource, error) {\n\tfmt.Println(\"INFO: Creating CalculatorResource\")\n\treturn &CalculatorResource{}, nil\n}\n\nfunc destroyCalcResource(r *CalculatorResource) error {\n\tfmt.Println(\"INFO: Destroying CalculatorResource\")\n\treturn nil\n}\n\nfunc main() {\n\tctx := context.Background()\n\n\tconfig := tasker.Config[*CalculatorResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 2,\n\t\tCtx:         ctx,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\tfmt.Println(\"Queuing a simple addition task...\")\n\tgo func() {\n\t\tsum, err := manager.QueueTask(func(r *CalculatorResource) (int, error) {\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\ta, b := 10, 25\n\t\t\tfmt.Printf(\"Worker processing: %d + %d\\n\", a, b)\n\t\t\treturn a + b, nil\n\t\t})\n\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Task 1 failed: %v\\n\", err)\n\t\t} else {\n\t\t\tfmt.Printf(\"Task 1 (Addition) Result: %d\\n\", sum)\n\t\t}\n\t}()\n\n\ttime.Sleep(500 * time.Millisecond)\n\n\tstats := manager.Stats()\n\tfmt.Printf(\"\\n--- Current Stats ---\\n\")\n\tfmt.Printf(\"Active Workers: %d\\n\", stats.ActiveWorkers)\n\tfmt.Printf(\"Queued Tasks: %d\\n\", stats.QueuedTasks)\n\tfmt.Printf(\"Available Resources: %d\\n\", stats.AvailableResources)\n\tfmt.Println(\"----------------------\")\n}\n```",
      "agentGuidance": {
        "decisionPoints": [],
        "verificationSteps": [
          "Check: `go get github.com/asaidimu/tasker` executed successfully → Expected: Package installed, no errors.",
          "Check: `go run main.go` produces output similar to example → Expected: \"INFO: Creating CalculatorResource\" and task results are printed."
        ],
        "quickPatterns": [
          "Pattern: Basic TaskManager Initialization\n```go\n// Define your resource type\ntype MyResource struct{}\n\n// Implement onCreate and onDestroy functions\nfunc createMyResource() (*MyResource, error) { /* ... */ }\nfunc destroyMyResource(r *MyResource) error { /* ... */ }\n\n// Configure and create TaskManager\nconfig := tasker.Config[*MyResource]{\n    OnCreate: createMyResource,\n    OnDestroy: destroyMyResource,\n    WorkerCount: 2,\n    Ctx: context.Background(),\n}\nmanager, err := tasker.NewTaskManager[*MyResource, any](config)\nif err != nil { log.Fatal(err) }\ndefer manager.Stop()\n```"
        ],
        "diagnosticPaths": [
          "Error `Error creating task manager: worker count must be positive` -> Symptom: Manager creation fails with configuration error -> Check: Verify `Config.WorkerCount` is > 0 -> Fix: Set `WorkerCount` to a positive integer.",
          "Error `Error creating task manager: onCreate function is required` -> Symptom: Manager creation fails due to missing resource creator -> Check: Ensure `Config.OnCreate` is assigned a non-nil function -> Fix: Provide a valid `OnCreate` function."
        ]
      }
    },
    {
      "title": "Core Operations",
      "path": "core-operations/task-execution-and-queues.md",
      "content": "### Task Execution and Queues\nTasker provides flexible mechanisms for submitting and executing tasks, catering to different concurrency and priority needs.\n\n#### `NewTaskManager` Initialization\n`tasker.NewTaskManager[R, E](config Config[R])` is the primary constructor. It takes a `Config` struct that dictates how the `TaskManager` behaves. The `R` generic type specifies the resource type, and `E` specifies the expected return type of your tasks.\n\n#### Task Submission Methods\n\n*   **`QueueTask(task func(R) (E, error)) (E, error)`**\n    Adds a task to the standard queue for asynchronous execution. The call blocks until the task completes and returns its result or error. Suitable for general background processing.\n\n*   **`QueueTaskWithPriority(task func(R) (E, error)) (E, error)`**\n    Adds a task to a dedicated high-priority queue. Tasks in this queue are processed before tasks in the main queue, ensuring faster execution for critical operations. This call also blocks until completion.\n\n*   **`QueueTaskOnce(task func(R) (E, error)) (E, error)`**\n    Similar to `QueueTask`, but with an important distinction: if the task fails and `CheckHealth` indicates an unhealthy worker, this specific task will *not* be re-queued by Tasker's internal retry mechanism. Use for non-idempotent operations where \"at-most-once\" processing by the `TaskManager` is desired.\n\n*   **`QueueTaskWithPriorityOnce(task func(R) (E, error)) (E, error)`**\n    Combines the high-priority behavior of `QueueTaskWithPriority` with the \"at-most-once\" execution semantics of `QueueTaskOnce`.\n\n*   **`RunTask(task func(R) (E, error)) (E, error)`**\n    Executes a task immediately, bypassing the main and priority queues. It first attempts to acquire a resource from an internal pre-allocated pool. If no resource is immediately available in the pool, it temporarily creates a new one via your `OnCreate` function for the duration of the task. This is a synchronous call, blocking until the task finishes. Ideal for urgent, low-latency operations that should not be delayed by queuing.\n\n#### Worker Goroutine Flow\nWorkers continuously monitor the `priorityQueue` and `mainQueue`. When a task is available, a worker picks it up, executes the `run` function, handles any errors, and sends the result back to the caller. If `CheckHealth` indicates an issue, the worker may be replaced, and the task potentially re-queued.\n\n```go\n// Example of task submission (from examples/intermediate/main.go)\n// Queue a normal image resize task\ngo func() {\n    result, err := manager.QueueTask(func(proc *ImageProcessor) (string, error) {\n        fmt.Printf(\"Worker %d processing normal resize for imageA.jpg\\n\", proc.ID)\n        time.Sleep(150 * time.Millisecond)\n        // ... processing logic ...\n        return \"imageA_resized.jpg\", nil\n    })\n    // Handle result/error\n}()\n\n// Queue a high-priority thumbnail generation task\ngo func() {\n    result, err := manager.QueueTaskWithPriority(func(proc *ImageProcessor) (string, error) {\n        fmt.Printf(\"Worker %d processing HIGH PRIORITY thumbnail for video.mp4\\n\", proc.ID)\n        time.Sleep(50 * time.Millisecond)\n        return \"video_thumbnail.jpg\", nil\n    })\n    // Handle result/error\n}()\n\n// Run an immediate task\nimmediateResult, immediateErr := manager.RunTask(func(proc *ImageProcessor) (string, error) {\n    fmt.Printf(\"IMMEDIATE Task processing fast preview with processor %d\\n\", proc.ID)\n    time.Sleep(20 * time.Millisecond)\n    return \"fast_preview.jpg\", nil\n})\n// Handle result/error\n```",
      "agentGuidance": {
        "decisionPoints": [
          "IF [task is time-sensitive or critical] THEN [use `QueueTaskWithPriority` or `RunTask`] ELSE [use `QueueTask`]",
          "IF [task must execute immediately and block caller] THEN [use `RunTask`] ELSE [use `QueueTask` or `QueueTaskWithPriority`]",
          "IF [task is not idempotent and should not be retried by the manager on health failure] THEN [use `QueueTaskOnce` or `QueueTaskWithPriorityOnce`] ELSE [use `QueueTask` or `QueueTaskWithPriority`]"
        ],
        "verificationSteps": [
          "Check: `QueueTask` call blocks until completion → Expected: Return value or error is received after task function finishes.",
          "Check: `RunTask` executes immediately without queueing → Expected: Resource acquisition and execution begin synchronously.",
          "Check: High-priority tasks are processed before normal tasks → Expected: `QueueTaskWithPriority` results appear faster than `QueueTask` results under load."
        ],
        "quickPatterns": [
          "Pattern: Queue a standard task\n```go\nresult, err := manager.QueueTask(func(res *MyResource) (string, error) {\n    // Task logic here\n    return \"Success\", nil\n})\nif err != nil { /* handle error */ }\n```",
          "Pattern: Queue a high-priority task\n```go\nresult, err := manager.QueueTaskWithPriority(func(res *MyResource) (string, error) {\n    // Priority task logic here\n    return \"High Priority Success\", nil\n})\nif err != nil { /* handle error */ }\n```",
          "Pattern: Execute an immediate task\n```go\nresult, err := manager.RunTask(func(res *MyResource) (string, error) {\n    // Immediate task logic here\n    return \"Immediate Success\", nil\n})\nif err != nil { /* handle error */ }\n```"
        ],
        "diagnosticPaths": [
          "Error `task manager is shutting down` -> Symptom: `QueueTask` or `RunTask` returns an error immediately -> Check: Verify the `TaskManager` has not been stopped or killed -> Fix: Only submit tasks while `TaskManager` is active.",
          "Error `failed to create temporary resource` -> Symptom: `RunTask` fails without executing the task function -> Check: Review `Config.OnCreate` for errors or blocking operations -> Fix: Ensure `OnCreate` is reliable and non-blocking."
        ]
      }
    },
    {
      "title": "Core Operations",
      "path": "core-operations/resource-management-and-health-checks.md",
      "content": "### Resource Management and Health Checks\nTasker provides robust mechanisms for managing the lifecycle of your task resources and reacting to their health status.\n\n#### `OnCreate func() (R, error)`\nThis function is crucial for initializing and preparing your resources. It's called when a worker starts up or when `RunTask` needs a temporary resource. `OnCreate` should establish connections, load configurations, or perform any setup necessary for your resource `R` to be operational.\n\n#### `OnDestroy func(R) error`\nThis function handles the cleanup and deallocation of your resources. It's called when a worker shuts down, a temporary `RunTask` resource is no longer needed, or the entire `TaskManager` is stopping/killing. `OnDestroy` should close connections, release memory, or perform any finalization to prevent resource leaks.\n\n#### `CheckHealth func(error) bool`\nAn optional but highly recommended function for defining custom logic to determine if an error returned by a task indicates an \"unhealthy\" state for the worker or its associated resource. If `CheckHealth` returns `false` (meaning unhealthy):\n\n1.  The worker processing that task is considered faulty and will be shut down.\n2.  Its resource will be destroyed via `OnDestroy`.\n3.  A new worker will be created to replace the unhealthy one.\n4.  The original task that caused the unhealthy error will be re-queued (up to `MaxRetries` times) to be processed by a newly healthy worker.\n\nIf `CheckHealth` returns `true` (or if the function is `nil`), the error is considered a task-specific failure that does not impact worker health, and the worker continues operating.\n\n```go\n// Example: Image Processing with custom health check (from examples/intermediate/main.go)\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype ImageProcessor struct {\n\tID        int\n\tIsHealthy bool\n}\n\nfunc createImageProcessor() (*ImageProcessor, error) {\n\tid := rand.Intn(1000)\n\tfmt.Printf(\"INFO: Creating ImageProcessor %d\\n\", id)\n\treturn &ImageProcessor{ID: id, IsHealthy: true}, nil\n}\n\nfunc destroyImageProcessor(p *ImageProcessor) error {\n\tfmt.Printf(\"INFO: Destroying ImageProcessor %d\\n\", p.ID)\n\treturn nil\n}\n\nfunc checkImageProcessorHealth(err error) bool {\n\tif err != nil && err.Error() == \"processor_crash\" {\n\t\tfmt.Printf(\"WARN: Detected unhealthy error: %v. Worker will be replaced.\\n\", err)\n\t\treturn false // This error indicates an unhealthy state\n\t}\n\treturn true // Other errors are just task failures, not worker health issues\n}\n\nfunc main() {\n\tctx := context.Background()\n\n\tconfig := tasker.Config[*ImageProcessor]{\n\t\tOnCreate:    createImageProcessor,\n\t\tOnDestroy:   destroyImageProcessor,\n\t\tWorkerCount: 2,\n\t\tCtx:         ctx,\n\t\tCheckHealth: checkImageProcessorHealth, // Custom health check applied here\n\t\tMaxRetries:  1,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*ImageProcessor, string](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\t// Simulate a task that might crash a worker\n\tgo func() {\n\t\t_, err := manager.QueueTask(func(proc *ImageProcessor) (string, error) {\n\t\t\tif rand.Intn(2) == 0 { // 50% chance to simulate a crash\n\t\t\t\treturn \"\", errors.New(\"processor_crash\") // Triggers CheckHealth to return false\n\t\t\t}\n\t\t\treturn \"processed\", nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"Problematic task failed: %v\\n\", err) }\n\t}()\n\n\ttime.Sleep(1 * time.Second) // Allow time for potential worker replacement\n}\n```",
      "agentGuidance": {
        "decisionPoints": [
          "IF [task failure indicates resource/worker malfunction (e.g., connection lost)] THEN [implement `CheckHealth` to return `false` for that error] ELSE [allow `CheckHealth` to return `true` (default behavior)]"
        ],
        "verificationSteps": [
          "Check: `OnCreate` is called upon worker startup or `RunTask` with empty pool → Expected: Resource initialization logs appear.",
          "Check: `OnDestroy` is called upon worker shutdown or `RunTask` resource disposal → Expected: Resource cleanup logs appear.",
          "Check: Task fails with error that `CheckHealth` marks as unhealthy → Expected: Worker is replaced, `OnDestroy` and `OnCreate` are called again for the replacement worker, and the task might be retried."
        ],
        "quickPatterns": [
          "Pattern: Basic `OnCreate`/`OnDestroy`\n```go\nfunc onCreateDB() (*sql.DB, error) {\n    db, err := sql.Open(\"postgres\", \"conn_str\")\n    // Handle err, ping db, etc.\n    return db, err\n}\n\nfunc onDestroyDB(db *sql.DB) error {\n    return db.Close()\n}\n```",
          "Pattern: Custom `CheckHealth` for specific errors\n```go\nfunc checkDBHealth(err error) bool {\n    if err != nil && strings.Contains(err.Error(), \"connection refused\") {\n        return false // Unhealthy, worker should be replaced\n    }\n    return true // Other errors are just task failures\n}\n```"
        ],
        "diagnosticPaths": [
          "Error `Worker failed to create resource: [details]` -> Symptom: Workers fail to start -> Check: Debug `Config.OnCreate` function for logic errors, network issues, or invalid credentials -> Fix: Correct `OnCreate` implementation, ensure external dependencies are reachable.",
          "Error `Error destroying resource: [details]` -> Symptom: Resource leaks or warnings on worker exit -> Check: Debug `Config.OnDestroy` function for cleanup failures -> Fix: Ensure `OnDestroy` properly releases all resource components (e.g., closing file descriptors, network connections).",
          "Error `Unhealthy worker is exiting.` -> Symptom: Workers are frequently created/destroyed (thrashing) -> Check: Review `CheckHealth` logic; ensure it only returns `false` for genuinely unhealthy, unrecoverable worker states, not transient task errors -> Fix: Refine `CheckHealth` to be more precise."
        ]
      }
    },
    {
      "title": "Core Operations",
      "path": "core-operations/shutdown-procedures.md",
      "content": "### Shutdown Procedures\nTasker provides two distinct methods for shutting down the `TaskManager`, offering control over how currently executing and queued tasks are handled.\n\n#### `Stop() error`\nInitiates a graceful shutdown of the `TaskManager`. This is the recommended shutdown method for most applications.\n\n**Behavior:**\n1.  Stops accepting new tasks. Any subsequent calls to `QueueTask`, `RunTask`, etc., will immediately return an error.\n2.  Signals all existing workers to enter a \"drain\" mode. Workers will finish processing any tasks currently in their queues (`mainQueue` and `priorityQueue`) before exiting.\n3.  Waits for all active and draining workers to complete.\n4.  Releases all managed resources by calling `OnDestroy` for each.\n\n**When to Use:**\n*   When you need to ensure all outstanding tasks are completed before the application exits or the `TaskManager` is no longer needed.\n*   For controlled application shutdowns or during hot reloads where service continuity is important.\n\n#### `Kill() error`\nImmediately terminates the `TaskManager` without waiting for queued tasks to complete. This is useful for emergency shutdowns or when rapid termination is prioritized over task completion.\n\n**Behavior:**\n1.  Stops accepting new tasks, similar to `Stop()`.\n2.  Cancels the underlying `context.Context` that all worker goroutines are derived from, causing them to exit immediately without processing any more queued tasks.\n3.  Drops all tasks currently waiting in the `mainQueue` and `priorityQueue`.\n4.  Releases all managed resources by calling `OnDestroy` for each.\n\n**When to Use:**\n*   During application crashes or unrecoverable error states where immediate resource release is necessary.\n*   In testing scenarios where quick teardown is desired.\n\n```go\n// Example: Graceful vs. Immediate Shutdown (based on examples/advanced/main.go)\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype HeavyComputeResource struct { ID int }\nfunc createComputeResource() (*HeavyComputeResource, error) { /* ... */ return &HeavyComputeResource{ID: 1}, nil}\nfunc destroyComputeResource(r *HeavyComputeResource) error { /* ... */ return nil}\nfunc checkComputeHealth(err error) bool { return true }\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\n\tconfig := tasker.Config[*HeavyComputeResource]{\n\t\tOnCreate:    createComputeResource,\n\t\tOnDestroy:   destroyComputeResource,\n\t\tWorkerCount: 2,\n\t\tCtx:         ctx,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*HeavyComputeResource, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\n\tvar tasksSubmitted atomic.Int32\n\tfor i := 0; i < 5; i++ {\n\t\ttasksSubmitted.Add(1)\n\t\tgo func(taskID int) {\n\t\t\t_, err := manager.QueueTask(func(res *HeavyComputeResource) (string, error) {\n\t\t\t\tfmt.Printf(\"Worker %d processing Task %d\\n\", res.ID, taskID)\n\t\t\t\ttime.Sleep(200 * time.Millisecond) // Simulate work\n\t\t\t\treturn fmt.Sprintf(\"Task %d completed\", taskID), nil\n\t\t\t})\n\t\t\tif err != nil { fmt.Printf(\"Task %d failed: %v\\n\", taskID, err) }\n\t\t}(i)\n\t}\n\n\tfmt.Println(\"Allowing tasks to start...\")\n\ttime.Sleep(100 * time.Millisecond)\n\n\tfmt.Println(\"Initiating graceful shutdown...\")\n\terr = manager.Stop() // This will wait for all 5 tasks to finish\n\tif err != nil { fmt.Printf(\"Error during shutdown: %v\\n\", err) }\n\tfmt.Println(\"Task manager gracefully shut down.\")\n\n\t// For a 'Kill' example, replace manager.Stop() with manager.Kill()\n\t// and observe tasks being cancelled immediately.\n\t// manager.Kill()\n}\n```",
      "agentGuidance": {
        "decisionPoints": [
          "IF [all in-flight and queued tasks must complete] THEN [call `manager.Stop()`] ELSE [call `manager.Kill()` (to immediately terminate)]",
          "IF [application needs to shut down quickly regardless of task completion] THEN [call `manager.Kill()`] ELSE [call `manager.Stop()` (for graceful termination)]"
        ],
        "verificationSteps": [
          "Check: `manager.Stop()` called → Expected: All queued tasks are eventually processed and complete before `Stop()` returns.",
          "Check: `manager.Kill()` called → Expected: `Kill()` returns quickly, and tasks in queues are not processed, running tasks may be interrupted.",
          "Check: After shutdown, `Stats().ActiveWorkers` is 0 → Expected: All worker goroutines have exited and resources are released."
        ],
        "quickPatterns": [
          "Pattern: Graceful shutdown\n```go\nmanager, _ := tasker.NewTaskManager[MyResource, any](config)\ndefer manager.Stop() // Recommended for most applications\n// ... queue tasks ...\n```",
          "Pattern: Immediate shutdown (e.g., for testing or emergency)\n```go\nmanager, _ := tasker.NewTaskManager[MyResource, any](config)\n// ... queue tasks ...\nerr := manager.Kill() // Forceful termination\nif err != nil { log.Printf(\"Kill failed: %v\", err) }\n```"
        ],
        "diagnosticPaths": [
          "Error `manager.Stop() hangs indefinitely` -> Symptom: Application does not exit after calling `Stop()` -> Check: Ensure all task functions eventually complete or respond to `context.Context` cancellation -> Fix: Add context cancellation checks within long-running tasks or ensure `OnDestroy` is non-blocking.",
          "Error `tasks fail with 'task manager is shutting down' immediately after calling Stop()` -> Symptom: New tasks are rejected immediately -> Check: Confirm `Stop()` was intended to be called before new task submissions ceased -> Fix: Submit tasks only when `TaskManager` is in a running state."
        ]
      }
    },
    {
      "title": "Task-Based Guide",
      "path": "task-based-guide/asynchronous-background-jobs.md",
      "content": "### Asynchronous Background Jobs\n\n**Goal**: Execute tasks in the background without blocking the main application flow, ensuring they are processed as workers become available.\n\n**Approach**: Use `QueueTask` for standard, asynchronous processing. The call to `QueueTask` itself blocks until the task *completes* and its result/error is returned, making it easy to handle outcomes directly where the task is initiated. To truly run in the background, `QueueTask` calls are typically wrapped in a new goroutine.\n\n**Example: Simple Addition and Subtraction**\nThis example showcases how `QueueTask` can be used to offload computational work to a managed worker pool.\n\n```go\n// examples/basic/main.go (Simplified)\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype CalculatorResource struct{}\n\nfunc createCalcResource() (*CalculatorResource, error) {\n\treturn &CalculatorResource{}, nil\n}\n\nfunc destroyCalcResource(r *CalculatorResource) error {\n\treturn nil\n}\n\nfunc main() {\n\tconfig := tasker.Config[*CalculatorResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 2,\n\t\tCtx:         context.Background(),\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\t// Task 1: Addition\n\tgo func() {\n\t\tsum, err := manager.QueueTask(func(r *CalculatorResource) (int, error) {\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\treturn 10 + 25, nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"Addition task failed: %v\\n\", err) }\n\t\telse { fmt.Printf(\"Addition Result: %d\\n\", sum) }\n\t}()\n\n\t// Task 2: Subtraction\n\tgo func() {\n\t\tdiff, err := manager.QueueTask(func(r *CalculatorResource) (int, error) {\n\t\t\ttime.Sleep(70 * time.Millisecond)\n\t\t\treturn 100 - 40, nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"Subtraction task failed: %v\\n\", err) }\n\t\telse { fmt.Printf(\"Subtraction Result: %d\\n\", diff) }\n\t}()\n\n\ttime.Sleep(1 * time.Second) // Allow tasks to complete\n}\n```\n\n**Outcome**: The `main` function continues execution, while the addition and subtraction tasks are handled concurrently by the `TaskManager`'s worker pool. Results are printed as tasks complete.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [task does not require immediate blocking execution by the caller] THEN [wrap `QueueTask` call in a new goroutine] ELSE [consider `RunTask`]"
        ],
        "verificationSteps": [
          "Check: `QueueTask` returns a result/error only after the task function completes → Expected: `sum` and `diff` values are correctly calculated.",
          "Check: Multiple `QueueTask` calls are processed concurrently → Expected: Output shows workers handling tasks in parallel, not strictly sequentially from the main goroutine."
        ],
        "quickPatterns": [
          "Pattern: Queue task in a goroutine\n```go\ngo func() {\n    result, err := manager.QueueTask(func(res *MyResource) (string, error) {\n        // Your asynchronous task logic\n        return \"Async done\", nil\n    })\n    if err != nil { /* handle error */ }\n    else { /* process result */ }\n}()\n```"
        ],
        "diagnosticPaths": [
          "Error `QueueTask call blocks indefinitely` -> Symptom: The goroutine calling `QueueTask` stops responding -> Check: Verify the task function itself is not deadlocking or hanging, and that worker count is sufficient -> Fix: Ensure task function completes, increase `WorkerCount` if queues are persistently full."
        ]
      }
    },
    {
      "title": "Task-Based Guide",
      "path": "task-based-guide/high-priority-operations.md",
      "content": "### High-Priority Operations\n\n**Goal**: Ensure that critical tasks are processed with higher precedence than regular tasks, even under heavy load.\n\n**Approach**: Use `QueueTaskWithPriority`. Tasks submitted via this method are added to a dedicated priority queue, which workers check *before* the main task queue. This guarantees that urgent operations are picked up and executed sooner.\n\n**Example: Urgent Thumbnail Generation**\nIn an image processing system, generating a thumbnail for a newly uploaded video might be more urgent than resizing a static image. This example demonstrates how `QueueTaskWithPriority` facilitates this prioritization.\n\n```go\n// examples/intermediate/main.go (Simplified)\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype ImageProcessor struct { ID int }\nfunc createImageProcessor() (*ImageProcessor, error) { return &ImageProcessor{ID: 1}, nil }\nfunc destroyImageProcessor(p *ImageProcessor) error { return nil }\nfunc checkImageProcessorHealth(err error) bool { return true }\n\nfunc main() {\n\tconfig := tasker.Config[*ImageProcessor]{\n\t\tOnCreate:    createImageProcessor,\n\t\tOnDestroy:   destroyImageProcessor,\n\t\tWorkerCount: 2,\n\t\tCtx:         context.Background(),\n\t}\n\tmanager, err := tasker.NewTaskManager[*ImageProcessor, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\tdefer manager.Stop()\n\n\t// Queue a normal image resize task (takes longer)\n\tgo func() {\n\t\tfmt.Println(\"Queueing normal resize...\")\n\t\tresult, err := manager.QueueTask(func(proc *ImageProcessor) (string, error) {\n\t\t\tfmt.Printf(\"Worker %d processing normal resize\\n\", proc.ID)\n\t\t\ttime.Sleep(150 * time.Millisecond)\n\t\t\treturn \"imageA_resized.jpg\", nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"Normal Resize Failed: %v\\n\", err) }\n\t\telse { fmt.Printf(\"Normal Resize Completed: %s\\n\", result) }\n\t}()\n\n\t// Queue a high-priority thumbnail generation task (faster, but queued after normal)\n\tgo func() {\n\t\tfmt.Println(\"Queueing high-priority thumbnail...\")\n\t\tresult, err := manager.QueueTaskWithPriority(func(proc *ImageProcessor) (string, error) {\n\t\t\tfmt.Printf(\"Worker %d processing HIGH PRIORITY thumbnail\\n\", proc.ID)\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\treturn \"video_thumbnail.jpg\", nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"Priority Thumbnail Failed: %v\\n\", err) }\n\t\telse { fmt.Printf(\"Priority Thumbnail Completed: %s\\n\", result) }\n\t}()\n\n\ttime.Sleep(500 * time.Millisecond) // Allow tasks to run\n}\n```\n\n**Outcome**: Despite being potentially queued later, the \"HIGH PRIORITY thumbnail\" task is likely to be processed and completed before the \"normal resize\" task, demonstrating the effect of priority queuing.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [task is critical to user experience or system responsiveness] THEN [use `QueueTaskWithPriority`] ELSE [use `QueueTask`]",
          "IF [multiple tasks are queued concurrently and order of execution matters for urgency] THEN [prioritize critical tasks with `QueueTaskWithPriority`]"
        ],
        "verificationSteps": [
          "Check: High-priority task completes before or significantly faster than a longer normal task queued before it (under load) → Expected: Output order confirms priority execution.",
          "Check: `Stats().PriorityTasks` count increases upon submission and decreases upon processing → Expected: Live stats reflect priority queue utilization."
        ],
        "quickPatterns": [
          "Pattern: High-priority task with result handling\n```go\ngo func() {\n    result, err := manager.QueueTaskWithPriority(func(res *MyResource) (string, error) {\n        // Your high-priority task logic\n        return \"Critical task done\", nil\n    })\n    if err != nil { /* handle error */ }\n    else { /* process result */ }\n}()\n```"
        ],
        "diagnosticPaths": [
          "Error `Priority task not completing faster than normal tasks` -> Symptom: Priority queuing appears ineffective -> Check: Verify enough workers are active to pick up tasks; ensure priority queue is not full preventing submission -> Fix: Adjust `WorkerCount` or `MaxWorkerCount`, check for blocking operations in `OnCreate`."
        ]
      }
    },
    {
      "title": "Task-Based Guide",
      "path": "task-based-guide/immediate-synchronous-tasks.md",
      "content": "### Immediate Synchronous Tasks\n\n**Goal**: Execute a task immediately, blocking the caller until completion, without waiting in a queue. Ideal for user-facing, low-latency operations.\n\n**Approach**: Use `RunTask`. This method is designed for urgent, synchronous operations. It attempts to get a resource from the pre-allocated `resourcePool`. If the pool is empty, it temporarily creates a new resource for the task's duration via `OnCreate`, uses it, and then destroys it via `OnDestroy`.\n\n**Example: Generating a Fast Preview**\nImagine a scenario where a user needs an instant preview of an image. This task shouldn't wait in a queue behind other background image processing jobs.\n\n```go\n// examples/intermediate/main.go (Simplified)\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype ImageProcessor struct { ID int }\nfunc createImageProcessor() (*ImageProcessor, error) { return &ImageProcessor{ID: 1}, nil }\nfunc destroyImageProcessor(p *ImageProcessor) error { return nil }\nfunc checkImageProcessorHealth(err error) bool { return true }\n\nfunc main() {\n\tconfig := tasker.Config[*ImageProcessor]{\n\t\tOnCreate:         createImageProcessor,\n\t\tOnDestroy:        destroyImageProcessor,\n\t\tWorkerCount:      2,\n\t\tCtx:              context.Background(),\n\t\tResourcePoolSize: 1, // Configure a pool for RunTask\n\t}\n\tmanager, err := tasker.NewTaskManager[*ImageProcessor, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\tdefer manager.Stop()\n\n\tfmt.Println(\"Running an immediate task...\")\n\t// This call blocks until the task completes\n\timmediateResult, immediateErr := manager.RunTask(func(proc *ImageProcessor) (string, error) {\n\t\tfmt.Printf(\"IMMEDIATE Task processing fast preview with processor %d\\n\", proc.ID)\n\t\ttime.Sleep(20 * time.Millisecond)\n\t\treturn \"fast_preview.jpg\", nil\n\t})\n\n\tif immediateErr != nil {\n\t\tfmt.Printf(\"Immediate Task Failed: %v\\n\", immediateErr)\n\t} else {\n\t\tfmt.Printf(\"Immediate Task Completed: %s\\n\", immediateResult)\n\t}\n\n\ttime.Sleep(50 * time.Millisecond)\n}\n```\n\n**Outcome**: The `RunTask` call blocks until the `fast_preview.jpg` is generated. The `ImageProcessor` resource is either pulled from the `resourcePool` or created temporarily, ensuring minimal latency.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [caller requires immediate blocking result and task should bypass queues] THEN [use `RunTask`] ELSE [consider `QueueTask` or `QueueTaskWithPriority`]"
        ],
        "verificationSteps": [
          "Check: `RunTask` call blocks and returns immediately upon task completion → Expected: No significant delay beyond task execution time.",
          "Check: `Stats().AvailableResources` decreases when `RunTask` is called and increases after (if from pool) → Expected: Resource pool usage reflects `RunTask` activity.",
          "Check: `OnCreate` is called temporarily if `ResourcePoolSize` is 0 or pool is empty and `RunTask` is called → Expected: Logs confirm temporary resource creation/destruction."
        ],
        "quickPatterns": [
          "Pattern: Synchronous `RunTask` execution\n```go\nresult, err := manager.RunTask(func(res *MyResource) (string, error) {\n    // Your immediate, synchronous task logic\n    return \"Immediate result\", nil\n})\nif err != nil { /* handle error */ }\n// Process result immediately\n```"
        ],
        "diagnosticPaths": [
          "Error `RunTask returns 'failed to create temporary resource'` -> Symptom: No resources are available or `OnCreate` fails for temporary resource -> Check: Ensure `Config.OnCreate` is robust; check if `ResourcePoolSize` is adequate or if resource creation is consistently failing -> Fix: Address `OnCreate` errors, consider increasing `ResourcePoolSize` or improving resource availability."
        ]
      }
    },
    {
      "title": "Task-Based Guide",
      "path": "task-based-guide/resilient-error-handling.md",
      "content": "### Resilient Error Handling and At-Most-Once Execution\n\n**Goal**: Build a resilient task processing system that automatically recovers from worker/resource failures and correctly handles tasks that should not be retried.\n\n**Approach**: Utilize `Config.CheckHealth` to define what constitutes an \"unhealthy\" worker/resource error, enabling automatic worker replacement and task retries (`Config.MaxRetries`). For non-idempotent operations, use `QueueTaskOnce` or `QueueTaskWithPriorityOnce` to prevent automatic re-queuing on health-related failures.\n\n**Example: Image Processing with Crash Simulation**\nThis scenario simulates an image processor crashing due to a bad input, triggering the health check and potentially a retry, or preventing a retry for a sensitive operation.\n\n```go\n// examples/intermediate/main.go (Excerpt)\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype ImageProcessor struct { ID int }\nfunc createImageProcessor() (*ImageProcessor, error) { /* ... */ return &ImageProcessor{ID: 1}, nil}\nfunc destroyImageProcessor(p *ImageProcessor) error { /* ... */ return nil}\n\n// Custom health check: \"processor_crash\" means the worker is unhealthy and needs replacement.\nfunc checkImageProcessorHealth(err error) bool {\n\tif err != nil && err.Error() == \"processor_crash\" {\n\t\tfmt.Printf(\"WARN: Detected unhealthy error: %v. Worker will be replaced.\\n\", err)\n\t\treturn false // This error indicates an unhealthy state\n\t}\n\treturn true // Other errors are just task failures, not worker health issues\n}\n\nfunc main() {\n\tconfig := tasker.Config[*ImageProcessor]{\n\t\tOnCreate:    createImageProcessor,\n\t\tOnDestroy:   destroyImageProcessor,\n\t\tWorkerCount: 2,\n\t\tCtx:         context.Background(),\n\t\tCheckHealth: checkImageProcessorHealth, // Use custom health check\n\t\tMaxRetries:  1,                         // One retry on unhealthy errors\n\t}\n\tmanager, err := tasker.NewTaskManager[*ImageProcessor, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\tdefer manager.Stop()\n\n\t// Task that might cause an \"unhealthy\" error (will be retried once)\n\tgo func() {\n\t\tfmt.Println(\"Queueing task that might crash a worker (retried)...\")\n\t\t_, err := manager.QueueTask(func(proc *ImageProcessor) (string, error) {\n\t\t\tif rand.Intn(2) == 0 { return \"\", errors.New(\"processor_crash\") }\n\t\t\treturn \"processed_retried\", nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"Retried task failed: %v\\n\", err) }\n\t\telse { fmt.Println(\"Retried task completed.\") }\n\t}()\n\n\t// Task that might cause an \"unhealthy\" error (NOT retried - at-most-once)\n\tgo func() {\n\t\tfmt.Println(\"Queueing task that might crash (at-most-once)...\")\n\t\t_, err := manager.QueueTaskOnce(func(proc *ImageProcessor) (string, error) {\n\t\t\tif rand.Intn(2) == 0 { return \"\", errors.New(\"processor_crash\") }\n\t\t\treturn \"processed_once\", nil\n\t\t})\n\t\tif err != nil { fmt.Printf(\"At-most-once task failed: %v\\n\", err) }\n\t\telse { fmt.Println(\"At-most-once task completed.\") }\n\t}()\n\n\ttime.Sleep(2 * time.Second) // Allow time for tasks and potential worker replacements\n}\n```\n\n**Outcome**: When a task returns a `processor_crash` error:\n*   For `QueueTask`, `CheckHealth` identifies it as unhealthy, the worker is replaced, and the task is re-queued for another attempt (up to `MaxRetries`).\n*   For `QueueTaskOnce`, `CheckHealth` still identifies it as unhealthy and the worker is replaced, but the task is *not* re-queued, ensuring it's attempted at most once by the `TaskManager`'s retry logic.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [task failure means the worker/resource is broken and needs replacement] THEN [implement `CheckHealth` to return `false`] ELSE [treat as task-specific error only]",
          "IF [operation is non-idempotent and should not be re-executed by the manager if worker fails] THEN [use `QueueTaskOnce` or `QueueTaskWithPriorityOnce`] ELSE [use standard `QueueTask` or `QueueTaskWithPriority`]"
        ],
        "verificationSteps": [
          "Check: `CheckHealth` returns `false` for specific error → Expected: Worker `OnDestroy` is called, new worker `OnCreate` is called, and task is retried (if not `*Once` task).",
          "Check: `QueueTaskOnce` fails due to unhealthy worker → Expected: Task result/error is returned to caller, but task is NOT re-queued internally by Tasker."
        ],
        "quickPatterns": [
          "Pattern: Task with potential unhealthy error\n```go\n_, err := manager.QueueTask(func(res *MyResource) (string, error) {\n    if someCondition { return \"\", errors.New(\"unrecoverable_resource_error\") }\n    return \"ok\", nil\n})\n```",
          "Pattern: At-most-once task\n```go\n_, err := manager.QueueTaskOnce(func(res *MyResource) (string, error) {\n    // Non-idempotent operation\n    return \"ok\", nil\n})\n```"
        ],
        "diagnosticPaths": [
          "Error `Max retries exceeded for task` -> Symptom: Task repeatedly fails and is not processed -> Check: Review `CheckHealth` and task logic. The task is consistently causing an unhealthy state, or `MaxRetries` is too low -> Fix: Debug the root cause of the unhealthy state, increase `MaxRetries` for transient issues, or use `*Once` tasks if re-execution is undesired.",
          "Error `Task not retried as expected` -> Symptom: Task fails but `CheckHealth` indicated unhealthy state, yet no retry occurred -> Check: Ensure the task was not submitted using `*Once` methods; verify `MaxRetries` is greater than 0 -> Fix: Adjust task submission method or `MaxRetries`."
        ]
      }
    },
    {
      "title": "Task-Based Guide",
      "path": "task-based-guide/dynamic-scaling-with-bursting.md",
      "content": "### Dynamic Scaling (Bursting)\n\n**Goal**: Automatically adjust the number of active workers based on real-time task load to optimize resource utilization and maintain throughput.\n\n**Approach**: Configure `Config.BurstInterval`. Tasker's internal burst manager periodically monitors the `TaskArrivalRate` and `TaskCompletionRate` (from `Metrics()`).\n\n*   If `TaskArrivalRate > TaskCompletionRate`: The system is falling behind, so the burst manager dynamically starts additional \"burst\" workers (up to `MaxWorkerCount`) to meet demand.\n*   If `TaskCompletionRate > TaskArrivalRate`: The system is over-provisioned, and the burst manager signals idle burst workers to gracefully shut down, reducing resource consumption.\n\n**Example: Handling Burst Loads for Heavy Computation**\nThis advanced example demonstrates a scenario where a large number of computational tasks are queued rapidly, forcing Tasker to scale up workers, and then scaling down once the load subsides.\n\n```go\n// examples/advanced/main.go (Simplified)\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype HeavyComputeResource struct { ID int }\nfunc createComputeResource() (*HeavyComputeResource, error) { /* ... */ return &HeavyComputeResource{ID: rand.Intn(1000)}, nil }\nfunc destroyComputeResource(r *HeavyComputeResource) error { /* ... */ return nil}\nfunc checkComputeHealth(err error) bool { return true }\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tconfig := tasker.Config[*HeavyComputeResource]{\n\t\tOnCreate: createComputeResource,\n\t\tOnDestroy: destroyComputeResource,\n\t\tWorkerCount: 2,                     // Start with 2 base workers\n\t\tCtx: ctx,\n\t\tBurstInterval: 200 * time.Millisecond, // Check every 200ms\n\t\tMaxRetries: 0,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*HeavyComputeResource, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\n\tvar tasksSubmitted atomic.Int32\n\tfor i := 0; i < 20; i++ { // Aggressively queue 20 tasks\n\t\ttasksSubmitted.Add(1)\n\t\tgo func(taskID int) {\n\t\t\t_, _ = manager.QueueTask(func(res *HeavyComputeResource) (string, error) {\n\t\t\t\ttime.Sleep(time.Duration(100 + rand.Intn(300)) * time.Millisecond)\n\t\t\t\treturn fmt.Sprintf(\"Task %d completed\", taskID), nil\n\t\t\t})\n\t\t}(i)\n\t}\n\n\tfmt.Println(\"Monitoring stats (watch for BurstWorkers)...\")\n\tticker := time.NewTicker(250 * time.Millisecond)\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\tstats := manager.Stats()\n\t\t\t\tmetrics := manager.Metrics()\n\t\t\t\tfmt.Printf(\"Stats: Active=%d (Base=%d, Burst=%d), Queued=%d, ArrivalRate=%.2f, CompletionRate=%.2f\\n\",\n\t\t\t\t\tstats.ActiveWorkers, stats.BaseWorkers, stats.BurstWorkers,\n\t\t\t\t\tstats.QueuedTasks + stats.PriorityTasks,\n\t\t\t\t\tmetrics.TaskArrivalRate, metrics.TaskCompletionRate)\n\t\t\tcase <-done:\n\t\t\t\tticker.Stop()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\ttime.Sleep(5 * time.Second) // Observe scaling up and down\n\tclose(done) // Stop monitoring\n\n\tfmt.Println(\"Initiating graceful shutdown...\")\n\t_ = manager.Stop()\n}\n```\n\n**Outcome**: The `Stats()` output will show `BurstWorkers` dynamically increasing when the `QueuedTasks` grow and `TaskArrivalRate` exceeds `TaskCompletionRate`, then decreasing once tasks are processed and throughput catches up with or exceeds demand. This demonstrates efficient resource scaling.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [workload is highly variable with unpredictable spikes] THEN [enable dynamic scaling by setting `BurstInterval` > 0 and `MaxWorkerCount` appropriately] ELSE [rely on fixed `WorkerCount`]",
          "IF [resource costs are high per worker] THEN [configure dynamic scaling to optimize worker count based on demand] ELSE [maintain a static worker pool]"
        ],
        "verificationSteps": [
          "Check: Under high load, `Stats().BurstWorkers` increases from 0 → Expected: New worker creation logs appear, `ActiveWorkers` count grows.",
          "Check: After load subsides, `Stats().BurstWorkers` decreases to 0 → Expected: Burst worker destruction logs appear, `ActiveWorkers` count returns to `BaseWorkers`.",
          "Check: `TaskArrivalRate` vs `TaskCompletionRate` correlation with `BurstWorkers` count changes → Expected: System scales up when arrival rate > completion rate, scales down when arrival rate < completion rate."
        ],
        "quickPatterns": [
          "Pattern: Enable dynamic scaling\n```go\nconfig := tasker.Config[*MyResource]{\n    // ... other config ...\n    WorkerCount:   2,  // Base workers\n    MaxWorkerCount: 10, // Max total workers\n    BurstInterval: 100 * time.Millisecond, // Check every 100ms\n}\nmanager, _ := tasker.NewTaskManager[*MyResource, any](config)\n```"
        ],
        "diagnosticPaths": [
          "Error `System not scaling up during high load` -> Symptom: `QueuedTasks` remains high, `ActiveWorkers` does not increase -> Check: Verify `BurstInterval` is set and `MaxWorkerCount` is greater than `WorkerCount`; ensure `MetricsCollector` is active -> Fix: Adjust `BurstInterval` (e.g., lower it) or increase `MaxWorkerCount`.",
          "Error `System thrashing (constantly scaling up/down)` -> Symptom: `BurstWorkers` rapidly fluctuates -> Check: `BurstInterval` might be too low; rapid changes in `TaskArrivalRate` or `TaskCompletionRate` -> Fix: Increase `BurstInterval` to smooth out scaling decisions; analyze workload patterns."
        ]
      }
    },
    {
      "title": "Advanced Usage",
      "path": "advanced-usage/custom-logging-metrics.md",
      "content": "### Custom Logging and Metrics Integration\n\nTasker provides interfaces to integrate with your preferred logging solution and external metrics systems, giving you full control over observability.\n\n#### Custom Logging with `tasker.Logger`\nBy default, Tasker uses a no-op logger. To see internal Tasker messages or integrate with your logging framework (e.g., `logrus`, `zap`), implement the `tasker.Logger` interface and pass it to `Config.Logger`.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\n// MyCustomLogger implements tasker.Logger\ntype MyCustomLogger struct{}\n\nfunc (l *MyCustomLogger) Debugf(format string, args ...any) { log.Printf(\"[DEBUG] \"+format, args...) }\nfunc (l *MyCustomLogger) Infof(format string, args ...any)  { log.Printf(\"[INFO] \"+format, args...) }\nfunc (l *MyCustomLogger) Warnf(format string, args ...any)  { log.Printf(\"[WARN] \"+format, args...) }\nfunc (l *MyCustomLogger) Errorf(format string, args ...any) { log.Printf(\"[ERROR] \"+format, args...) }\n\n// (Assume CalculatorResource and its onCreate/onDestroy are defined as before)\ntype CalculatorResource struct{}\nfunc createCalcResource() (*CalculatorResource, error) { /* ... */ return &CalculatorResource{}, nil }\nfunc destroyCalcResource(r *CalculatorResource) error { /* ... */ return nil }\n\nfunc main() {\n\tconfig := tasker.Config[*CalculatorResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 1,\n\t\tCtx:         context.Background(),\n\t\tLogger:      &MyCustomLogger{}, // Inject your custom logger here\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\tdefer manager.Stop()\n\n\t_, _ = manager.QueueTask(func(r *CalculatorResource) (int, error) { return 1 + 1, nil })\n\ttime.Sleep(100 * time.Millisecond)\n}\n```\n\n**Outcome**: Tasker's internal messages will now be routed through `MyCustomLogger`, appearing in `log.Printf` output prefixed with `[DEBUG]`, `[INFO]`, etc.\n\n#### Custom Metrics with `tasker.MetricsCollector`\nTasker automatically collects a rich set of performance metrics. You can access these metrics via `manager.Metrics()` (which returns a `TaskMetrics` struct). If you wish to integrate with an external metrics system (e.g., Prometheus), you can provide your own implementation of `tasker.MetricsCollector`.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\n// MyCustomMetricsCollector implements tasker.MetricsCollector\ntype MyCustomMetricsCollector struct {\n\tsync.Mutex\n\t// Store metrics in a way suitable for your external system\n\tTotalArrived uint64\n\tTotalCompleted uint64\n\tLastMetrics tasker.TaskMetrics\n}\n\nfunc (m *MyCustomMetricsCollector) RecordArrival()       { m.Lock(); defer m.Unlock(); m.TotalArrived++ }\nfunc (m *MyCustomMetricsCollector) RecordCompletion(s tasker.TaskLifecycleTimestamps) {\n    m.Lock(); defer m.Unlock(); m.TotalCompleted++;\n    // In a real collector, you'd process stamps for latency percentiles etc.\n}\nfunc (m *MyCustomMetricsCollector) RecordFailure(s tasker.TaskLifecycleTimestamps) { /* ... */ }\nfunc (m *MyCustomMetricsCollector) RecordRetry()       { /* ... */ }\nfunc (m *mYCustomMetricsCollector) Metrics() tasker.TaskMetrics {\n    m.Lock(); defer m.Unlock();\n    // In a real collector, calculate current metrics from aggregated data\n    m.LastMetrics.TotalTasksCompleted = m.TotalCompleted\n    m.LastMetrics.TotalTasksArrived = m.TotalArrived // This is simplified, actual rate calc is complex\n    return m.LastMetrics\n}\n\n// (Assume CalculatorResource and its onCreate/onDestroy are defined as before)\ntype CalculatorResource struct{}\nfunc createCalcResource() (*CalculatorResource, error) { /* ... */ return &CalculatorResource{}, nil }\nfunc destroyCalcResource(r *CalculatorResource) error { /* ... */ return nil }\n\nfunc main() {\n\tcustomCollector := &MyCustomMetricsCollector{}\n\tconfig := tasker.Config[*CalculatorResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 1,\n\t\tCtx:         context.Background(),\n\t\tCollector:   customCollector, // Inject your custom collector here\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\tdefer manager.Stop()\n\n\t_, _ = manager.QueueTask(func(r *CalculatorResource) (int, error) { return 1 + 1, nil })\n\ttime.Sleep(100 * time.Millisecond)\n\n\tfmt.Printf(\"Custom Collector Total Tasks Arrived: %d\\n\", customCollector.TotalArrived)\n\tfmt.Printf(\"Custom Collector Total Tasks Completed: %d\\n\", customCollector.TotalCompleted)\n}\n```\n\n**Outcome**: Tasker's internal events (`RecordArrival`, `RecordCompletion`, etc.) will call your `MyCustomMetricsCollector` methods, allowing you to feed this data into your chosen metrics backend. `manager.Metrics()` will also return data from your custom collector.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [internal Tasker logs are required for debugging or monitoring] THEN [implement `tasker.Logger` and assign to `Config.Logger`] ELSE [use default no-op logger]",
          "IF [detailed Tasker performance metrics need to be exposed to an external monitoring system (e.g., Prometheus)] THEN [implement `tasker.MetricsCollector` and assign to `Config.Collector`] ELSE [rely on `manager.Stats()` and `manager.Metrics()` for in-process inspection]"
        ],
        "verificationSteps": [
          "Check: Custom logger methods are called for Tasker internal messages → Expected: Log output matches custom logger format.",
          "Check: Custom metrics collector methods (`RecordArrival`, `RecordCompletion`) are invoked on relevant task events → Expected: Internal counts in custom collector reflect actual task activity.",
          "Check: `manager.Metrics()` returns data populated by custom collector → Expected: Metrics reflect the state managed by the custom collector."
        ],
        "quickPatterns": [
          "Pattern: Minimal custom logger\n```go\nimport \"log\"\n\ntype ConsoleLogger struct{}\nfunc (ConsoleLogger) Debugf(f string, a ...any) { log.Printf(\"DEBUG: \"+f, a...) }\nfunc (ConsoleLogger) Infof(f string, a ...any)  { log.Printf(\"INFO: \"+f, a...) }\nfunc (ConsoleLogger) Warnf(f string, a ...any)  { log.Printf(\"WARN: \"+f, a...) }\nfunc (ConsoleLogger) Errorf(f string, a ...any) { log.Printf(\"ERROR: \"+f, a...) }\n\n// Usage:\nconfig.Logger = ConsoleLogger{}\n```",
          "Pattern: Simple custom metrics collector (for total tasks completed)\n```go\nimport \"sync/atomic\"\n\ntype CounterCollector struct { totalCompleted atomic.Uint64 }\nfunc (c *CounterCollector) RecordArrival() {} // No-op\nfunc (c *CounterCollector) RecordCompletion(s tasker.TaskLifecycleTimestamps) { c.totalCompleted.Add(1) }\nfunc (c *CounterCollector) RecordFailure(s tasker.TaskLifecycleTimestamps) {} // No-op\nfunc (c *CounterCollector) RecordRetry() {} // No-op\nfunc (c *CounterCollector) Metrics() tasker.TaskMetrics { return tasker.TaskMetrics{TotalTasksCompleted: c.totalCompleted.Load()} }\n\n// Usage:\nconfig.Collector = &CounterCollector{}\n```"
        ],
        "diagnosticPaths": [
          "Error `No Tasker logs appearing` -> Symptom: Tasker is running but no `INFO`, `DEBUG` messages are visible -> Check: Ensure `Config.Logger` is assigned a working logger instance and not `nil` or the `noOpLogger` -> Fix: Provide a concrete implementation for `Config.Logger`.",
          "Error `Metrics in external system are incorrect or missing` -> Symptom: `TaskMetrics` values are not propagating to monitoring dashboard -> Check: Verify the custom `MetricsCollector` implementation correctly processes `RecordArrival`, `RecordCompletion`, etc., and exports data to the external system -> Fix: Debug the `MetricsCollector`'s internal logic and export mechanism."
        ]
      }
    },
    {
      "title": "Advanced Usage",
      "path": "advanced-usage/context-cancellation.md",
      "content": "### Context Cancellation and Task Lifecycles\n\nTasker relies heavily on `context.Context` for managing the lifecycle of its workers and the `TaskManager` itself. Understanding how contexts are used is crucial for proper integration and shutdown.\n\n#### `Config.Ctx`\nThe `Ctx` field in `tasker.Config` is the parent `context.Context` for the entire `TaskManager`. All worker goroutines and the internal burst manager goroutine derive their contexts from this parent context. \n\n**Impact of `Config.Ctx` Cancellation:**\n*   **Graceful Shutdown (`Stop()`):** When `manager.Stop()` is called, Tasker cancels its internal context (which is derived from `Config.Ctx` by `NewTaskManager`). This signals all workers to enter a drain mode, finishing any queued tasks before exiting.\n*   **Immediate Shutdown (`Kill()`):** When `manager.Kill()` is called, Tasker also cancels its internal context, but signals workers to exit immediately without draining queues. This propagates cancellation to any long-running tasks that respect the context.\n*   **External Cancellation:** If the `cancel` function associated with the `Config.Ctx` *you provided* is called externally, it will trigger Tasker's graceful shutdown procedure, as if `manager.Stop()` was implicitly called. This is a common pattern for integrating Tasker into larger applications that manage their lifecycles via a root context.\n\n#### Propagating Contexts to Tasks\nWhile `Tasker` manages the worker contexts, your individual task functions (`func(R) (E, error)`) do *not* directly receive a `context.Context`. If your task logic needs to respect external cancellation signals (e.g., for long-running I/O operations), you must pass a context into your task function using a closure or by modifying your resource `R` to carry a context.\n\n**Example: Task with Internal Context Handling**\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype DataProcessor struct{}\nfunc createDataProcessor() (*DataProcessor, error) { return &DataProcessor{}, nil }\nfunc destroyDataProcessor(r *DataProcessor) error { return nil }\nfunc checkDataHealth(err error) bool { return true }\n\nfunc main() {\n\t// Create a parent context for the TaskManager\n\tappCtx, appCancel := context.WithCancel(context.Background())\n\n\tconfig := tasker.Config[*DataProcessor]{\n\t\tOnCreate:    createDataProcessor,\n\t\tOnDestroy:   destroyDataProcessor,\n\t\tWorkerCount: 1,\n\t\tCtx:         appCtx, // TaskManager derives its context from appCtx\n\t}\n\tmanager, err := tasker.NewTaskManager[*DataProcessor, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\t// Do NOT defer manager.Stop() here, we will call appCancel manually.\n\n\t// Task that respects its own context for cancellation\n\tgo func() {\n\t\t// Create a task-specific context for a long operation\n\t\ttaskCtx, taskCancel := context.WithTimeout(context.Background(), 500*time.Millisecond)\n\t\tdefer taskCancel()\n\n\t\tresult, err := manager.QueueTask(func(p *DataProcessor) (string, error) {\n\t\t\tfmt.Println(\"Task: Starting long operation...\")\n\t\t\tselect {\n\t\t\tcase <-time.After(1 * time.Second): // Simulate very long work\n\t\t\t\treturn \"Long operation completed\", nil\n\t\t\tcase <-taskCtx.Done():\n\t\t\t\treturn \"\", taskCtx.Err() // Task cancelled by its own context\n\t\t\t}\n\t\t})\n\n\t\tif err != nil { fmt.Printf(\"Task finished with error: %v\\n\", err) }\n\t\telse { fmt.Printf(\"Task result: %s\\n\", result) }\n\t}()\n\n\t// Simulate external signal to stop the application (and thus the TaskManager)\n\ttime.Sleep(200 * time.Millisecond)\n\tfmt.Println(\"Application: Signaling cancellation...\")\n\tappCancel() // Cancelling appCtx will trigger Tasker's graceful shutdown\n\n\ttime.Sleep(1 * time.Second) // Allow time for TaskManager to shut down\n\tfmt.Println(\"Application: Exited.\")\n}\n```\n\n**Outcome**: The `appCancel()` call triggers Tasker's shutdown. The long-running task, if it respects `taskCtx.Done()`, will be cancelled by its *own* context's timeout, and the Tasker will gracefully shut down after the task function returns its error.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [long-running tasks need to be interruptible by application shutdown or explicit cancellation] THEN [pass a dedicated `context.Context` into the task function (e.g., via closure) and ensure task respects `ctx.Done()`] ELSE [tasks will run to completion or only be interrupted by `manager.Kill()`]"
        ],
        "verificationSteps": [
          "Check: Cancelling `Config.Ctx` causes `manager.Stop()` behavior → Expected: Workers drain queues and exit gracefully.",
          "Check: Task function internally checks `context.Context.Done()` → Expected: Long-running task is interrupted when its passed context is cancelled."
        ],
        "quickPatterns": [
          "Pattern: Passing context into a task function via closure\n```go\nfunc processData(ctx context.Context, data []byte) error {\n    select {\n    case <-ctx.Done():\n        return ctx.Err()\n    case <-time.After(100 * time.Millisecond):\n        // Simulate work\n        return nil\n    }\n}\n\n// ... in main/caller ...\n\ntaskCtx, taskCancel := context.WithCancel(context.Background())\ndefer taskCancel()\n\n_, err := manager.QueueTask(func(res *MyResource) (string, error) {\n    err := processData(taskCtx, someData)\n    return \"\", err\n})\n```"
        ],
        "diagnosticPaths": [
          "Error `Tasks not cancelling/exiting during graceful shutdown` -> Symptom: `manager.Stop()` hangs, workers continue processing indefinitely -> Check: Ensure long-running task functions check `context.Context.Done()` at appropriate intervals and return an error -> Fix: Instrument tasks to respond to context cancellation.",
          "Error `TaskManager not shutting down when external root context is cancelled` -> Symptom: Application hangs despite `appCtx.Cancel()` being called -> Check: Confirm the `Config.Ctx` provided to `NewTaskManager` is the correct parent context that is being cancelled -> Fix: Ensure `Config.Ctx` is properly linked to the external cancellation mechanism."
        ]
      }
    },
    {
      "title": "Advanced Usage",
      "path": "advanced-usage/performance-tuning-monitoring.md",
      "content": "### Performance Tuning and Monitoring\n\nTasker provides comprehensive statistics and metrics to help you monitor its operational state and performance, enabling effective tuning and troubleshooting.\n\n#### Real-time Statistics with `Stats()`\n`manager.Stats()` returns a `TaskStats` struct, providing an immediate snapshot of the `TaskManager`'s current operational state:\n\n*   `BaseWorkers`: Number of permanently active workers.\n*   `ActiveWorkers`: Total currently active workers (base + burst).\n*   `BurstWorkers`: Number of dynamically scaled-up workers.\n*   `QueuedTasks`: Number of tasks in the main queue.\n*   `PriorityTasks`: Number of tasks in the priority queue.\n*   `AvailableResources`: Number of resources in the `RunTask` resource pool.\n\nThese statistics are useful for quick checks on worker counts and queue backlogs.\n\n#### Comprehensive Performance Metrics with `Metrics()`\n`manager.Metrics()` returns a `TaskMetrics` struct, offering deeper insights into the system's performance, throughput, and reliability over time. This data is collected by an internal `MetricsCollector` (or your custom one).\n\nKey `TaskMetrics` fields include:\n\n*   **Latency & Duration**: `AverageExecutionTime`, `MinExecutionTime`, `MaxExecutionTime`, `P95ExecutionTime`, `P99ExecutionTime`, `AverageWaitTime`.\n*   **Throughput & Volume**: `TaskArrivalRate`, `TaskCompletionRate`, `TotalTasksCompleted`.\n*   **Reliability & Error**: `TotalTasksFailed`, `TotalTasksRetried`, `SuccessRate`, `FailureRate`.\n\nThese metrics are essential for identifying performance bottlenecks, understanding system load, and diagnosing issues related to task failures or retries.\n\n**Example: Monitoring Stats and Metrics**\n```go\n// examples/advanced/main.go (Excerpt)\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\ntype HeavyComputeResource struct { ID int }\nfunc createComputeResource() (*HeavyComputeResource, error) { /* ... */ return &HeavyComputeResource{ID: 1}, nil}\nfunc destroyComputeResource(r *HeavyComputeResource) error { /* ... */ return nil}\nfunc checkComputeHealth(err error) bool { return true }\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tconfig := tasker.Config[*HeavyComputeResource]{\n\t\tOnCreate: createComputeResource,\n\t\tOnDestroy: destroyComputeResource,\n\t\tWorkerCount: 2,\n\t\tCtx: ctx,\n\t\tBurstInterval: 200 * time.Millisecond,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*HeavyComputeResource, string](config)\n\tif err != nil { log.Fatalf(\"Error creating task manager: %v\", err) }\n\n\t// Queue some tasks\n\tvar tasksSubmitted atomic.Int32\n\tfor i := 0; i < 5; i++ {\n\t\ttasksSubmitted.Add(1)\n\t\tgo func(taskID int) {\n\t\t\t_, _ = manager.QueueTask(func(res *HeavyComputeResource) (string, error) {\n\t\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\t\treturn fmt.Sprintf(\"Task %d completed\", taskID), nil\n\t\t\t})\n\t\t}(i)\n\t}\n\n\t// Monitor stats and metrics periodically\n\tticker := time.NewTicker(250 * time.Millisecond)\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\tstats := manager.Stats()\n\t\t\t\tmetrics := manager.Metrics()\n\t\t\t\tfmt.Printf(\"Stats: Active=%d (Base=%d, Burst=%d), Queued=%d, ArrivalRate=%.2f, CompletionRate=%.2f\\n\",\n\t\t\t\t\tstats.ActiveWorkers, stats.BaseWorkers, stats.BurstWorkers,\n\t\t\t\t\tstats.QueuedTasks + stats.PriorityTasks,\n\t\t\t\t\tmetrics.TaskArrivalRate, metrics.TaskCompletionRate)\n\t\t\tcase <-done:\n\t\t\t\tticker.Stop()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\ttime.Sleep(1 * time.Second) // Allow some monitoring\n\tclose(done) // Stop monitoring\n\n\t_ = manager.Stop()\n}\n```\n\n**Outcome**: The console output will show real-time updates of worker counts, queued tasks, and calculated arrival/completion rates, allowing you to observe the `TaskManager`'s performance characteristics.",
      "agentGuidance": {
        "decisionPoints": [
          "IF [immediate operational status (worker count, queue size) is needed] THEN [use `manager.Stats()`] ELSE [for deeper performance analysis, use `manager.Metrics()`]",
          "IF [long-term trends or detailed latency breakdowns (percentiles) are required] THEN [regularly poll `manager.Metrics()` and push to external monitoring system] ELSE [use `manager.Stats()` for quick checks]"
        ],
        "verificationSteps": [
          "Check: `Stats().ActiveWorkers` matches expected base + burst workers → Expected: Worker count is accurate.",
          "Check: `Stats().QueuedTasks` increases upon task submission and decreases upon processing → Expected: Queue size reflects task backlog.",
          "Check: `Metrics().TaskArrivalRate` and `Metrics().TaskCompletionRate` reflect actual task flow → Expected: Rates align with submitted and completed tasks."
        ],
        "quickPatterns": [
          "Pattern: Polling stats and metrics\n```go\nticker := time.NewTicker(5 * time.Second)\ndefer ticker.Stop()\nfor range ticker.C {\n    stats := manager.Stats()\n    metrics := manager.Metrics()\n    fmt.Printf(\"Active: %d, Queued: %d, Avg Exec: %s\\n\", stats.ActiveWorkers, stats.QueuedTasks, metrics.AverageExecutionTime)\n}\n```"
        ],
        "diagnosticPaths": [
          "Error `High QueuedTasks, low ActiveWorkers` -> Symptom: Tasks are backing up, but not enough workers are active -> Check: Verify `WorkerCount` and `MaxWorkerCount` in `Config`; check if dynamic scaling is enabled (`BurstInterval` > 0) -> Fix: Increase `WorkerCount` or `MaxWorkerCount`, adjust `BurstInterval`.",
          "Error `AverageWaitTime is consistently high` -> Symptom: Tasks spend too much time in queues -> Check: `ActiveWorkers` might be too low relative to `TaskArrivalRate`; tasks might be very long-running -> Fix: Increase worker capacity, optimize task execution time."
        ]
      }
    },
    {
      "title": "Problem Solving",
      "path": "problem-solving/common-issues-troubleshooting.md",
      "content": "### Common Issues & Troubleshooting\n\nThis section addresses frequently encountered problems and provides guidance for diagnosing and resolving them.\n\n#### \"Task manager is shutting down: cannot queue task\"\n*   **Symptom**: You attempt to `QueueTask`, `RunTask`, or similar methods, and they immediately return an error indicating the manager is shutting down.\n*   **Reason**: This error occurs if you try to submit tasks after `manager.Stop()` or `manager.Kill()` has been called, or if the `context.Context` provided in `Config.Ctx` has been cancelled externally.\n*   **Diagnosis**: Check the lifecycle of your `TaskManager` instance. Ensure that task submission logic only runs when the manager is expected to be active.\n*   **Resolution**: Only submit tasks while the manager is actively running. Implement checks (`if !manager.IsRunning()`) if necessary (though `IsRunning` is internal, the error itself indicates the state). For graceful shutdowns, ensure new tasks are not submitted after `Stop()` is initiated.\n\n#### Resource Creation Failure\n*   **Symptom**: `TaskManager` initialization fails, or workers fail to start with errors originating from your `OnCreate` function (e.g., \"Failed to create resource for pool: connection refused\").\n*   **Reason**: Your `OnCreate` function encountered an error while trying to provision a resource.\n*   **Diagnosis**: Inspect the error message from `OnCreate`. This typically points to issues with external dependencies (database, network, file system) or misconfigurations.\n*   **Resolution**: Debug your `OnCreate` implementation. Ensure all prerequisites for resource creation are met (e.g., correct connection strings, network access, necessary credentials). Make `OnCreate` robust to handle transient setup issues if possible.\n\n#### Workers Not Starting/Stopping as Expected\n*   **Symptom**: The number of `ActiveWorkers` in `Stats()` does not match your `WorkerCount` configuration, or workers persist after shutdown calls.\n*   **Reason**: \n    *   **Not Starting**: `OnCreate` might be failing, or `WorkerCount` is zero/negative.\n    *   **Not Stopping**: Long-running tasks within workers might not be responding to context cancellation during `Stop()`, or `OnDestroy` is blocking.\n*   **Diagnosis**: \n    *   Verify `Config.WorkerCount` is positive and `Config.MaxWorkerCount` (if used for bursting) allows for more workers.\n    *   Check `OnCreate` for errors.\n    *   For stopping issues, review your task functions and `OnDestroy` to ensure they are non-blocking and respect the context cancellation signal where appropriate.\n*   **Resolution**: \n    *   Correct `WorkerCount` and debug `OnCreate`.\n    *   Instrument your long-running tasks to periodically check `ctx.Done()` (if passing context) and return if cancelled. Ensure `OnDestroy` is quick and non-blocking.\n\n#### Deadlocks/Goroutine Leaks\n*   **Symptom**: Your application hangs, CPU usage is high without progress, or `pprof` shows numerous goroutines stuck.\n*   **Reason**: While Tasker is designed to prevent these internally, improper usage of `OnCreate`, `OnDestroy`, or your task functions (`func(R) (E, error)`) can lead to deadlocks or goroutine leaks (e.g., blocking indefinitely, not closing channels, or unhandled panics).\n*   **Diagnosis**: Use Go's built-in `pprof` tool to inspect goroutine stacks and identify where the deadlock or leak is occurring. Pay close attention to your custom `OnCreate`, `OnDestroy`, and task logic.\n*   **Resolution**: Ensure all custom functions are non-blocking and handle their own concurrency and resource management. If a task can panic, wrap its content in a `defer recover()` block to convert panics into errors that Tasker can handle gracefully.",
      "agentGuidance": {
        "decisionPoints": [],
        "verificationSteps": [
          "Check: `manager.Stop()` returns immediately without waiting for tasks to complete -> Expected: The `Kill()` method was invoked instead, or tasks are not blocking graceful shutdown."
        ],
        "quickPatterns": [],
        "diagnosticPaths": [
          "Error `task manager is shutting down` -> Symptom: Task submission fails instantly -> Check: Is `manager.Stop()` or `manager.Kill()` called before task submission? -> Fix: Reorder calls or handle shutdown state appropriately for new task submissions.",
          "Error `Resource creation failed for worker` -> Symptom: Workers are not starting -> Check: `OnCreate` function for external dependency issues (network, credentials, permissions) -> Fix: Resolve external dependency issues or improve `OnCreate` error handling.",
          "Error `Application hangs on manager.Stop()` -> Symptom: Graceful shutdown is not completing -> Check: Long-running tasks or `OnDestroy` functions are blocking -> Fix: Modify tasks to respect context cancellation; ensure `OnDestroy` is non-blocking.",
          "Error `High goroutine count / Application unresponsive` -> Symptom: Resource leaks or deadlocks -> Check: Use `go tool pprof` to analyze goroutine stacks for blocking operations in `OnCreate`, `OnDestroy`, or task functions -> Fix: Identify and unblock or optimize problematic code sections."
        ]
      }
    },
    {
      "title": "Problem Solving",
      "path": "problem-solving/error-reference.md",
      "content": "### Error Reference\n\nThis section details common error conditions you might encounter when using Tasker, their triggers, and recommended resolutions.\n\n#### 1. `errors.New(\"task manager is shutting down\")`\n*   **Type**: Go `error` (returned directly)\n*   **Symptoms**: Calls to `QueueTask`, `QueueTaskWithPriority`, `QueueTaskOnce`, `QueueTaskWithPriorityOnce`, or `RunTask` immediately return this error.\n*   **Properties**: No specific error properties.\n*   **Scenarios**:\n    *   **Trigger**: Attempting to queue a task after `manager.Stop()` has been called.\n    *   **Example**: \n        ```go\n        manager.Stop()\n        _, err := manager.QueueTask(func(r *MyResource) (int, error) { return 1, nil })\n        // err will be \"task manager is shutting down\"\n        ```\n    *   **Reason**: The `TaskManager` transitions to a `stopping` or `killed` state, preventing new tasks from being accepted.\n*   **Diagnosis**: Check the call sequence in your application. Is task submission happening after the manager is explicitly told to shut down or after its root context is cancelled?\n*   **Resolution**: Ensure tasks are only submitted when the `TaskManager` is in a running state. Refactor task submission logic to respect the application's shutdown signals.\n*   **Prevention**: Always check application lifecycle status before attempting to queue tasks, or structure your application so that task producers cease before `TaskManager` shutdown is initiated.\n*   **Handling Patterns**: \n    ```go\n    result, err := manager.QueueTask(myTaskFunc)\n    if err != nil {\n        if errors.Is(err, errors.New(\"task manager is shutting down\")) {\n            log.Println(\"Cannot queue task: manager is shutting down.\")\n            // Gracefully exit or handle the un-queued task\n        } else {\n            log.Printf(\"Task failed: %v\", err)\n        }\n    }\n    ```\n*   **Propagation Behavior**: This error is directly returned to the caller of the task submission method.\n\n#### 2. `errors.New(\"worker count must be positive\")`\n*   **Type**: Go `error` (returned directly)\n*   **Symptoms**: `tasker.NewTaskManager` returns this error during initialization.\n*   **Properties**: No specific error properties.\n*   **Scenarios**:\n    *   **Trigger**: Providing `Config.WorkerCount` as 0 or a negative number.\n    *   **Example**: \n        ```go\n        config := tasker.Config{ WorkerCount: 0, /* ... */ }\n        _, err := tasker.NewTaskManager(config)\n        // err will be \"worker count must be positive\"\n        ```\n    *   **Reason**: The `TaskManager` requires at least one worker to operate.\n*   **Diagnosis**: Inspect the `WorkerCount` field in your `tasker.Config` struct.\n*   **Resolution**: Set `Config.WorkerCount` to a positive integer (e.g., 1 or more).\n*   **Prevention**: Validate configuration inputs before passing them to `NewTaskManager`.\n*   **Handling Patterns**: Typically handled at application startup; a fatal log is common.\n    ```go\n    manager, err := tasker.NewTaskManager(config)\n    if err != nil {\n        log.Fatalf(\"Failed to initialize TaskManager: %v\", err)\n    }\n    ```\n*   **Propagation Behavior**: Returned immediately by `NewTaskManager`.\n\n#### 3. `errors.New(\"onCreate function is required\")`\n*   **Type**: Go `error` (returned directly)\n*   **Symptoms**: `tasker.NewTaskManager` returns this error during initialization.\n*   **Properties**: No specific error properties.\n*   **Scenarios**:\n    *   **Trigger**: Providing `Config.OnCreate` as `nil`.\n    *   **Example**: \n        ```go\n        config := tasker.Config{ OnCreate: nil, /* ... */ }\n        _, err := tasker.NewTaskManager(config)\n        // err will be \"onCreate function is required\"\n        ```\n    *   **Reason**: The `TaskManager` needs a function to create new instances of your resource `R`.\n*   **Diagnosis**: Inspect the `OnCreate` field in your `tasker.Config` struct.\n*   **Resolution**: Provide a non-nil function for `Config.OnCreate` that correctly initializes your resource.\n*   **Prevention**: Validate configuration inputs or ensure `OnCreate` is always set.\n*   **Handling Patterns**: Same as \"worker count must be positive\".\n*   **Propagation Behavior**: Returned immediately by `NewTaskManager`.\n\n#### 4. `errors.New(\"onDestroy function is required\")`\n*   **Type**: Go `error` (returned directly)\n*   **Symptoms**: `tasker.NewTaskManager` returns this error during initialization.\n*   **Properties**: No specific error properties.\n*   **Scenarios**:\n    *   **Trigger**: Providing `Config.OnDestroy` as `nil`.\n    *   **Example**: \n        ```go\n        config := tasker.Config{ OnDestroy: nil, /* ... */ }\n        _, err := tasker.NewTaskManager(config)\n        // err will be \"onDestroy function is required\"\n        ```\n    *   **Reason**: The `TaskManager` needs a function to properly clean up resources when workers shut down.\n*   **Diagnosis**: Inspect the `OnDestroy` field in your `tasker.Config` struct.\n*   **Resolution**: Provide a non-nil function for `Config.OnDestroy` that correctly cleans up your resource.\n*   **Prevention**: Validate configuration inputs or ensure `OnDestroy` is always set.\n*   **Handling Patterns**: Same as \"worker count must be positive\".\n*   **Propagation Behavior**: Returned immediately by `NewTaskManager`.\n\n#### 5. `fmt.Errorf(\"failed to create temporary resource: %w\", originalErr)`\n*   **Type**: Go `error` (wrapped error)\n*   **Symptoms**: `RunTask` returns this error.\n*   **Properties**: Wraps the original error from your `Config.OnCreate` function.\n*   **Scenarios**:\n    *   **Trigger**: `RunTask` attempts to create a temporary resource (because the `resourcePool` is empty or `ResourcePoolSize` is 0) and your `Config.OnCreate` function returns an error.\n    *   **Example**: \n        ```go\n        // If createImageProcessor returns an error\n        _, err := manager.RunTask(func(p *ImageProcessor) (string, error) { /* ... */ })\n        // err will contain \"failed to create temporary resource: [original error from onCreate]\"\n        ```\n    *   **Reason**: The temporary resource needed for immediate execution could not be provisioned.\n*   **Diagnosis**: Inspect the wrapped error (`errors.Unwrap(err)`) to find the root cause from your `OnCreate` function. This is often a transient issue with the external dependency.\n*   **Resolution**: Address the underlying cause in your `OnCreate` function. Ensure the resource it tries to create is available and accessible. Consider making `OnCreate` more resilient to transient failures or increasing `ResourcePoolSize` if resource creation is slow.\n*   **Prevention**: Implement robust error handling and retries within your `OnCreate` function if it interacts with unreliable external systems.\n*   **Handling Patterns**: \n    ```go\n    _, err := manager.RunTask(myImmediateTaskFunc)\n    if err != nil {\n        if errors.Is(err, fmt.Errorf(\"failed to create temporary resource\")) {\n            log.Printf(\"Could not execute immediate task due to resource provisioning error: %v\", errors.Unwrap(err))\n        } else {\n            log.Printf(\"Immediate task failed: %v\", err)\n        }\n    }\n    ```\n*   **Propagation Behavior**: Returned directly to the caller of `RunTask`.\n\n#### 6. `fmt.Errorf(\"max retries exceeded: %w\", originalErr)` (Task-specific error)\n*   **Type**: Go `error` (wrapped error)\n*   **Symptoms**: A `QueueTask` or `QueueTaskWithPriority` call eventually returns this error after internal retries.\n*   **Properties**: Wraps the original error that triggered the retry mechanism and failed the `Config.CheckHealth` function.\n*   **Scenarios**:\n    *   **Trigger**: A task repeatedly fails, and for each failure, your `Config.CheckHealth` function returns `false` (indicating an unhealthy worker/resource). After `Config.MaxRetries` attempts, the task is finally marked as failed.\n    *   **Example**: \n        ```go\n        // Given CheckHealth marks \"processor_crash\" as unhealthy\n        _, err := manager.QueueTask(func(proc *ImageProcessor) (string, error) {\n            return \"\", errors.New(\"processor_crash\") // This task will retry MaxRetries times\n        })\n        // After max retries, err will be \"max retries exceeded: processor_crash\"\n        ```\n    *   **Reason**: The task continuously leads to an unhealthy worker state, exhausting all configured retry attempts.\n*   **Diagnosis**: The root cause lies in the original error (`errors.Unwrap(err)`) that consistently triggers the unhealthy state. Debug the task logic and the resource it uses to understand why it's failing.\n*   **Resolution**: Fix the underlying problem causing the task to fail unhealthily. This might involve updating external services, changing task input, or improving resource robustness. Adjust `MaxRetries` if the issue is truly transient and requires more attempts.\n*   **Prevention**: Improve task idempotency, make `OnCreate` more robust to unhealthy states, or refine `CheckHealth` if some errors are not truly indicative of a worker/resource health issue.\n*   **Handling Patterns**: \n    ```go\n    _, err := manager.QueueTask(myProblematicTaskFunc)\n    if err != nil {\n        if errors.Is(err, errors.New(\"max retries exceeded\")) {\n            log.Printf(\"Task failed permanently after retries, original error: %v\", errors.Unwrap(err))\n            // Log this as a critical failure, potentially alert\n        } else {\n            log.Printf(\"Task failed with other error: %v\", err)\n        }\n    }\n    ```\n*   **Propagation Behavior**: Returned to the caller of `QueueTask` or `QueueTaskWithPriority`.",
      "agentGuidance": {
        "decisionPoints": [],
        "verificationSteps": [],
        "quickPatterns": [],
        "diagnosticPaths": []
      }
    }
  ],
  "reference": {
    "system": {
      "name": "Tasker",
      "language": "Go",
      "description": "A powerful and flexible Go library for efficient concurrent task management, featuring a customizable worker pool, dynamic scaling, priority queuing, and robust resource lifecycle management.",
      "keyFeatures": [
        "Concurrent Task Execution",
        "Generic Resource Management",
        "Rate-Based Dynamic Worker Scaling",
        "Priority Queues",
        "Immediate Task Execution with Resource Pooling (`RunTask`)",
        "Customizable Health Checks & Retries",
        "\"At-Most-Once\" Task Execution",
        "Graceful & Immediate Shutdown",
        "Real-time Performance Metrics",
        "Custom Logging"
      ]
    },
    "dependencies": {
      "external": [
        {
          "name": "github.com/asaidimu/tasker",
          "purpose": "The core Tasker library itself; used as a dependency in example applications. Provides the TaskManager interface and implementation.",
          "interfaces": [
            {
              "name": "TaskManager",
              "description": "The primary interface for managing asynchronous and synchronous task execution within a pool of workers and resources.",
              "methods": [
                {
                  "name": "QueueTask",
                  "signature": "QueueTask(task func(R) (E, error)) (E, error)",
                  "parameters": "task: A function representing the task's logic, accepting a resource of type `R` and returning a result `E` or an error.",
                  "returnValue": "Returns the task's result of type `E` and any error encountered during execution. The call blocks until completion."
                },
                {
                  "name": "RunTask",
                  "signature": "RunTask(task func(R) (E, error)) (E, error)",
                  "parameters": "task: A function representing the task's logic, accepting a resource of type `R` and returning a result `E` or an error.",
                  "returnValue": "Returns the task's result of type `E` and any error encountered during execution. The call blocks until completion."
                },
                {
                  "name": "QueueTaskWithPriority",
                  "signature": "QueueTaskWithPriority(task func(R) (E, error)) (E, error)",
                  "parameters": "task: A function representing the task's high-priority logic, accepting a resource of type `R` and returning a result `E` or an error.",
                  "returnValue": "Returns the task's result of type `E` and any error encountered during execution. The call blocks until completion."
                },
                {
                  "name": "QueueTaskOnce",
                  "signature": "QueueTaskOnce(task func(R) (E, error)) (E, error)",
                  "parameters": "task: A function representing the task's logic, accepting a resource of type `R` and returning a result `E` or an error. This task will not be re-queued if `CheckHealth` indicates an unhealthy state.",
                  "returnValue": "Returns the task's result of type `E` and any error encountered during execution. The call blocks until completion."
                },
                {
                  "name": "QueueTaskWithPriorityOnce",
                  "signature": "QueueTaskWithPriorityOnce(task func(R) (E, error)) (E, error)",
                  "parameters": "task: A function representing the task's high-priority logic, accepting a resource of type `R` and returning a result `E` or an error. This task will not be re-queued if `CheckHealth` indicates an unhealthy state.",
                  "returnValue": "Returns the task's result of type `E` and any error encountered during execution. The call blocks until completion."
                },
                {
                  "name": "Stop",
                  "signature": "Stop() error",
                  "parameters": "None.",
                  "returnValue": "Returns an error if the manager is already stopping or killed, otherwise nil."
                },
                {
                  "name": "Kill",
                  "signature": "Kill() error",
                  "parameters": "None.",
                  "returnValue": "Returns an error if the manager is already killed, otherwise nil."
                },
                {
                  "name": "Stats",
                  "signature": "Stats() TaskStats",
                  "parameters": "None.",
                  "returnValue": "Returns a `TaskStats` struct containing current operational statistics."
                },
                {
                  "name": "Metrics",
                  "signature": "Metrics() TaskMetrics",
                  "parameters": "None.",
                  "returnValue": "Returns a `TaskMetrics` struct containing aggregated performance metrics."
                }
              ]
            },
            {
              "name": "Logger",
              "description": "Interface for custom logging within Tasker. Users can provide their own implementation.",
              "methods": [
                {
                  "name": "Debugf",
                  "signature": "Debugf(format string, args ...any)",
                  "parameters": "format: Format string; args: Arguments for formatting.",
                  "returnValue": "None."
                },
                {
                  "name": "Infof",
                  "signature": "Infof(format string, args ...any)",
                  "parameters": "format: Format string; args: Arguments for formatting.",
                  "returnValue": "None."
                },
                {
                  "name": "Warnf",
                  "signature": "Warnf(format string, args ...any)",
                  "parameters": "format: Format string; args: Arguments for formatting.",
                  "returnValue": "None."
                },
                {
                  "name": "Errorf",
                  "signature": "Errorf(format string, args ...any)",
                  "parameters": "format: Format string; args: Arguments for formatting.",
                  "returnValue": "None."
                }
              ]
            },
            {
              "name": "MetricsCollector",
              "description": "Interface for collecting and calculating performance and reliability metrics for the TaskManager. Users can provide their own implementation.",
              "methods": [
                {
                  "name": "RecordArrival",
                  "signature": "RecordArrival()",
                  "parameters": "None.",
                  "returnValue": "None."
                },
                {
                  "name": "RecordCompletion",
                  "signature": "RecordCompletion(stamps TaskLifecycleTimestamps)",
                  "parameters": "stamps: `TaskLifecycleTimestamps` containing queued, started, and finished times for a completed task.",
                  "returnValue": "None."
                },
                {
                  "name": "RecordFailure",
                  "signature": "RecordFailure(stamps TaskLifecycleTimestamps)",
                  "parameters": "stamps: `TaskLifecycleTimestamps` containing queued, started, and finished times for a failed task.",
                  "returnValue": "None."
                },
                {
                  "name": "RecordRetry",
                  "signature": "RecordRetry()",
                  "parameters": "None.",
                  "returnValue": "None."
                },
                {
                  "name": "Metrics",
                  "signature": "Metrics() TaskMetrics",
                  "parameters": "None.",
                  "returnValue": "Returns a `TaskMetrics` struct containing a snapshot of aggregated performance metrics."
                }
              ]
            }
          ],
          "installation": "go get github.com/asaidimu/tasker",
          "version": ">=1.0.0"
        },
        {
          "name": "context",
          "purpose": "Go standard library package for carrying deadlines, cancellation signals, and other request-scoped values across API boundaries and between goroutines.",
          "interfaces": [
            {
              "name": "context.Context",
              "description": "The fundamental interface for context propagation in Go.",
              "methods": [
                {
                  "name": "Done",
                  "signature": "Done() <-chan struct{}",
                  "parameters": "None.",
                  "returnValue": "Returns a channel that is closed when the context is cancelled or times out."
                },
                {
                  "name": "Err",
                  "signature": "Err() error",
                  "parameters": "None.",
                  "returnValue": "Returns a non-nil error if Done is closed, specifying why the context was cancelled (e.g., `Canceled` or `DeadlineExceeded`)."
                }
              ]
            }
          ],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "errors",
          "purpose": "Go standard library package for error handling, including creating new errors and unwrapping them.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "fmt",
          "purpose": "Go standard library package for formatted I/O.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "log",
          "purpose": "Go standard library package for simple logging.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "math",
          "purpose": "Go standard library package for common mathematical functions.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "math/rand",
          "purpose": "Go standard library package for pseudo-random number generation.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "sort",
          "purpose": "Go standard library package for sorting slices and user-defined collections.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "sync",
          "purpose": "Go standard library package for basic synchronization primitives like mutexes and wait groups.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "sync/atomic",
          "purpose": "Go standard library package for low-level atomic memory primitives.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        },
        {
          "name": "time",
          "purpose": "Go standard library package for measuring and displaying time.",
          "interfaces": [],
          "installation": "Built-in to Go standard library.",
          "version": ">=1.24.3 (Go version)"
        }
      ],
      "peer": [
        {
          "name": "Go Runtime",
          "reason": "Required for compiling and running Tasker applications. Tasker leverages Go's concurrency primitives (goroutines, channels) directly.",
          "version": ">=1.24.3"
        }
      ]
    },
    "integration": {
      "environmentRequirements": "To use Tasker, you need a Go development environment with Go version 1.24.3 or higher. Tasker itself is cross-platform, so it runs wherever Go is supported (Linux, Windows, macOS, etc.). No special compiler settings are required beyond standard Go build practices.",
      "initializationPatterns": [
        {
          "description": "Standard initialization of Tasker with custom resource lifecycle functions and basic worker configuration. This pattern demonstrates the minimum required setup for a functional TaskManager.",
          "codeExample": "package main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker\"\n)\n\n// Define your custom resource type\ntype DatabaseConnection struct { ID int }\n\n// onCreate: Function to create a new database connection\nfunc createDBConnection() (*DatabaseConnection, error) {\n\tlog.Println(\"INFO: Creating DatabaseConnection\")\n\t// Simulate connecting to a database\n\ttime.Sleep(10 * time.Millisecond)\n\treturn &DatabaseConnection{ID: 123}, nil\n}\n\n// onDestroy: Function to close the database connection\nfunc destroyDBConnection(conn *DatabaseConnection) error {\n\tlog.Printf(\"INFO: Destroying DatabaseConnection %d\\n\", conn.ID)\n\t// Simulate closing the connection\n\treturn nil\n}\n\nfunc main() {\n\t// Create a background context for the TaskManager\n\tctx := context.Background()\n\n\t// Configure the TaskManager\n\tconfig := tasker.Config[*DatabaseConnection]{\n\t\tOnCreate:    createDBConnection,    // Required: function to create resource\n\t\tOnDestroy:   destroyDBConnection,   // Required: function to destroy resource\n\t\tWorkerCount: 5,                     // Required: number of base workers\n\t\tCtx:         ctx,                   // Required: parent context for lifecycle\n\t\t// Optional fields:\n\t\t// MaxWorkerCount: 10,\n\t\t// BurstInterval: 100 * time.Millisecond,\n\t\t// CheckHealth: func(err error) bool { return true },\n\t\t// MaxRetries: 3,\n\t\t// ResourcePoolSize: 5,\n\t\t// Logger: &myCustomLogger{},\n\t\t// Collector: &myCustomMetricsCollector{},\n\t}\n\n\t// Create a new TaskManager instance\n\tmanager, err := tasker.NewTaskManager[*DatabaseConnection, string](config) // Tasks will return string results\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create TaskManager: %v\", err)\n\t}\n\n\t// Ensure graceful shutdown when main exits\n\tdefer manager.Stop()\n\n\tlog.Println(\"TaskManager initialized and running. Add tasks here.\")\n\t// Example task (non-blocking for main)\n\tgo func() {\n\t\t_, err := manager.QueueTask(func(db *DatabaseConnection) (string, error) {\n\t\t\tlog.Printf(\"Worker processing database query with connection %d\\n\", db.ID)\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\treturn \"Query result\", nil\n\t\t})\n\t\tif err != nil { log.Printf(\"Task failed: %v\\n\", err) }\n\t\telse { log.Println(\"Task completed.\") }\n\t}()\n\n\ttime.Sleep(200 * time.Millisecond) // Allow time for tasks\n}\n"
        }
      ],
      "commonPitfalls": [
        {
          "issue": "Blocking operations in `OnCreate` or `OnDestroy`",
          "solution": "`OnCreate` and `OnDestroy` functions should be non-blocking and execute quickly. Blocking operations can delay worker startup/shutdown, leading to performance issues or graceful shutdown hangs. If an operation *must* block (e.g., waiting for an external service to become available on startup), consider handling it with timeouts or asynchronous initialization outside these functions where possible. Make sure `OnDestroy` never deadlocks or waits indefinitely."
        },
        {
          "issue": "Task functions that do not respect context cancellation",
          "solution": "For long-running tasks, if you want them to be interruptible during `manager.Stop()` or `manager.Kill()`, your task function's internal logic needs to periodically check a `context.Context.Done()` channel and return if it's closed. Tasker itself doesn't directly pass a context to your `func(R) (E, error)`, so you must manage context propagation within your application (e.g., by capturing a context in a closure)."
        },
        {
          "issue": "Incorrect `CheckHealth` logic leading to worker thrashing",
          "solution": "If your `CheckHealth` function returns `false` for every task error (even transient ones), it can cause workers to be constantly replaced (`OnDestroy` then `OnCreate`), leading to high resource churn and degraded performance. Ensure `CheckHealth` returns `false` only for errors that truly indicate an unhealthy, unrecoverable worker or resource state, not for recoverable task-specific failures."
        },
        {
          "issue": "Queueing tasks after manager shutdown",
          "solution": "Attempting to call `QueueTask`, `RunTask`, etc., after `manager.Stop()` or `manager.Kill()` has been invoked will immediately return an error (`task manager is shutting down`). Ensure your application's task submission logic is aware of the TaskManager's lifecycle and ceases submissions during shutdown."
        }
      ],
      "lifecycleDependencies": "The Tasker's lifecycle is managed by the `context.Context` provided in `Config.Ctx`. When this context is cancelled (either explicitly by your application or implicitly by `manager.Stop()`/`manager.Kill()`), it signals all internal goroutines (workers, burst manager) to begin their shutdown procedures. Workers will call `OnDestroy` on their associated resources as they exit. The `NewTaskManager` function itself calls `OnCreate` to populate the initial `resourcePool` and for each base worker, so `OnCreate` must be ready before `NewTaskManager` is called."
    },
    "types": {
      "Config[R]": {
        "id": "type:Config[R]",
        "definition": "type Config[R any] struct {\n    OnCreate func() (R, error)\n    OnDestroy func(R) error\n    WorkerCount int\n    Ctx context.Context\n    CheckHealth func(error) bool\n    MaxWorkerCount int\n    BurstInterval time.Duration\n    MaxRetries int\n    ResourcePoolSize int\n    Logger Logger\n    Collector MetricsCollector\n    BurstTaskThreshold int `deprecated`\n    BurstWorkerCount int `deprecated`\n}",
        "purpose": "Configures the behavior and parameters for a new `TaskManager` instance, including resource management, worker scaling, and error handling policies.",
        "related": {
          "methods": ["method:NewTaskManager"],
          "patterns": []
        },
        "interfaceContract": {
          "requiredMethods": [],
          "optionalMethods": [],
          "parameterObjectStructures": {
            "OnCreate": "func() (R, error) - Function to create a new resource of type `R`. Required.",
            "OnDestroy": "func(R) error - Function to destroy a resource of type `R`. Required.",
            "WorkerCount": "int - Initial and minimum number of base workers. Must be > 0. Required.",
            "Ctx": "context.Context - Parent context for the TaskManager. Required.",
            "CheckHealth": "func(error) bool - Optional function to determine if an error indicates an unhealthy worker/resource. Default: always returns true.",
            "MaxWorkerCount": "int - Maximum total workers (base + burst). Default: `WorkerCount * 2`.",
            "BurstInterval": "time.Duration - Frequency for burst manager checks. Default: 100ms. Set to 0 to disable bursting.",
            "MaxRetries": "int - Max retries for a task on unhealthy errors. Default: 3. Set to 0 for no retries.",
            "ResourcePoolSize": "int - Number of resources to pre-allocate for `RunTask`. Default: `WorkerCount`.",
            "Logger": "Logger - Custom logger implementation. Default: no-op logger.",
            "Collector": "MetricsCollector - Custom metrics collector. Default: internal collector."
          }
        }
      },
      "Logger": {
        "id": "type:Logger",
        "definition": "type Logger interface {\n    Debugf(format string, args ...any)\n    Infof(format string, args ...any)\n    Warnf(format string, args ...any)\n    Errorf(format string, args ...any)\n}",
        "purpose": "Defines the interface for logging messages from the TaskManager, allowing users to integrate their own preferred logging library.",
        "related": {
          "methods": [],
          "patterns": ["pattern:Custom Logging"]
        },
        "interfaceContract": {
          "requiredMethods": [
            {
              "name": "Debugf",
              "signature": "Debugf(format string, args ...any)",
              "parameters": "format: A format string for the log message; args: Variadic arguments for the format string.",
              "returnValue": "None.",
              "sideEffects": "Logs a debug-level message to the configured output."
            },
            {
              "name": "Infof",
              "signature": "Infof(format string, args ...any)",
              "parameters": "format: A format string for the log message; args: Variadic arguments for the format string.",
              "returnValue": "None.",
              "sideEffects": "Logs an info-level message to the configured output."
            },
            {
              "name": "Warnf",
              "signature": "Warnf(format string, args ...any)",
              "parameters": "format: A format string for the log message; args: Variadic arguments for the format string.",
              "returnValue": "None.",
              "sideEffects": "Logs a warning-level message to the configured output."
            },
            {
              "name": "Errorf",
              "signature": "Errorf(format string, args ...any)",
              "parameters": "format: A format string for the log message; args: Variadic arguments for the format string.",
              "returnValue": "None.",
              "sideEffects": "Logs an error-level message to the configured output."
            }
          ],
          "optionalMethods": [],
          "parameterObjectStructures": {}
        }
      },
      "MetricsCollector": {
        "id": "type:MetricsCollector",
        "definition": "type MetricsCollector interface {\n    RecordArrival()\n    RecordCompletion(stamps TaskLifecycleTimestamps)\n    RecordFailure(stamps TaskLifecycleTimestamps)\n    RecordRetry()\n    Metrics() TaskMetrics\n}",
        "purpose": "Defines the interface for collecting and calculating performance and reliability metrics for the TaskManager, enabling integration with external monitoring systems.",
        "related": {
          "methods": [],
          "patterns": ["pattern:Custom Metrics"]
        },
        "interfaceContract": {
          "requiredMethods": [
            {
              "name": "RecordArrival",
              "signature": "RecordArrival()",
              "parameters": "None.",
              "returnValue": "None.",
              "sideEffects": "Increments a counter for total tasks arrived, used for arrival rate calculation."
            },
            {
              "name": "RecordCompletion",
              "signature": "RecordCompletion(stamps TaskLifecycleTimestamps)",
              "parameters": "stamps: `TaskLifecycleTimestamps` struct containing `QueuedAt`, `StartedAt`, and `FinishedAt` times.",
              "returnValue": "None.",
              "sideEffects": "Updates internal counters for completed tasks and aggregates execution/wait times for latency metrics."
            },
            {
              "name": "RecordFailure",
              "signature": "RecordFailure(stamps TaskLifecycleTimestamps)",
              "parameters": "stamps: `TaskLifecycleTimestamps` struct containing `QueuedAt`, `StartedAt`, and `FinishedAt` times.",
              "returnValue": "None.",
              "sideEffects": "Increments a counter for total tasks failed."
            },
            {
              "name": "RecordRetry",
              "signature": "RecordRetry()",
              "parameters": "None.",
              "returnValue": "None.",
              "sideEffects": "Increments a counter for total tasks retried."
            },
            {
              "name": "Metrics",
              "signature": "Metrics() TaskMetrics",
              "parameters": "None.",
              "returnValue": "Returns a `TaskMetrics` struct containing aggregated performance metrics.",
              "sideEffects": "Calculates and returns a snapshot of current metrics based on collected data."
            }
          ],
          "optionalMethods": [],
          "parameterObjectStructures": {}
        }
      },
      "TaskStats": {
        "id": "type:TaskStats",
        "definition": "type TaskStats struct {\n    BaseWorkers int32\n    ActiveWorkers int32\n    BurstWorkers int32\n    QueuedTasks int32\n    PriorityTasks int32\n    AvailableResources int32\n}",
        "purpose": "Provides a real-time snapshot of the `TaskManager`'s current operational state, including worker counts, queued tasks, and resource availability.",
        "related": {
          "methods": ["method:Stats"],
          "patterns": []
        },
        "interfaceContract": {
          "requiredMethods": [],
          "optionalMethods": [],
          "parameterObjectStructures": {
            "BaseWorkers": "int32 - Number of permanently active workers configured.",
            "ActiveWorkers": "int32 - Total number of currently active workers (base + dynamically scaled/burst workers).",
            "BurstWorkers": "int32 - Number of dynamically scaled-up workers currently active.",
            "QueuedTasks": "int32 - Number of tasks currently waiting in the main queue.",
            "PriorityTasks": "int32 - Number of tasks currently waiting in the priority queue.",
            "AvailableResources": "int32 - Number of resources currently available in the internal pool for `RunTask` operations."
          }
        }
      },
      "TaskMetrics": {
        "id": "type:TaskMetrics",
        "definition": "type TaskMetrics struct {\n    AverageExecutionTime time.Duration\n    MinExecutionTime time.Duration\n    MaxExecutionTime time.Duration\n    P95ExecutionTime time.Duration\n    P99ExecutionTime time.Duration\n    AverageWaitTime time.Duration\n    TaskArrivalRate float64\n    TaskCompletionRate float64\n    TotalTasksCompleted uint64\n    TotalTasksFailed uint64\n    TotalTasksRetried uint64\n    SuccessRate float64\n    FailureRate float64\n}",
        "purpose": "Provides a comprehensive snapshot of performance, throughput, and reliability metrics for a `TaskManager` instance, offering deep insights into the behavior of the task execution system over time.",
        "related": {
          "methods": ["method:Metrics"],
          "patterns": []
        },
        "interfaceContract": {
          "requiredMethods": [],
          "optionalMethods": [],
          "parameterObjectStructures": {
            "AverageExecutionTime": "time.Duration - Average time spent executing a task.",
            "MinExecutionTime": "time.Duration - Shortest task execution time recorded.",
            "MaxExecutionTime": "time.Duration - Longest task execution time recorded.",
            "P95ExecutionTime": "time.Duration - 95th percentile of task execution time.",
            "P99ExecutionTime": "time.Duration - 99th percentile of task execution time.",
            "AverageWaitTime": "time.Duration - Average time a task spends in a queue before execution.",
            "TaskArrivalRate": "float64 - Number of new tasks added to queues per second.",
            "TaskCompletionRate": "float64 - Number of tasks successfully completed per second.",
            "TotalTasksCompleted": "uint64 - Total count of tasks completed successfully since TaskManager started.",
            "TotalTasksFailed": "uint64 - Total count of tasks that failed permanently (all retries exhausted).",
            "TotalTasksRetried": "uint64 - Total number of times any task was re-queued for retry.",
            "SuccessRate": "float64 - Ratio of successfully completed tasks to total terminal tasks (0.0 to 1.0).",
            "FailureRate": "float64 - Ratio of failed tasks to total terminal tasks (0.0 to 1.0)."
          }
        }
      },
      "TaskLifecycleTimestamps": {
        "id": "type:TaskLifecycleTimestamps",
        "definition": "type TaskLifecycleTimestamps struct {\n    QueuedAt time.Time\n    StartedAt time.Time\n    FinishedAt time.Time\n}",
        "purpose": "Holds critical timestamps for a task's journey, used by `MetricsCollector` implementations to calculate performance metrics.",
        "related": {
          "methods": ["method:RecordCompletion", "method:RecordFailure"],
          "patterns": []
        },
        "interfaceContract": {
          "requiredMethods": [],
          "optionalMethods": [],
          "parameterObjectStructures": {
            "QueuedAt": "time.Time - Timestamp when the task was first added to a queue.",
            "StartedAt": "time.Time - Timestamp when a worker began executing the task.",
            "FinishedAt": "time.Time - Timestamp when the task execution completed (successfully or not)."
          }
        }
      }
    },
    "methods": {
      "NewTaskManager": {
        "id": "method:NewTaskManager",
        "useCase": "To initialize and start a new concurrent task management system with a pool of workers and resources.",
        "signature": "NewTaskManager[R any, E any](config Config[R]) (TaskManager[R, E], error)",
        "parameters": "config: `Config[R]` struct containing all necessary parameters for TaskManager setup, including resource creation/destruction functions, worker counts, contexts, and optional health checks/metrics.",
        "prerequisites": "`config.WorkerCount` must be > 0. `config.OnCreate` and `config.OnDestroy` must be non-nil functions. `config.Ctx` must be a valid `context.Context`.",
        "sideEffects": "Initializes `ResourcePoolSize` resources, starts `WorkerCount` base worker goroutines, and starts a burst manager goroutine (if `BurstInterval` > 0).",
        "returnValue": "Returns a `TaskManager[R, E]` interface instance ready to accept tasks, or an error if initialization fails (e.g., invalid config, resource creation failure).",
        "exceptions": [
          "errors.New(\"worker count must be positive\")",
          "errors.New(\"onCreate function is required\")",
          "errors.New(\"onDestroy function is required\")",
          "fmt.Errorf(\"failed to initialize resource pool: %w\", originalErr)"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:Config[R]", "type:TaskManager"],
          "patterns": ["pattern:Basic TaskManager Initialization"],
          "errors": []
        }
      },
      "NewCollector": {
        "id": "method:NewCollector",
        "useCase": "To create a default, in-memory implementation of the `MetricsCollector` interface. This is typically used internally by `NewTaskManager` if no custom collector is provided.",
        "signature": "NewCollector() MetricsCollector",
        "parameters": "None.",
        "prerequisites": "None.",
        "sideEffects": "Initializes a collector with `startTime` set to the current time.",
        "returnValue": "Returns a `MetricsCollector` interface instance.",
        "exceptions": [],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:MetricsCollector", "type:TaskMetrics"],
          "patterns": [],
          "errors": []
        }
      },
      "QueueTask": {
        "id": "method:QueueTask",
        "useCase": "To submit a standard asynchronous task for execution by an available worker. The caller blocks until the task completes.",
        "signature": "QueueTask[R any, E any](task func(R) (E, error)) (E, error)",
        "parameters": "task: A function `func(R) (E, error)` encapsulating the work to be done. It receives a resource of type `R` and returns a result `E` or an error.",
        "prerequisites": "The `TaskManager` must be in a running state.",
        "sideEffects": "Adds the task to the main queue. Increments `QueuedTasks` and triggers `MetricsCollector.RecordArrival()`. Upon completion, decrements `QueuedTasks` and triggers `MetricsCollector.RecordCompletion()` or `RecordFailure()`.",
        "returnValue": "Returns the result `E` from the task's execution and any error `error` that occurred.",
        "exceptions": [
          "errors.New(\"task manager is shutting down\")"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:Task"],
          "patterns": ["pattern:Queue task in a goroutine"],
          "errors": ["error:task manager is shutting down", "error:max retries exceeded"]
        }
      },
      "QueueTaskOnce": {
        "id": "method:QueueTaskOnce",
        "useCase": "To submit a standard asynchronous task that, if it fails and `CheckHealth` indicates an unhealthy worker, will NOT be re-queued by Tasker's internal retry mechanism. Useful for non-idempotent operations.",
        "signature": "QueueTaskOnce[R any, E any](task func(R) (E, error)) (E, error)",
        "parameters": "task: A function `func(R) (E, error)` encapsulating the work to be done. It receives a resource of type `R` and returns a result `E` or an error.",
        "prerequisites": "The `TaskManager` must be in a running state.",
        "sideEffects": "Adds the task to the main queue. Increments `QueuedTasks` and triggers `MetricsCollector.RecordArrival()`. Upon completion, decrements `QueuedTasks` and triggers `MetricsCollector.RecordCompletion()` or `RecordFailure()`. Sets task's internal retry counter to `MaxRetries` to prevent automatic re-queuing on health-related failures.",
        "returnValue": "Returns the result `E` from the task's execution and any error `error` that occurred.",
        "exceptions": [
          "errors.New(\"task manager is shutting down\")"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:Task"],
          "patterns": ["pattern:At-most-once task"],
          "errors": ["error:task manager is shutting down"]
        }
      },
      "QueueTaskWithPriority": {
        "id": "method:QueueTaskWithPriority",
        "useCase": "To submit a high-priority asynchronous task that will be processed before tasks in the main queue. The caller blocks until the task completes.",
        "signature": "QueueTaskWithPriority[R any, E any](task func(R) (E, error)) (E, error)",
        "parameters": "task: A function `func(R) (E, error)` encapsulating the high-priority work. It receives a resource of type `R` and returns a result `E` or an error.",
        "prerequisites": "The `TaskManager` must be in a running state.",
        "sideEffects": "Adds the task to the priority queue. Increments `PriorityTasks` and triggers `MetricsCollector.RecordArrival()`. Upon completion, decrements `PriorityTasks` and triggers `MetricsCollector.RecordCompletion()` or `RecordFailure()`.",
        "returnValue": "Returns the result `E` from the task's execution and any error `error` that occurred.",
        "exceptions": [
          "errors.New(\"task manager is shutting down\")"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:Task"],
          "patterns": ["pattern:High-priority task with result handling"],
          "errors": ["error:task manager is shutting down", "error:max retries exceeded"]
        }
      },
      "QueueTaskWithPriorityOnce": {
        "id": "method:QueueTaskWithPriorityOnce",
        "useCase": "To submit a high-priority asynchronous task that, if it fails and `CheckHealth` indicates an unhealthy worker, will NOT be re-queued by Tasker's internal retry mechanism. Useful for non-idempotent high-priority operations.",
        "signature": "QueueTaskWithPriorityOnce[R any, E any](task func(R) (E, error)) (E, error)",
        "parameters": "task: A function `func(R) (E, error)` encapsulating the high-priority work. It receives a resource of type `R` and returns a result `E` or an error.",
        "prerequisites": "The `TaskManager` must be in a running state.",
        "sideEffects": "Adds the task to the priority queue. Increments `PriorityTasks` and triggers `MetricsCollector.RecordArrival()`. Upon completion, decrements `PriorityTasks` and triggers `MetricsCollector.RecordCompletion()` or `RecordFailure()`. Sets task's internal retry counter to `MaxRetries` to prevent automatic re-queuing on health-related failures.",
        "returnValue": "Returns the result `E` from the task's execution and any error `error` that occurred.",
        "exceptions": [
          "errors.New(\"task manager is shutting down\")"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:Task"],
          "patterns": ["pattern:At-most-once task"],
          "errors": ["error:task manager is shutting down"]
        }
      },
      "RunTask": {
        "id": "method:RunTask",
        "useCase": "To execute a task immediately and synchronously, bypassing queues. It acquires a resource from the pool or creates a temporary one.",
        "signature": "RunTask[R any, E any](task func(R) (E, error)) (E, error)",
        "parameters": "task: A function `func(R) (E, error)` encapsulating the immediate work. It receives a resource of type `R` and returns a result `E` or an error.",
        "prerequisites": "The `TaskManager` must be in a running state.",
        "sideEffects": "Acquires a resource (from pool or temporary creation), executes the task, then returns/destroys the resource. Triggers `MetricsCollector.RecordArrival()`, `RecordCompletion()`, or `RecordFailure()`.",
        "returnValue": "Returns the result `E` from the task's execution and any error `error` that occurred.",
        "exceptions": [
          "errors.New(\"task manager is shutting down\")",
          "fmt.Errorf(\"failed to create temporary resource: %w\", originalErr)"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:Task"],
          "patterns": ["pattern:Execute an immediate task"],
          "errors": ["error:task manager is shutting down", "error:failed to create temporary resource"]
        }
      },
      "Stop": {
        "id": "method:Stop",
        "useCase": "To gracefully shut down the TaskManager, allowing all queued and currently executing tasks to complete before releasing resources.",
        "signature": "Stop() error",
        "parameters": "None.",
        "prerequisites": "The `TaskManager` must be in a running state.",
        "sideEffects": "Transitions manager to 'stopping' state, cancels main context, stops burst manager, drains all task queues, waits for all workers to finish, and destroys all resources via `OnDestroy`.",
        "returnValue": "Returns nil on successful graceful shutdown, or an error if the manager is already stopping or killed.",
        "exceptions": [
          "errors.New(\"task manager already stopping or killed\")"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": [],
          "patterns": ["pattern:Graceful shutdown"],
          "errors": []
        }
      },
      "Kill": {
        "id": "method:Kill",
        "useCase": "To immediately terminate the TaskManager, cancelling all running tasks, dropping all queued tasks, and releasing resources without waiting for completion.",
        "signature": "Kill() error",
        "parameters": "None.",
        "prerequisites": "The `TaskManager` must be in a running or stopping state.",
        "sideEffects": "Transitions manager to 'killed' state, cancels main context, stops burst manager, drops all queued tasks, terminates running tasks, waits for all workers to exit, and destroys all resources via `OnDestroy`.",
        "returnValue": "Returns nil on successful immediate shutdown, or an error if the manager is already killed.",
        "exceptions": [
          "errors.New(\"task manager already killed\")"
        ],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": [],
          "patterns": ["pattern:Immediate shutdown (e.g., for testing or emergency)"],
          "errors": []
        }
      },
      "Stats": {
        "id": "method:Stats",
        "useCase": "To retrieve real-time operational statistics about the TaskManager's current state.",
        "signature": "Stats() TaskStats",
        "parameters": "None.",
        "prerequisites": "None.",
        "sideEffects": "None.",
        "returnValue": "Returns a `TaskStats` struct containing current worker counts, queue sizes, and resource availability.",
        "exceptions": [],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:TaskStats"],
          "patterns": ["pattern:Polling stats and metrics"],
          "errors": []
        }
      },
      "Metrics": {
        "id": "method:Metrics",
        "useCase": "To retrieve comprehensive aggregated performance metrics about task execution, throughput, and reliability.",
        "signature": "Metrics() TaskMetrics",
        "parameters": "None.",
        "prerequisites": "None.",
        "sideEffects": "None (reads aggregated data from the internal or custom `MetricsCollector`).",
        "returnValue": "Returns a `TaskMetrics` struct containing calculated performance indicators like execution times, wait times, and success/failure rates.",
        "exceptions": [],
        "availability": "sync",
        "status": "active",
        "related": {
          "types": ["type:TaskMetrics", "type:MetricsCollector"],
          "patterns": ["pattern:Polling stats and metrics"],
          "errors": []
        }
      },
      "NewRunner": {
        "id": "method:NewRunner",
        "useCase": "Deprecated: Use `NewTaskManager` instead. This function was an alias for `NewTaskManager`.",
        "signature": "NewRunner[R any, E any](config Config[R]) (TaskManager[R, E], error)",
        "parameters": "config: `Config[R]` struct.",
        "prerequisites": "Same as `NewTaskManager`.",
        "sideEffects": "Same as `NewTaskManager`.",
        "returnValue": "Same as `NewTaskManager`.",
        "exceptions": [],
        "availability": "sync",
        "status": "deprecated",
        "related": {
          "types": ["type:Config[R]", "type:TaskManager"],
          "patterns": [],
          "errors": []
        }
      }
    },
    "decisionTrees": {
      "ChooseTaskSubmissionMethod": {
        "id": "decisionTree:ChooseTaskSubmissionMethod",
        "question": "Which task submission method should I use?",
        "logic": "IF [task is time-sensitive or critical] THEN [IF [task must execute immediately and block caller] THEN [approach: `RunTask`] ELSE [approach: `QueueTaskWithPriority`]] ELSE [approach: `QueueTask`]",
        "validationMethod": "Verify observed task latency and queueing behavior match expected outcomes (e.g., `RunTask` has minimal queue time, `QueueTaskWithPriority` beats `QueueTask` under load).",
        "related": {
          "methods": ["method:QueueTask", "method:QueueTaskWithPriority", "method:RunTask"],
          "patterns": []
        }
      },
      "HandleTaskRetries": {
        "id": "decisionTree:HandleTaskRetries",
        "question": "How should task failures be handled, especially concerning retries?",
        "logic": "IF [task failure means the worker/resource is broken and needs replacement] THEN [approach: implement `Config.CheckHealth` to return `false` for that error] ELSE [approach: treat as task-specific error only (worker continues)]; AND IF [operation is non-idempotent and should not be re-executed by the manager if worker fails] THEN [approach: use `QueueTaskOnce` or `QueueTaskWithPriorityOnce`] ELSE [approach: use standard `QueueTask` or `QueueTaskWithPriority`]",
        "validationMethod": "Observe worker replacement behavior and task retry counts in logs/metrics. Verify non-idempotent tasks are not re-queued by Tasker's internal mechanism.",
        "related": {
          "methods": ["method:QueueTaskOnce", "method:QueueTaskWithPriorityOnce"],
          "patterns": ["pattern:Custom `CheckHealth` for specific errors", "pattern:At-most-once task"]
        }
      },
      "ChooseShutdownMethod": {
        "id": "decisionTree:ChooseShutdownMethod",
        "question": "How should the TaskManager be shut down?",
        "logic": "IF [all in-flight and queued tasks must complete] THEN [approach: `manager.Stop()`] ELSE [approach: `manager.Kill()` (to immediately terminate and cancel tasks)]",
        "validationMethod": "Observe whether tasks in queues are processed (`Stop()`) or dropped (`Kill()`) and the time taken for the shutdown call to return.",
        "related": {
          "methods": ["method:Stop", "method:Kill"],
          "patterns": ["pattern:Graceful shutdown", "pattern:Immediate shutdown (e.g., for testing or emergency)"]
        }
      },
      "EnableDynamicScaling": {
        "id": "decisionTree:EnableDynamicScaling",
        "question": "Should dynamic worker scaling be enabled?",
        "logic": "IF [workload is highly variable with unpredictable spikes] THEN [approach: enable dynamic scaling by setting `Config.BurstInterval` > 0 and `Config.MaxWorkerCount` appropriately] ELSE [approach: rely on fixed `Config.WorkerCount`]",
        "validationMethod": "Monitor `Stats().BurstWorkers` and `TaskMetrics.TaskArrivalRate` vs `TaskCompletionRate` under load. Confirm workers scale up/down as expected.",
        "related": {
          "methods": ["method:Stats", "method:Metrics"],
          "patterns": ["pattern:Enable dynamic scaling"]
        }
      },
      "ConfigureLoggingAndMetrics": {
        "id": "decisionTree:ConfigureLoggingAndMetrics",
        "question": "How to integrate with custom logging or metrics systems?",
        "logic": "IF [internal Tasker logs are required for debugging or monitoring] THEN [approach: implement `tasker.Logger` and assign to `Config.Logger`] ELSE [use default no-op logger]; AND IF [detailed Tasker performance metrics need to be exposed to an external monitoring system (e.g., Prometheus)] THEN [approach: implement `tasker.MetricsCollector` and assign to `Config.Collector`] ELSE [rely on `manager.Stats()` and `manager.Metrics()` for in-process inspection]",
        "validationMethod": "Verify log output matches custom logger format. Confirm metrics are reported to external systems as expected.",
        "related": {
          "methods": ["method:Stats", "method:Metrics"],
          "patterns": ["pattern:Minimal custom logger", "pattern:Simple custom metrics collector (for total tasks completed)"]
        }
      }
    },
    "patterns": {
      "Basic TaskManager Initialization": {
        "id": "pattern:Basic TaskManager Initialization",
        "description": "Demonstrates the minimal configuration and setup required to create a functional `TaskManager` instance.",
        "example": {
          "code": "import (\n    \"context\"\n    \"log\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// Define your resource type\ntype MyResource struct{}\n\n// Implement onCreate and onDestroy functions\nfunc createMyResource() (*MyResource, error) { \n    log.Println(\"Resource created\")\n    return &MyResource{}, nil \n}\nfunc destroyMyResource(r *MyResource) error { \n    log.Println(\"Resource destroyed\")\n    return nil \n}\n\n// Configure and create TaskManager\nconfig := tasker.Config[*MyResource]{\n    OnCreate: createMyResource,\n    OnDestroy: destroyMyResource,\n    WorkerCount: 2,\n    Ctx: context.Background(),\n}\nmanager, err := tasker.NewTaskManager[*MyResource, any](config)\nif err != nil { log.Fatal(err) }\ndefer manager.Stop()",
          "validation": "Manager is successfully initialized and can accept tasks. `createMyResource` and `destroyMyResource` are called during manager lifecycle."
        },
        "related": {
          "methods": ["method:NewTaskManager"],
          "errors": ["error:worker count must be positive", "error:onCreate function is required", "error:onDestroy function is required"]
        }
      },
      "Queue task in a goroutine": {
        "id": "pattern:Queue task in a goroutine",
        "description": "Shows how to submit a task to the `TaskManager` asynchronously by wrapping the `QueueTask` call in its own goroutine, allowing the caller to continue execution without blocking.",
        "example": {
          "code": "import (\n    \"fmt\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// manager is an initialized tasker.TaskManager\n\ngo func() {\n    taskID := 1\n    result, err := manager.QueueTask(func(res *MyResource) (string, error) {\n        fmt.Printf(\"Worker processing Task %d\\n\", taskID)\n        time.Sleep(100 * time.Millisecond)\n        return fmt.Sprintf(\"Task %d completed\", taskID), nil\n    })\n    if err != nil { \n        fmt.Printf(\"Task %d failed: %v\\n\", taskID, err)\n    } else { \n        fmt.Printf(\"Task %d result: %s\\n\", taskID, result)\n    }\n}()\nfmt.Println(\"Main thread continues immediately.\")\ntime.Sleep(200 * time.Millisecond) // Allow task to run",
          "validation": "The 'Main thread continues immediately.' message appears almost instantly, and the task completion message appears later, confirming asynchronous execution."
        },
        "related": {
          "methods": ["method:QueueTask"],
          "errors": ["error:task manager is shutting down"]
        }
      },
      "High-priority task with result handling": {
        "id": "pattern:High-priority task with result handling",
        "description": "Demonstrates submitting a task to the high-priority queue and handling its outcome, ensuring it's processed ahead of standard tasks.",
        "example": {
          "code": "import (\n    \"fmt\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// manager is an initialized tasker.TaskManager\n\ngo func() {\n    fmt.Println(\"Queueing high-priority task...\")\n    result, err := manager.QueueTaskWithPriority(func(res *MyResource) (string, error) {\n        fmt.Println(\"Worker processing HIGH PRIORITY task!\")\n        time.Sleep(50 * time.Millisecond)\n        return \"Urgent report generated!\", nil\n    })\n    if err != nil { \n        fmt.Printf(\"Urgent task failed: %v\\n\", err)\n    } else { \n        fmt.Printf(\"Urgent task result: %s\\n\", result)\n    }\n}()\n\n// Simultaneously queue a lower priority task\ngo func() {\n    fmt.Println(\"Queueing normal task...\")\n    _, _ = manager.QueueTask(func(res *MyResource) (string, error) {\n        fmt.Println(\"Worker processing normal task.\")\n        time.Sleep(150 * time.Millisecond)\n        return \"Normal task done\", nil\n    })\n}()\ntime.Sleep(300 * time.Millisecond) // Allow tasks to run",
          "validation": "The 'Worker processing HIGH PRIORITY task!' and 'Urgent task result:' messages appear before or significantly faster than messages from the 'normal task', especially if both are queued close together."
        },
        "related": {
          "methods": ["method:QueueTaskWithPriority", "method:QueueTask"],
          "errors": ["error:task manager is shutting down"]
        }
      },
      "Execute an immediate task": {
        "id": "pattern:Execute an immediate task",
        "description": "Illustrates how to execute a task synchronously and immediately using `RunTask`, bypassing the queueing system, and either utilizing a pooled resource or creating a temporary one.",
        "example": {
          "code": "import (\n    \"fmt\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// manager is an initialized tasker.TaskManager\n\nfmt.Println(\"Running an immediate task...\")\nimmediateResult, immediateErr := manager.RunTask(func(res *MyResource) (string, error) {\n    fmt.Println(\"IMMEDIATE Task processing fast preview.\")\n    time.Sleep(20 * time.Millisecond)\n    return \"fast_preview.jpg\", nil\n})\nif immediateErr != nil { \n    fmt.Printf(\"Immediate Task Failed: %v\\n\", immediateErr)\n} else { \n    fmt.Printf(\"Immediate Task Completed: %s\\n\", immediateResult)\n}\nfmt.Println(\"Immediate task call returned.\")",
          "validation": "The 'IMMEDIATE Task processing...' message appears before 'Immediate task call returned.', and the call to `RunTask` itself blocks until the task is complete, confirming synchronous execution."
        },
        "related": {
          "methods": ["method:RunTask"],
          "errors": ["error:task manager is shutting down", "error:failed to create temporary resource"]
        }
      },
      "Custom `CheckHealth` for specific errors": {
        "id": "pattern:Custom `CheckHealth` for specific errors",
        "description": "Defines a custom `CheckHealth` function to categorize task errors, signaling `TaskManager` to replace a worker if the error indicates a resource or worker malfunction.",
        "example": {
          "code": "import (\n    \"errors\"\n    \"fmt\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// checkImageProcessorHealth: Custom health check.\n// If the error is \"processor_crash\", consider the worker/resource unhealthy.\nfunc checkImageProcessorHealth(err error) bool {\n    if err != nil && err.Error() == \"processor_crash\" {\n        fmt.Printf(\"WARN: Detected unhealthy error: %v. Worker will be replaced.\\n\", err)\n        return false // This error indicates an unhealthy state\n    }\n    return true // Other errors are just task failures, not worker health issues\n}\n\n// ... in Config ...\nconfig := tasker.Config[*ImageProcessor]{\n    // ...\n    CheckHealth: checkImageProcessorHealth,\n    // ...\n}\n",
          "validation": "When a task returns an error matching `\"processor_crash\"`, the `WARN` message from `checkImageProcessorHealth` appears, and a worker replacement sequence (destroy/create logs) is observed. The task might be re-queued if `MaxRetries` > 0."
        },
        "related": {
          "methods": ["method:NewTaskManager"],
          "errors": ["error:max retries exceeded"]
        }
      },
      "At-most-once task": {
        "id": "pattern:At-most-once task",
        "description": "Illustrates how to queue a task using `QueueTaskOnce` (or `QueueTaskWithPriorityOnce`) to ensure it is not re-queued by Tasker's internal retry mechanism if it causes an unhealthy worker state. Useful for non-idempotent operations.",
        "example": {
          "code": "import (\n    \"errors\"\n    \"fmt\"\n    \"math/rand\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// manager is an initialized tasker.TaskManager with checkHealth configured for \"processor_crash\"\n\ngo func() {\n    fmt.Println(\"Queueing task that might crash (at-most-once)...\")\n    _, err := manager.QueueTaskOnce(func(proc *ImageProcessor) (string, error) {\n        fmt.Println(\"Worker processing at-most-once task.\")\n        time.Sleep(100 * time.Millisecond)\n        if rand.Intn(2) == 0 { // 50% chance to simulate a crash\n            return \"\", errors.New(\"processor_crash\") // This triggers CheckHealth to return false\n        }\n        return \"processed_once_successfully\", nil\n    })\n    if err != nil { \n        fmt.Printf(\"At-most-once task finished with error: %v\\n\", err)\n    } else { \n        fmt.Println(\"At-most-once task completed successfully.\")\n    }\n}()\ntime.Sleep(300 * time.Millisecond) // Allow task to run",
          "validation": "If the task returns `\"processor_crash\"`, `CheckHealth` is triggered, the worker is replaced, but the task's final error (e.g., `processor_crash`) is immediately returned to the caller of `QueueTaskOnce` without any internal retries by Tasker."
        },
        "related": {
          "methods": ["method:QueueTaskOnce", "method:QueueTaskWithPriorityOnce"],
          "errors": ["error:max retries exceeded"]
        }
      },
      "Enable dynamic scaling": {
        "id": "pattern:Enable dynamic scaling",
        "description": "Shows how to configure Tasker to automatically scale its worker count up and down based on real-time task arrival and completion rates.",
        "example": {
          "code": "import (\n    \"context\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// Assume MyResource, createMyResource, destroyMyResource, checkMyHealth are defined\n\nconfig := tasker.Config[*MyResource]{\n    OnCreate:      createMyResource,\n    OnDestroy:     destroyMyResource,\n    WorkerCount:   2,                     // Base workers\n    MaxWorkerCount: 10,                    // Max total workers (base + burst)\n    Ctx:           context.Background(),\n    BurstInterval: 100 * time.Millisecond, // Check every 100ms for scaling\n    MaxRetries:    1,\n}\nmanager, err := tasker.NewTaskManager[*MyResource, any](config)\nif err != nil { log.Fatal(err) }\ndefer manager.Stop()",
          "validation": "Monitor `manager.Stats().BurstWorkers` and `manager.Metrics().TaskArrivalRate`/`TaskCompletionRate`. When `TaskArrivalRate` significantly exceeds `TaskCompletionRate`, `BurstWorkers` should increase. When the load subsides, `BurstWorkers` should decrease."
        },
        "related": {
          "methods": ["method:NewTaskManager", "method:Stats", "method:Metrics"],
          "errors": []
        }
      },
      "Graceful shutdown": {
        "id": "pattern:Graceful shutdown",
        "description": "Demonstrates the recommended way to shut down the `TaskManager`, ensuring all pending and in-flight tasks complete before resources are released.",
        "example": {
          "code": "import (\n    \"context\"\n    \"log\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// Assume MyResource, createMyResource, destroyMyResource are defined\n\nconfig := tasker.Config[*MyResource]{ /* ... */ Ctx: context.Background() }\nmanager, err := tasker.NewTaskManager[*MyResource, any](config)\nif err != nil { log.Fatal(err) }\n\n// Defer Stop() call ensures graceful shutdown on main exit\ndefer func() {\n    log.Println(\"Initiating graceful shutdown...\")\n    err := manager.Stop()\n    if err != nil { log.Printf(\"Error during graceful shutdown: %v\", err) }\n    log.Println(\"TaskManager gracefully shut down.\")\n}()\n\n// Queue some tasks that will run to completion\nfor i := 0; i < 3; i++ {\n    go func(id int) {\n        _, _ = manager.QueueTask(func(res *MyResource) (string, error) {\n            log.Printf(\"Task %d processing...\\n\", id)\n            time.Sleep(100 * time.Millisecond)\n            return \"Done\", nil\n        })\n    }(i)\n}\n\ntime.Sleep(50 * time.Millisecond) // Allow some tasks to queue/start\nlog.Println(\"Main function exiting, defer will call manager.Stop()...\")",
          "validation": "All 'Task X processing...' messages and 'Task X completed' messages are printed, and `manager.Stop()` returns without error, indicating all tasks finished before shutdown."
        },
        "related": {
          "methods": ["method:Stop"],
          "errors": ["error:task manager is shutting down"]
        }
      },
      "Immediate shutdown (e.g., for testing or emergency)": {
        "id": "pattern:Immediate shutdown (e.g., for testing or emergency)",
        "description": "Illustrates how to forcefully terminate the `TaskManager`, cancelling active tasks and dropping queued ones, prioritizing speed over task completion.",
        "example": {
          "code": "import (\n    \"context\"\n    \"log\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// Assume MyResource, createMyResource, destroyMyResource are defined\n\nconfig := tasker.Config[*MyResource]{ /* ... */ Ctx: context.Background() }\nmanager, err := tasker.NewTaskManager[*MyResource, any](config)\nif err != nil { log.Fatal(err) }\n\n// Queue some tasks, some of which might be dropped\nfor i := 0; i < 5; i++ {\n    go func(id int) {\n        _, taskErr := manager.QueueTask(func(res *MyResource) (string, error) {\n            log.Printf(\"Task %d processing...\\n\", id)\n            time.Sleep(500 * time.Millisecond) // Long-running task\n            return \"Done\", nil\n        })\n        if taskErr != nil { log.Printf(\"Task %d finished with error: %v\\n\", id, taskErr) }\n    }(i)\n}\n\ntime.Sleep(50 * time.Millisecond) // Allow some tasks to queue\nlog.Println(\"Initiating immediate shutdown...\")\nerr = manager.Kill()\nif err != nil { log.Printf(\"Error during immediate shutdown: %v\", err) }\nlog.Println(\"TaskManager immediately shut down.\")\ntime.Sleep(100 * time.Millisecond) // Give time for messages to print",
          "validation": "`manager.Kill()` returns quickly. Not all 'Task X processing...' messages might appear or complete, and some tasks might report errors like 'task manager is shutting down' or 'context canceled', indicating they were interrupted or dropped."
        },
        "related": {
          "methods": ["method:Kill"],
          "errors": ["error:task manager is shutting down"]
        }
      },
      "Polling stats and metrics": {
        "id": "pattern:Polling stats and metrics",
        "description": "Shows how to periodically retrieve and print the `TaskManager`'s operational statistics and performance metrics.",
        "example": {
          "code": "import (\n    \"context\"\n    \"fmt\"\n    \"time\"\n    \"github.com/asaidimu/tasker\"\n)\n\n// manager is an initialized tasker.TaskManager\n\nticker := time.NewTicker(1 * time.Second)\ndone := make(chan struct{})\ngo func() {\n    for {\n        select {\n        case <-ticker.C:\n            stats := manager.Stats()\n            metrics := manager.Metrics()\n            fmt.Printf(\"Stats: Active=%d, Queued=%d (Prio=%d), AvailRes=%d\\n\",\n                stats.ActiveWorkers, stats.QueuedTasks, stats.PriorityTasks, stats.AvailableResources)\n            fmt.Printf(\"Metrics: ArrivalRate=%.2f/s, AvgExecTime=%s, SuccessRate=%.2f\\n\",\n                metrics.TaskArrivalRate, metrics.AverageExecutionTime, metrics.SuccessRate)\n        case <-done:\n            ticker.Stop()\n            return\n        }\n    }\n}()\n\n// Simulate some work or wait\ntime.Sleep(5 * time.Second)\n\nclose(done) // Stop the monitoring goroutine",
          "validation": "Console output regularly displays updated statistics and metrics, reflecting changes in worker count, queue size, and task rates as tasks are submitted and completed."
        },
        "related": {
          "methods": ["method:Stats", "method:Metrics"],
          "errors": []
        }
      },
      "Minimal custom logger": {
        "id": "pattern:Minimal custom logger",
        "description": "Provides a basic implementation of the `tasker.Logger` interface that prints Tasker's internal log messages to the console using `log.Printf`.",
        "example": {
          "code": "import \"log\"\nimport \"github.com/asaidimu/tasker\"\n\ntype ConsoleLogger struct{}\n\nfunc (ConsoleLogger) Debugf(format string, args ...any) { log.Printf(\"DEBUG: \"+format, args...) }\nfunc (ConsoleLogger) Infof(format string, args ...any)  { log.Printf(\"INFO: \"+format, args...) }\nfunc (ConsoleLogger) Warnf(format string, args ...any)  { log.Printf(\"WARN: \"+format, args...) }\nfunc (ConsoleLogger) Errorf(format string, args ...any) { log.Printf(\"ERROR: \"+format, args...) }\n\n// Usage in Tasker Config:\n// config.Logger = ConsoleLogger{}\n",
          "validation": "When the TaskManager is running, `log.Printf` output should include Tasker's internal messages prefixed with 'DEBUG:', 'INFO:', 'WARN:', or 'ERROR:'."
        },
        "related": {
          "methods": ["method:NewTaskManager"],
          "errors": []
        }
      },
      "Simple custom metrics collector (for total tasks completed)": {
        "id": "pattern:Simple custom metrics collector (for total tasks completed)",
        "description": "An example of a minimal `tasker.MetricsCollector` implementation that only tracks the total number of completed tasks.",
        "example": {
          "code": "import \"sync/atomic\"\nimport \"github.com/asaidimu/tasker\"\n\ntype CounterCollector struct { \n    totalCompleted atomic.Uint64 \n}\n\nfunc (c *CounterCollector) RecordArrival() {} \nfunc (c *CounterCollector) RecordCompletion(s tasker.TaskLifecycleTimestamps) { \n    c.totalCompleted.Add(1) \n}\nfunc (c *CounterCollector) RecordFailure(s tasker.TaskLifecycleTimestamps) {} \nfunc (c *CounterCollector) RecordRetry() {} \n\nfunc (c *CounterCollector) Metrics() tasker.TaskMetrics { \n    return tasker.TaskMetrics{TotalTasksCompleted: c.totalCompleted.Load()} \n}\n\n// Usage in Tasker Config:\n// config.Collector = &CounterCollector{}\n",
          "validation": "After tasks are completed, retrieving `manager.Metrics().TotalTasksCompleted` (or accessing the `totalCompleted` field of `CounterCollector` directly) should reflect the correct count of completed tasks."
        },
        "related": {
          "methods": ["method:NewTaskManager", "method:Metrics"],
          "errors": []
        }
      }
    },
    "errors": {
      "task manager is shutting down": {
        "id": "error:task manager is shutting down",
        "type": "error",
        "symptoms": "Calls to `QueueTask`, `QueueTaskWithPriority`, `QueueTaskOnce`, `QueueTaskWithPriorityOnce`, or `RunTask` immediately return this specific error.",
        "properties": "None (standard Go error).",
        "scenarios": [
          {
            "trigger": "Attempting to queue a task after `manager.Stop()` has been called.",
            "example": "import \"github.com/asaidimu/tasker\"\n// manager is an initialized TaskManager\nmanager.Stop()\n_, err := manager.QueueTask(func(r *MyResource) (int, error) { return 1, nil })\n// err will be 'task manager is shutting down'",
            "reason": "The TaskManager has transitioned to a 'stopping' or 'killed' state and will no longer accept new tasks."
          },
          {
            "trigger": "Attempting to queue a task after `manager.Kill()` has been called.",
            "example": "import \"github.com/asaidimu/tasker\"\n// manager is an initialized TaskManager\nmanager.Kill()\n_, err := manager.QueueTaskWithPriority(func(r *MyResource) (int, error) { return 1, nil })\n// err will be 'task manager is shutting down'",
            "reason": "The TaskManager has been forcefully terminated and new tasks are rejected."
          }
        ],
        "diagnosis": "Check the application's shutdown logic and ensure that task submission stops before the TaskManager's shutdown methods are invoked.",
        "resolution": "Only submit tasks when the TaskManager is in an active running state. Design task producers to cease operation gracefully when a shutdown is initiated.",
        "prevention": "Implement state checks in task submission pathways to prevent queuing tasks when the manager is no longer accepting them. For instance, a global cancellation context for the entire application that triggers Tasker's shutdown should also signal task producers to stop.",
        "handlingPatterns": "```go\nresult, err := manager.QueueTask(myTaskFunc)\nif err != nil {\n    if errors.Is(err, errors.New(\"task manager is shutting down\")) {\n        log.Println(\"Cannot queue task: TaskManager is shutting down.\")\n        // Gracefully handle the un-queued task, perhaps add to a dead-letter queue or retry later\n    } else {\n        log.Printf(\"Task failed with other error: %v\", err)\n    }\n}\n```",
        "propagationBehavior": "Returned directly to the caller of the task submission method (`QueueTask`, `RunTask`, etc.)."
      },
      "worker count must be positive": {
        "id": "error:worker count must be positive",
        "type": "error",
        "symptoms": "`tasker.NewTaskManager` returns this error during initialization.",
        "properties": "None (standard Go error).",
        "scenarios": [
          {
            "trigger": "Initializing `TaskManager` with `Config.WorkerCount` set to 0.",
            "example": "import \"github.com/asaidimu/tasker\"\nconfig := tasker.Config[*CalculatorResource]{ WorkerCount: 0 /* ... other required fields ... */ }\n_, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n// err will be 'worker count must be positive'",
            "reason": "The `TaskManager` requires at least one worker to manage tasks; a count of zero is invalid."
          },
          {
            "trigger": "Initializing `TaskManager` with `Config.WorkerCount` set to a negative number (e.g., -1).",
            "example": "import \"github.com/asaidimu/tasker\"\nconfig := tasker.Config[*CalculatorResource]{ WorkerCount: -1 /* ... */ }\n_, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n// err will be 'worker count must be positive'",
            "reason": "Negative worker counts are logically impossible and indicate a configuration error."
          }
        ],
        "diagnosis": "Inspect the `WorkerCount` field in the `tasker.Config` struct passed to `NewTaskManager`.",
        "resolution": "Set `Config.WorkerCount` to a positive integer (e.g., 1, 2, or more) based on your concurrency needs.",
        "prevention": "Implement configuration validation in your application before creating the `TaskManager`.",
        "handlingPatterns": "```go\nmanager, err := tasker.NewTaskManager(config)\nif err != nil {\n    log.Fatalf(\"Failed to initialize TaskManager: Invalid worker count: %v\", err) // Typically fatal at startup\n}\n```",
        "propagationBehavior": "Returned immediately by `NewTaskManager`."
      },
      "onCreate function is required": {
        "id": "error:onCreate function is required",
        "type": "error",
        "symptoms": "`tasker.NewTaskManager` returns this error during initialization.",
        "properties": "None (standard Go error).",
        "scenarios": [
          {
            "trigger": "Initializing `TaskManager` with `Config.OnCreate` set to `nil`.",
            "example": "import \"github.com/asaidimu/tasker\"\nconfig := tasker.Config[*CalculatorResource]{ OnCreate: nil /* ... other required fields ... */ }\n_, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n// err will be 'onCreate function is required'",
            "reason": "The `TaskManager` must know how to create new instances of the resource `R` for its workers and internal pool."
          }
        ],
        "diagnosis": "Check the `OnCreate` field in the `tasker.Config` struct.",
        "resolution": "Provide a non-nil function for `Config.OnCreate` that implements the logic for creating your resource `R`.",
        "prevention": "Ensure `OnCreate` is always assigned a valid function during configuration.",
        "handlingPatterns": "Same as `worker count must be positive` - typically a fatal error during application startup.",
        "propagationBehavior": "Returned immediately by `NewTaskManager`."
      },
      "onDestroy function is required": {
        "id": "error:onDestroy function is required",
        "type": "error",
        "symptoms": "`tasker.NewTaskManager` returns this error during initialization.",
        "properties": "None (standard Go error).",
        "scenarios": [
          {
            "trigger": "Initializing `TaskManager` with `Config.OnDestroy` set to `nil`.",
            "example": "import \"github.com/asaidimu/tasker\"\nconfig := tasker.Config[*CalculatorResource]{ OnDestroy: nil /* ... other required fields ... */ }\n_, err := tasker.NewTaskManager[*CalculatorResource, int](config)\n// err will be 'onDestroy function is required'",
            "reason": "The `TaskManager` must know how to properly clean up resources when workers are shut down or resources are no longer needed."
          }
        ],
        "diagnosis": "Check the `OnDestroy` field in the `tasker.Config` struct.",
        "resolution": "Provide a non-nil function for `Config.OnDestroy` that implements the logic for destroying/cleaning up your resource `R`.",
        "prevention": "Ensure `OnDestroy` is always assigned a valid function during configuration.",
        "handlingPatterns": "Same as `worker count must be positive` - typically a fatal error during application startup.",
        "propagationBehavior": "Returned immediately by `NewTaskManager`."
      },
      "failed to create temporary resource": {
        "id": "error:failed to create temporary resource",
        "type": "Go wrapped error",
        "symptoms": "`RunTask` returns an error with this message, typically wrapping an underlying error from your `OnCreate` function.",
        "properties": "The original error from `Config.OnCreate` is wrapped. Can be unwrapped using `errors.Unwrap(err)`.",
        "scenarios": [
          {
            "trigger": "`RunTask` is called, and the resource pool is either empty or `ResourcePoolSize` is 0, forcing a temporary resource creation via `Config.OnCreate`, which then fails.",
            "example": "import \"errors\"\nimport \"github.com/asaidimu/tasker\"\n// If createImageProcessor returns an error during temporary creation\nmanager.RunTask(func(proc *ImageProcessor) (string, error) { return \"ok\", nil })\n// This call returns error like 'failed to create temporary resource: connection refused'",
            "reason": "The `OnCreate` function failed to provision a resource when `RunTask` needed a new one. This could be due to network issues, credential problems, or external service unavailability."
          }
        ],
        "diagnosis": "Unwrap the error to inspect the root cause. This points to a problem in your `Config.OnCreate` function or its external dependencies.",
        "resolution": "Address the underlying issue causing `OnCreate` to fail. This might involve ensuring network connectivity, correcting API keys, or handling transient external service errors within `OnCreate` itself.",
        "prevention": "Make your `OnCreate` function robust by adding retry logic or specific error handling for common transient issues with external dependencies. Consider increasing `ResourcePoolSize` to reduce the frequency of temporary resource creation if `OnCreate` is slow or prone to failures.",
        "handlingPatterns": "```go\n_, err := manager.RunTask(myImmediateTaskFunc)\nif err != nil {\n    if errors.Is(err, errors.New(\"failed to create temporary resource\")) {\n        log.Printf(\"Immediate task aborted due to resource provisioning error: %v\", errors.Unwrap(err))\n        // Handle by notifying user, logging, or retrying at application level\n    } else {\n        log.Printf(\"Immediate task failed for another reason: %v\", err)\n    }\n}\n```",
        "propagationBehavior": "Returned directly to the caller of `RunTask`."
      },
      "max retries exceeded": {
        "id": "error:max retries exceeded",
        "type": "Go wrapped error",
        "symptoms": "A `QueueTask` or `QueueTaskWithPriority` call eventually returns this error after internal retries. The wrapped error is the one that repeatedly caused the `CheckHealth` function to return `false`.",
        "properties": "The original error that triggered the unhealthy state is wrapped. Can be unwrapped using `errors.Unwrap(err)`.",
        "scenarios": [
          {
            "trigger": "A task repeatedly fails, and for each failure, `Config.CheckHealth` returns `false` (indicating an unhealthy worker/resource). After `Config.MaxRetries` attempts, the task is finally marked as failed.",
            "example": "import \"errors\"\nimport \"github.com/asaidimu/tasker\"\n// Assume CheckHealth returns false for 'processor_crash'\n_, err := manager.QueueTask(func(proc *ImageProcessor) (string, error) {\n    return \"\", errors.New(\"processor_crash\") // This task will be retried MaxRetries times\n})\n// After MaxRetries attempts, err will be 'max retries exceeded: processor_crash'",
            "reason": "The task consistently causes an unhealthy worker/resource state, and Tasker has exhausted all configured attempts to run it on a new worker."
          }
        ],
        "diagnosis": "The true problem lies in the original error that repeatedly triggers `CheckHealth` to return `false`. Debug the task logic and its interaction with the resource to understand why it's failing persistently.",
        "resolution": "Fix the underlying cause of the persistent unhealthy failures. This might require addressing issues in external services, modifying task input, or improving the robustness of your resource (`R`). Adjust `Config.MaxRetries` if the issue is truly transient but requires more attempts.",
        "prevention": "Improve `CheckHealth` precision (only return `false` for genuine worker/resource issues), enhance resource robustness, or ensure task idempotency so that retries are always safe.",
        "handlingPatterns": "```go\n_, err := manager.QueueTask(myProblematicTaskFunc)\nif err != nil {\n    if errors.Is(err, errors.New(\"max retries exceeded\")) {\n        originalErr := errors.Unwrap(err)\n        log.Printf(\"Task failed permanently after retries. Original cause: %v\", originalErr)\n        // This indicates a critical failure. Alert monitoring, dead-letter task, or trigger human intervention.\n    } else {\n        log.Printf(\"Task failed with other error: %v\", err)\n    }\n}\n```",
        "propagationBehavior": "Returned to the caller of `QueueTask` or `QueueTaskWithPriority`. Tasks submitted with `QueueTaskOnce` or `QueueTaskWithPriorityOnce` will return the original unhealthy error directly without wrapping in \"max retries exceeded\" if they fail due to an unhealthy worker."
      }
    }
  }
}