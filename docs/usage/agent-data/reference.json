{
  "system": {
    "name": "tasker",
    "language": "Go",
    "description": "A robust and flexible Go library for concurrent task management with dynamic worker scaling, priority queuing, and resource lifecycle control.",
    "keyFeatures": [
      "Concurrent Task Execution",
      "Generic Resource Management",
      "Rate-Based Dynamic Worker Scaling",
      "Priority Queues",
      "Immediate Task Execution with Resource Pooling",
      "Customizable Health Checks & Retries",
      "\"At-Most-Once\" Task Execution",
      "Graceful & Immediate Shutdown",
      "Real-time Performance Metrics",
      "Custom Logging"
    ]
  },
  "dependencies": {
    "external": [
      {
        "name": "github.com/asaidimu/tasker/v2",
        "purpose": "The core Go module for concurrent task management.",
        "interfaces": [],
        "installation": "go get github.com/asaidimu/tasker/v2",
        "version": ""
      }
    ],
    "peer": [
      {
        "name": "context",
        "reason": "Required for `context.Context` propagation to tasks for cancellation and deadlines.",
        "version": "Go Standard Library"
      },
      {
        "name": "errors",
        "reason": "Required for creating and handling errors.",
        "version": "Go Standard Library"
      },
      {
        "name": "fmt",
        "reason": "Required for formatted I/O (e.g., printing messages).",
        "version": "Go Standard Library"
      },
      {
        "name": "log",
        "reason": "Standard logging utility, though `tasker.Logger` allows custom integration.",
        "version": "Go Standard Library"
      },
      {
        "name": "math",
        "reason": "Used for mathematical operations, e.g., in metrics calculations for ceil/floor.",
        "version": "Go Standard Library"
      },
      {
        "name": "math/rand",
        "reason": "Used in examples for simulating random outcomes.",
        "version": "Go Standard Library"
      },
      {
        "name": "sync",
        "reason": "Required for synchronization primitives like `sync.Mutex` and `sync.WaitGroup`.",
        "version": "Go Standard Library"
      },
      {
        "name": "sync/atomic",
        "reason": "Required for atomic operations on shared counters.",
        "version": "Go Standard Library"
      },
      {
        "name": "time",
        "reason": "Required for time-related operations, durations, and timestamps.",
        "version": "Go Standard Library"
      }
    ]
  },
  "integration": {
    "environmentRequirements": "Go **1.24.3** or higher is required for building and running applications using `tasker`. The library is platform-agnostic and should run on any operating system supported by Go.",
    "initializationPatterns": [
      {
        "title": "Basic TaskManager Initialization",
        "description": "The most common way to initialize `tasker` is by providing `OnCreate`, `OnDestroy`, `WorkerCount`, and a `context.Context`.",
        "codeExample": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\n// MyResource represents a resource for tasks\ntype MyResource struct{ ID int }\n\n// onCreate simulates resource allocation\nfunc createMyResource() (*MyResource, error) {\n\tfmt.Println(\"INFO: Creating MyResource\")\n\treturn &MyResource{ID: time.Now().Nanosecond()}, nil\n}\n\n// onDestroy simulates resource deallocation\nfunc destroyMyResource(r *MyResource) error {\n\tfmt.Println(\"INFO: Destroying MyResource\", r.ID)\n\treturn nil\n}\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tconfig := tasker.Config[*MyResource]{\n\t\tOnCreate:    createMyResource,\n\t\tOnDestroy:   destroyMyResource,\n\t\tWorkerCount: 2, // Two base workers\n\t\tCtx:         ctx,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*MyResource, string](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\tfmt.Println(\"TaskManager initialized and running.\")\n\t// Application logic here\n\ttime.Sleep(500 * time.Millisecond)\n}\n"
      },
      {
        "title": "Custom Logger and Metrics Integration",
        "description": "Integrate custom logging and metrics collection by implementing the `tasker.Logger` and `tasker.MetricsCollector` interfaces.",
        "codeExample": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\n// MyCustomLogger implements tasker.Logger\ntype MyCustomLogger struct{}\n\nfunc (l *MyCustomLogger) Debugf(format string, args ...any) { log.Printf(\"[DEBUG] \"+format, args...) }\nfunc (l *MyCustomLogger) Infof(format string, args ...any)  { log.Printf(\"[INFO] \"+format, args...) }\nfunc (l *MyCustomLogger) Warnf(format string, args ...any)  { log.Printf(\"[WARN] \"+format, args...) }\nfunc (l *MyCustomLogger) Errorf(format string, args ...any) { log.Printf(\"[ERROR] \"+format, args...) }\n\n// MyCustomMetricsCollector implements tasker.MetricsCollector\ntype MyCustomMetricsCollector struct{}\n\nfunc (c *MyCustomMetricsCollector) RecordArrival()       { fmt.Println(\"Metric: Task Arrived\") }\nfunc (c *MyCustomMetricsCollector) RecordCompletion(s tasker.TaskLifecycleTimestamps) { fmt.Printf(\"Metric: Task Completed (exec: %v)\\n\", s.FinishedAt.Sub(s.StartedAt)) }\nfunc (c *MyCustomMetricsCollector) RecordFailure(s tasker.TaskLifecycleTimestamps)    { fmt.Println(\"Metric: Task Failed\") }\nfunc (c *MyCustomMetricsCollector) RecordRetry()         { fmt.Println(\"Metric: Task Retried\") }\nfunc (c *MyCustomMetricsCollector) Metrics() tasker.TaskMetrics { return tasker.TaskMetrics{} }\n\n// MyResource and lifecycle functions (same as above)\ntype MyResource struct{ ID int }\nfunc createMyResource() (*MyResource, error) { fmt.Println(\"INFO: Creating MyResource\"); return &MyResource{ID: time.Now().Nanosecond()}, nil }\nfunc destroyMyResource(r *MyResource) error { fmt.Println(\"INFO: Destroying MyResource\", r.ID); return nil }\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tconfig := tasker.Config[*MyResource]{\n\t\tOnCreate:    createMyResource,\n\t\tOnDestroy:   destroyMyResource,\n\t\tWorkerCount: 1,\n\t\tCtx:         ctx,\n\t\tLogger:      &MyCustomLogger{}, // Provide custom logger\n\t\tCollector:   &MyCustomMetricsCollector{}, // Provide custom collector\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*MyResource, string](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\t_, _ = manager.QueueTask(func(ctx context.Context, res *MyResource) (string, error) {\n\t\tfmt.Printf(\"Worker %d processing a task with custom logging/metrics\\n\", res.ID)\n\t\treturn \"done\", nil\n\t})\n\n\ttime.Sleep(100 * time.Millisecond)\n}\n"
      }
    ],
    "commonPitfalls": [
      {
        "issue": "Not handling `context.Context` cancellation within task functions.",
        "solution": "Tasks should periodically check `ctx.Done()` or use `select { case <-ctx.Done(): return nil, ctx.Err() ... }` to respond to manager shutdowns or timeouts."
      },
      {
        "issue": "Blocking indefinitely in `OnCreate`, `OnDestroy`, or task functions.",
        "solution": "Ensure these functions complete in a timely manner. Long-running initialization/cleanup should be handled asynchronously or outside `tasker`'s direct control if it risks blocking the pool."
      },
      {
        "issue": "Ignoring errors returned by `QueueTask` or `RunTask`.",
        "solution": "Always check the `error` return value to handle potential submission failures (e.g., manager shutting down) or task execution errors."
      }
    ],
    "lifecycleDependencies": "The `TaskManager` relies on a parent `context.Context` for its overall lifecycle. Cancelling this context (passed via `Config.Ctx`) initiates a graceful shutdown (`Stop()` behavior). Users must ensure the `TaskManager`'s `Stop()` or `Kill()` method is called when the application exits to properly release resources and wait for tasks (for `Stop()`). Resource creation (`OnCreate`) and destruction (`OnDestroy`) are directly tied to worker lifecycles and the `RunTask` operation. Workers will not start if `OnCreate` fails, and resources are destroyed when workers exit or the manager shuts down."
  },
  "interfaces": {
    "Logger": {
      "id": "interface:Logger",
      "definition": "type Logger interface {\n\tDebugf(format string, args ...any)\n\tInfof(format string, args ...any)\n\tWarnf(format string, args ...any)\n\tErrorf(format string, args ...any)\n}",
      "purpose": "Defines the interface for logging messages from the TaskManager, allowing users to integrate their own preferred logging library.",
      "related": {
        "methods": [],
        "types": [
          "type:Config"
        ],
        "patterns": [
          "pattern:Custom_logging_integration"
        ]
      },
      "interfaceContract": {
        "requiredMethods": [
          {
            "name": "Debugf",
            "signature": "Debugf(format string, args ...any)",
            "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
            "returnValue": "None",
            "sideEffects": "Should log a message at debug level."
          },
          {
            "name": "Infof",
            "signature": "Infof(format string, args ...any)",
            "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
            "returnValue": "None",
            "sideEffects": "Should log a message at info level."
          },
          {
            "name": "Warnf",
            "signature": "Warnf(format string, args ...any)",
            "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
            "returnValue": "None",
            "sideEffects": "Should log a message at warning level."
          },
          {
            "name": "Errorf",
            "signature": "Errorf(format string, args ...any)",
            "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
            "returnValue": "None",
            "sideEffects": "Should log a message at error level."
          }
        ],
        "optionalMethods": [],
        "parameterObjectStructures": {}
      }
    },
    "MetricsCollector": {
      "id": "interface:MetricsCollector",
      "definition": "type MetricsCollector interface {\n\tRecordArrival()\n\tRecordCompletion(stamps TaskLifecycleTimestamps)\n\tRecordFailure(stamps TaskLifecycleTimestamps)\n\tRecordRetry()\n\tMetrics() TaskMetrics\n}",
      "purpose": "Defines the interface for collecting and calculating performance and reliability metrics for the TaskManager. Implementations process task lifecycle events and aggregate data.",
      "related": {
        "methods": [],
        "types": [
          "type:Config",
          "type:TaskMetrics",
          "type:TaskLifecycleTimestamps"
        ],
        "patterns": [
          "pattern:Custom_metrics_integration"
        ]
      },
      "interfaceContract": {
        "requiredMethods": [
          {
            "name": "RecordArrival",
            "signature": "RecordArrival()",
            "parameters": "None",
            "returnValue": "None",
            "sideEffects": "Increments internal counter for task arrivals."
          },
          {
            "name": "RecordCompletion",
            "signature": "RecordCompletion(stamps TaskLifecycleTimestamps)",
            "parameters": "stamps: TaskLifecycleTimestamps (timing information for the completed task)",
            "returnValue": "None",
            "sideEffects": "Updates internal metrics for total tasks completed, execution time, wait time, and min/max/percentile execution times."
          },
          {
            "name": "RecordFailure",
            "signature": "RecordFailure(stamps TaskLifecycleTimestamps)",
            "parameters": "stamps: TaskLifecycleTimestamps (timing information for the failed task)",
            "returnValue": "None",
            "sideEffects": "Increments internal counter for total tasks failed."
          },
          {
            "name": "RecordRetry",
            "signature": "RecordRetry()",
            "parameters": "None",
            "returnValue": "None",
            "sideEffects": "Increments internal counter for total tasks retried."
          },
          {
            "name": "Metrics",
            "signature": "Metrics() TaskMetrics",
            "parameters": "None",
            "returnValue": "TaskMetrics (a snapshot of the currently aggregated performance metrics)",
            "sideEffects": "None (reads internal state, does not modify)"
          }
        ],
        "optionalMethods": [],
        "parameterObjectStructures": {}
      }
    },
    "TaskManager": {
      "id": "interface:TaskManager",
      "definition": "type TaskManager[R any, E any] interface { ... }",
      "purpose": "Defines the interface for managing asynchronous and synchronous task execution within a pool of workers and resources.",
      "related": {
        "methods": [
          "method:NewTaskManager"
        ],
        "types": [
          "type:Config",
          "type:TaskStats",
          "type:TaskMetrics"
        ],
        "patterns": []
      },
      "interfaceContract": {
        "requiredMethods": [
          {
            "name": "QueueTask",
            "signature": "QueueTask(task func(context.Context, R) (E, error)) (E, error)",
            "parameters": "task: func(context.Context, R) (E, error) (the function representing the task's logic)",
            "returnValue": "E (the task's result), error (any error encountered during execution or submission)",
            "sideEffects": "Adds task to main queue, increments `queuedTasks` metric, initiates task execution, decrements `queuedTasks` on processing. Blocks caller until task completes."
          },
          {
            "name": "QueueTaskWithCallback",
            "signature": "QueueTaskWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
            "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error) (function to be invoked with task's result/error)",
            "returnValue": "None",
            "sideEffects": "Adds task to main queue, increments `queuedTasks` metric, initiates task execution. Returns immediately. `callback` is invoked asynchronously."
          },
          {
            "name": "QueueTaskAsync",
            "signature": "QueueTaskAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "<-chan E (channel for task result), <-chan error (channel for task error)",
            "sideEffects": "Adds task to main queue, increments `queuedTasks` metric, initiates task execution. Returns immediately. Result/error are sent to provided channels."
          },
          {
            "name": "RunTask",
            "signature": "RunTask(task func(context.Context, R) (E, error)) (E, error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "E (the task's result), error (any error encountered during execution)",
            "sideEffects": "Acquires resource (from pool or temporary), executes task, releases/destroys resource. Blocks caller until task completes."
          },
          {
            "name": "QueueTaskWithPriority",
            "signature": "QueueTaskWithPriority(task func(context.Context, R) (E, error)) (E, error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "E (the task's result), error (any error encountered during execution or submission)",
            "sideEffects": "Adds task to priority queue, increments `priorityTasks` metric, initiates task execution, decrements `priorityTasks` on processing. Blocks caller until task completes."
          },
          {
            "name": "QueueTaskWithPriorityWithCallback",
            "signature": "QueueTaskWithPriorityWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
            "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error)",
            "returnValue": "None",
            "sideEffects": "Adds task to priority queue, increments `priorityTasks` metric, initiates task execution. Returns immediately. `callback` is invoked asynchronously."
          },
          {
            "name": "QueueTaskWithPriorityAsync",
            "signature": "QueueTaskWithPriorityAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "<-chan E (channel for task result), <-chan error (channel for task error)",
            "sideEffects": "Adds task to priority queue, increments `priorityTasks` metric, initiates task execution. Returns immediately. Result/error are sent to provided channels."
          },
          {
            "name": "QueueTaskOnce",
            "signature": "QueueTaskOnce(task func(context.Context, R) (E, error)) (E, error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "E (the task's result), error (any error encountered during execution or submission)",
            "sideEffects": "Adds task to main queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Blocks caller until task completes."
          },
          {
            "name": "QueueTaskOnceWithCallback",
            "signature": "QueueTaskOnceWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
            "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error)",
            "returnValue": "None",
            "sideEffects": "Adds task to main queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. `callback` is invoked asynchronously."
          },
          {
            "name": "QueueTaskOnceAsync",
            "signature": "QueueTaskOnceAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "<-chan E (channel for task result), <-chan error (channel for task error)",
            "sideEffects": "Adds task to main queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. Result/error are sent to provided channels."
          },
          {
            "name": "QueueTaskWithPriorityOnce",
            "signature": "QueueTaskWithPriorityOnce(task func(context.Context, R) (E, error)) (E, error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "E (the task's result), error (any error encountered during execution or submission)",
            "sideEffects": "Adds task to priority queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Blocks caller until task completes."
          },
          {
            "name": "QueueTaskWithPriorityOnceWithCallback",
            "signature": "QueueTaskWithPriorityOnceWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
            "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error)",
            "returnValue": "None",
            "sideEffects": "Adds task to priority queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. `callback` is invoked asynchronously."
          },
          {
            "name": "QueueTaskWithPriorityOnceAsync",
            "signature": "QueueTaskWithPriorityOnceAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
            "parameters": "task: func(context.Context, R) (E, error)",
            "returnValue": "<-chan E (channel for task result), <-chan error (channel for task error)",
            "sideEffects": "Adds task to priority queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. Result/error are sent to provided channels."
          },
          {
            "name": "Stop",
            "signature": "Stop() error",
            "parameters": "None",
            "returnValue": "error (nil on successful graceful shutdown, an error if manager is already stopping/killed)",
            "sideEffects": "Sets shutdown state to 'stopping', stops accepting new tasks, cancels main context, signals workers to drain queues, waits for all goroutines to complete, drains resource pool."
          },
          {
            "name": "Kill",
            "signature": "Kill() error",
            "parameters": "None",
            "returnValue": "error (nil on successful immediate shutdown, an error if manager is already killed)",
            "sideEffects": "Sets shutdown state to 'killed', stops accepting new tasks, cancels main context, signals workers to terminate immediately (dropping queued tasks), waits for all goroutines to complete, drains resource pool."
          },
          {
            "name": "Stats",
            "signature": "Stats() TaskStats",
            "parameters": "None",
            "returnValue": "TaskStats (current operational state snapshot)",
            "sideEffects": "None (reads atomic counters, does not modify state)"
          },
          {
            "name": "Metrics",
            "signature": "Metrics() TaskMetrics",
            "parameters": "None",
            "returnValue": "TaskMetrics (comprehensive performance metrics snapshot)",
            "sideEffects": "None (reads metrics collector state, does not modify)"
          }
        ],
        "optionalMethods": [],
        "parameterObjectStructures": {}
      }
    }
  },
  "types": {
    "Config": {
      "id": "type:Config",
      "definition": "type Config[R any] struct {\n\tOnCreate func() (R, error)\n\tOnDestroy func(R) error\n\tWorkerCount int\n\tCtx context.Context\n\tCheckHealth func(error) bool\n\tMaxWorkerCount int\n\tBurstInterval time.Duration\n\tMaxRetries int\n\tResourcePoolSize int\n\tLogger Logger\n\tCollector MetricsCollector\n\tDeprecatedBurstTaskThreshold int\n\tDeprecatedBurstWorkerCount int\n}",
      "purpose": "Holds configuration parameters for creating a new TaskManager instance, controlling worker behavior, resource management, and scaling policies.",
      "related": {
        "interfaces": [
          "interface:TaskManager",
          "interface:Logger",
          "interface:MetricsCollector"
        ],
        "methods": [
          "method:NewTaskManager"
        ],
        "patterns": []
      },
      "interfaceContract": {
        "requiredMethods": [],
        "optionalMethods": [],
        "parameterObjectStructures": {
          "Config": "```go\ntype Config[R any] struct {\n\tOnCreate func() (R, error) // Required: Function to create a new resource.\n\tOnDestroy func(R) error     // Required: Function to destroy a resource.\n\tWorkerCount int             // Required: Initial and minimum number of base workers (must be > 0).\n\tCtx context.Context         // Required: Parent context for the TaskManager.\n\tCheckHealth func(error) bool // Optional: Custom health check function. Defaults to always true.\n\tMaxWorkerCount int          // Optional: Maximum total workers (base + burst). Defaults to WorkerCount * 2.\n\tBurstInterval time.Duration  // Optional: Frequency for burst manager checks. Defaults to 100ms. Set to 0 to disable bursting.\n\tMaxRetries int               // Optional: Max retries for tasks on unhealthy errors. Defaults to 3.\n\tResourcePoolSize int         // Optional: Number of resources to pre-allocate for RunTask. Defaults to WorkerCount.\n\tLogger Logger                 // Optional: Custom logger interface.\n\tCollector MetricsCollector   // Optional: Custom metrics collector.\n\tBurstTaskThreshold int       // Deprecated: No longer used for rate-based scaling.\n\tBurstWorkerCount int         // Deprecated: No longer used for rate-based scaling.\n}\n```"
        }
      }
    },
    "TaskStats": {
      "id": "type:TaskStats",
      "definition": "type TaskStats struct {\n\tBaseWorkers        int32\n\tActiveWorkers      int32\n\tBurstWorkers       int32\n\tQueuedTasks        int32\n\tPriorityTasks      int32\n\tAvailableResources int32\n}",
      "purpose": "Provides insight into the task manager's current operational state.",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:Stats"
        ],
        "patterns": [
          "pattern:Get_live_stats"
        ]
      },
      "interfaceContract": {
        "requiredMethods": [],
        "optionalMethods": [],
        "parameterObjectStructures": {
          "TaskStats": "```go\ntype TaskStats struct {\n\tBaseWorkers        int32 // Number of permanently active workers.\n\tActiveWorkers      int32 // Total number of currently active workers (base + burst).\n\tBurstWorkers       int32 // Number of dynamically scaled-up workers.\n\tQueuedTasks        int32 // Number of tasks currently in the main queue.\n\tPriorityTasks      int32 // Number of tasks currently in the priority queue.\n\tAvailableResources int32 // Number of resources currently available in the resource pool for RunTask.\n}\n```"
        }
      }
    },
    "TaskMetrics": {
      "id": "type:TaskMetrics",
      "definition": "type TaskMetrics struct {\n\tAverageExecutionTime time.Duration\n\tMinExecutionTime time.Duration\n\tMaxExecutionTime time.Duration\n\tP95ExecutionTime time.Duration\n\tP99ExecutionTime time.Duration\n\tAverageWaitTime time.Duration\n\tTaskArrivalRate float64\n\tTaskCompletionRate float64\n\tTotalTasksCompleted uint64\n\tTotalTasksFailed uint64\n\tTotalTasksRetried uint64\n\tSuccessRate float64\n\tFailureRate float64\n}",
      "purpose": "Provides a comprehensive snapshot of performance, throughput, and reliability metrics for a TaskManager instance.",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "methods": [
          "method:Metrics"
        ],
        "patterns": [
          "pattern:Get_performance_metrics"
        ]
      },
      "interfaceContract": {
        "requiredMethods": [],
        "optionalMethods": [],
        "parameterObjectStructures": {
          "TaskMetrics": "```go\ntype TaskMetrics struct {\n\tAverageExecutionTime time.Duration // Average time spent executing a task.\n\tMinExecutionTime time.Duration    // Shortest execution time recorded.\n\tMaxExecutionTime time.Duration    // Longest execution time recorded.\n\tP95ExecutionTime time.Duration    // 95th percentile of task execution time.\n\tP99ExecutionTime time.Duration    // 99th percentile of task execution time.\n\tAverageWaitTime time.Duration     // Average time a task spends in a queue.\n\tTaskArrivalRate float64           // New tasks added to queues per second.\n\tTaskCompletionRate float64        // Tasks successfully completed per second.\n\tTotalTasksCompleted uint64       // Total tasks completed successfully since manager started.\n\tTotalTasksFailed uint64          // Total tasks failed after all retries.\n\tTotalTasksRetried uint64         // Total times any task has been retried.\n\tSuccessRate float64               // Ratio of completed to total terminal tasks.\n\tFailureRate float64               // Ratio of failed to total terminal tasks.\n}\n```"
        }
      }
    },
    "TaskLifecycleTimestamps": {
      "id": "type:TaskLifecycleTimestamps",
      "definition": "type TaskLifecycleTimestamps struct {\n\tQueuedAt time.Time\n\tStartedAt time.Time\n\tFinishedAt time.Time\n}",
      "purpose": "Holds critical timestamps from a task's journey, used by `MetricsCollector` to calculate performance metrics.",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "methods": [
          "method:RecordCompletion",
          "method:RecordFailure"
        ],
        "patterns": []
      },
      "interfaceContract": {
        "requiredMethods": [],
        "optionalMethods": [],
        "parameterObjectStructures": {
          "TaskLifecycleTimestamps": "```go\ntype TaskLifecycleTimestamps struct {\n\tQueuedAt time.Time   // Time when the task was first added to a queue.\n\tStartedAt time.Time  // Time when a worker began executing the task.\n\tFinishedAt time.Time // Time when the task execution completed (successfully or not).\n}\n```"
        }
      }
    }
  },
  "methods": {
    "NewRunner": {
      "id": "method:NewRunner",
      "useCase": "Deprecated: Use `NewTaskManager` instead.",
      "signature": "NewRunner[R any, E any](config Config[R]) (TaskManager[R, E], error)",
      "parameters": "config: Config[R] (configuration parameters for the task manager)",
      "prerequisites": "None.",
      "sideEffects": "Initializes and starts worker goroutines, resource pool, and burst manager.",
      "returnValue": "TaskManager[R, E] (an instance of the task manager), error (if initialization fails)",
      "exceptions": [
        "errors.New(\"worker count must be positive\")",
        "errors.New(\"onCreate function is required\")",
        "errors.New(\"onDestroy function is required\")",
        "fmt.Errorf(\"failed to initialize resource pool: %w\", err)"
      ],
      "availability": "sync",
      "status": "deprecated",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Config"
        ],
        "patterns": []
      }
    },
    "NewTaskManager": {
      "id": "method:NewTaskManager",
      "useCase": "To create and initialize a new `TaskManager` instance for managing concurrent tasks with custom resources.",
      "signature": "NewTaskManager[R any, E any](config Config[R]) (TaskManager[R, E], error)",
      "parameters": "config: Config[R] (configuration parameters for the task manager, including resource lifecycle, worker counts, health checks, and scaling)",
      "prerequisites": "The `Config` struct must be valid: `WorkerCount` > 0, `OnCreate` and `OnDestroy` functions must be provided.",
      "sideEffects": "Initializes the resource pool, starts the specified number of base worker goroutines, and begins the burst manager's dynamic scaling routine. Resources are created via `OnCreate`.",
      "returnValue": "TaskManager[R, E] (an instance of the task manager interface, ready to accept tasks), error (if initialization fails, e.g., invalid config or initial resource creation error)",
      "exceptions": [
        "errors.New(\"worker count must be positive\")",
        "errors.New(\"onCreate function is required\")",
        "errors.New(\"onDestroy function is required\")",
        "fmt.Errorf(\"failed to initialize resource pool: %w\", err)"
      ],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Config"
        ],
        "patterns": [
          "pattern:Basic_tasker_setup"
        ]
      }
    },
    "QueueTask": {
      "id": "method:QueueTask",
      "useCase": "To submit a standard asynchronous task to the manager's main queue and wait for its completion. This is suitable for background processing where immediate, non-blocking submission is not a strict requirement.",
      "signature": "QueueTask(task func(context.Context, R) (E, error)) (E, error)",
      "parameters": "task: func(context.Context, R) (E, error) (the function defining the task's logic. It receives a `context.Context` for cancellation and a resource `R`.)",
      "prerequisites": "The `TaskManager` must be in a running state. The task function should ideally be idempotent if retries are enabled and `CheckHealth` can deem a worker unhealthy.",
      "sideEffects": "Adds the task to the main queue. Increments `queuedTasks` and `totalTasksArrived` metrics. An available worker will eventually pick it up, execute it, and potentially retry it if `CheckHealth` indicates an unhealthy worker and `MaxRetries` allows. Blocks the calling goroutine until the task finishes execution or manager shuts down.",
      "returnValue": "E (the result of the task's execution), error (nil if successful, or an error if the task fails, is cancelled, or manager shuts down)",
      "exceptions": [
        "errors.New(\"task manager is shutting down\")",
        "fmt.Errorf(\"priority queue full, task requeue failed: %w\", err)"
      ],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [
          "pattern:Queue_and_wait"
        ],
        "errors": [
          "error:task manager is shutting down",
          "error:priority queue full, task requeue failed"
        ]
      }
    },
    "QueueTaskWithCallback": {
      "id": "method:QueueTaskWithCallback",
      "useCase": "To submit a standard asynchronous task to the manager's main queue without blocking the caller. The task's result is delivered via a callback function once completed.",
      "signature": "QueueTaskWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
      "parameters": "task: func(context.Context, R) (E, error) (the function defining the task's logic); callback: func(E, error) (a function that will be invoked with the task's result and error)",
      "prerequisites": "The `TaskManager` must be in a running state. The task function should ideally be idempotent if retries are enabled and `CheckHealth` can deem a worker unhealthy.",
      "sideEffects": "Adds the task to the main queue. Increments `queuedTasks` and `totalTasksArrived` metrics. An available worker will eventually pick it up. The provided `callback` function is invoked asynchronously upon task completion or failure.",
      "returnValue": "None (returns immediately)",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [
          "pattern:Queue_with_callback"
        ],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskAsync": {
      "id": "method:QueueTaskAsync",
      "useCase": "To submit a standard asynchronous task to the manager's main queue and receive its result/error via channels, allowing for flexible non-blocking consumption of results.",
      "signature": "QueueTaskAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
      "parameters": "task: func(context.Context, R) (E, error) (the function defining the task's logic)",
      "prerequisites": "The `TaskManager` must be in a running state. The task function should ideally be idempotent if retries are enabled and `CheckHealth` can deem a worker unhealthy.",
      "sideEffects": "Adds the task to the main queue. Increments `queuedTasks` and `totalTasksArrived` metrics. An available worker will eventually pick it up. Returns immediately. The result and error are sent to the returned channels once the task completes.",
      "returnValue": "<-chan E (a receive-only channel for the task's result), <-chan error (a receive-only channel for any error encountered)",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [
          "pattern:Queue_asynchronously_with_channels"
        ],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "RunTask": {
      "id": "method:RunTask",
      "useCase": "To execute a task immediately, bypassing queues, typically for urgent or latency-sensitive operations. It tries to use a resource from a pre-allocated pool or creates a temporary one if the pool is empty.",
      "signature": "RunTask(task func(context.Context, R) (E, error)) (E, error)",
      "parameters": "task: func(context.Context, R) (E, error) (the function defining the task's logic. It receives a `context.Context` for cancellation and a resource `R`.)",
      "prerequisites": "The `TaskManager` must be in a running state. `ResourcePoolSize` should be configured if aiming to reuse resources.",
      "sideEffects": "Acquires a resource (from pool or temporary creation), executes the task, then returns the resource to the pool or destroys it. Increments `totalTasksArrived`, `totalTasksCompleted`, or `totalTasksFailed` metrics. Blocks the calling goroutine until the task finishes execution.",
      "returnValue": "E (the result of the task's execution), error (nil if successful, or an error if the task fails, is cancelled, or resource creation fails)",
      "exceptions": [
        "errors.New(\"task manager is shutting down\")",
        "fmt.Errorf(\"failed to create temporary resource: %w\", err)"
      ],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down",
          "error:failed to create temporary resource"
        ]
      }
    },
    "QueueTaskWithPriority": {
      "id": "method:QueueTaskWithPriority",
      "useCase": "To submit a high-priority asynchronous task to a dedicated priority queue and wait for its completion. These tasks are processed before standard tasks.",
      "signature": "QueueTaskWithPriority(task func(context.Context, R) (E, error)) (E, error)",
      "parameters": "task: func(context.Context, R) (E, error) (the function defining the task's logic. It receives a `context.Context` for cancellation and a resource `R`.)",
      "prerequisites": "The `TaskManager` must be in a running state. The task function should ideally be idempotent if retries are enabled and `CheckHealth` can deem a worker unhealthy.",
      "sideEffects": "Adds the task to the priority queue. Increments `priorityTasks` and `totalTasksArrived` metrics. An available worker will pick it up preferentially, execute it, and potentially retry it. Blocks the calling goroutine until the task finishes execution or manager shuts down.",
      "returnValue": "E (the result of the task's execution), error (nil if successful, or an error if the task fails, is cancelled, or manager shuts down)",
      "exceptions": [
        "errors.New(\"task manager is shutting down\")"
      ],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskWithPriorityWithCallback": {
      "id": "method:QueueTaskWithPriorityWithCallback",
      "useCase": "To submit a high-priority asynchronous task to the dedicated priority queue without blocking the caller. The task's result is delivered via a callback function once completed.",
      "signature": "QueueTaskWithPriorityWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
      "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the priority queue. Increments `priorityTasks` and `totalTasksArrived` metrics. Returns immediately. The `callback` is invoked asynchronously upon task completion or failure.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskWithPriorityAsync": {
      "id": "method:QueueTaskWithPriorityAsync",
      "useCase": "To submit a high-priority asynchronous task to the dedicated priority queue and receive its result/error via channels, allowing for flexible non-blocking consumption of results.",
      "signature": "QueueTaskWithPriorityAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
      "parameters": "task: func(context.Context, R) (E, error)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the priority queue. Increments `priorityTasks` and `totalTasksArrived` metrics. Returns immediately. The result and error are sent to the returned channels once the task completes.",
      "returnValue": "<-chan E (a receive-only channel for the task's result), <-chan error (a receive-only channel for any error encountered)",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskOnce": {
      "id": "method:QueueTaskOnce",
      "useCase": "To submit a standard asynchronous task that will *not* be retried by the manager if it fails and `CheckHealth` indicates an unhealthy state. Suitable for non-idempotent operations where \"at-most-once\" execution by the task manager is desired.",
      "signature": "QueueTaskOnce(task func(context.Context, R) (E, error)) (E, error)",
      "parameters": "task: func(context.Context, R) (E, error) (the task function)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the main queue. Task is configured with maximum retries from the start, effectively disabling automatic re-queueing by `tasker`'s internal retry mechanism if `CheckHealth` returns false. Blocks the calling goroutine until the task finishes or manager shuts down.",
      "returnValue": "E (the result of the task), error (nil if successful, or an error if the task fails or manager shuts down)",
      "exceptions": [
        "errors.New(\"task manager is shutting down\")"
      ],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskOnceWithCallback": {
      "id": "method:QueueTaskOnceWithCallback",
      "useCase": "To submit a standard asynchronous task with \"at-most-once\" semantics using a callback for non-blocking result delivery.",
      "signature": "QueueTaskOnceWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
      "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the main queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. `callback` is invoked asynchronously upon task completion or failure.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskOnceAsync": {
      "id": "method:QueueTaskOnceAsync",
      "useCase": "To submit a standard asynchronous task with \"at-most-once\" semantics and receive its result/error via channels for non-blocking consumption.",
      "signature": "QueueTaskOnceAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
      "parameters": "task: func(context.Context, R) (E, error)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the main queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. Result/error are sent to provided channels.",
      "returnValue": "<-chan E (result channel), <-chan error (error channel)",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskWithPriorityOnce": {
      "id": "method:QueueTaskWithPriorityOnce",
      "useCase": "To submit a high-priority asynchronous task that will *not* be retried by the manager if it fails and `CheckHealth` indicates an unhealthy state. Suitable for non-idempotent high-priority operations.",
      "signature": "QueueTaskWithPriorityOnce(task func(context.Context, R) (E, error)) (E, error)",
      "parameters": "task: func(context.Context, R) (E, error) (the task function)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the priority queue. Task is configured with maximum retries, effectively disabling automatic re-queueing by `tasker`'s internal retry mechanism if `CheckHealth` returns false. Blocks the calling goroutine until the task finishes or manager shuts down.",
      "returnValue": "E (the result of the task), error (nil if successful, or an error if the task fails or manager shuts down)",
      "exceptions": [
        "errors.New(\"task manager is shutting down\")"
      ],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskWithPriorityOnceWithCallback": {
      "id": "method:QueueTaskWithPriorityOnceWithCallback",
      "useCase": "To submit a high-priority asynchronous task with \"at-most-once\" semantics using a callback for non-blocking result delivery.",
      "signature": "QueueTaskWithPriorityOnceWithCallback(task func(context.Context, R) (E, error), callback func(E, error))",
      "parameters": "task: func(context.Context, R) (E, error); callback: func(E, error)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the priority queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. `callback` is invoked asynchronously upon task completion or failure.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "QueueTaskWithPriorityOnceAsync": {
      "id": "method:QueueTaskWithPriorityOnceAsync",
      "useCase": "To submit a high-priority asynchronous task with \"at-most-once\" semantics and receive its result/error via channels for non-blocking consumption.",
      "signature": "QueueTaskWithPriorityOnceAsync(task func(context.Context, R) (E, error)) (<-chan E, <-chan error)",
      "parameters": "task: func(context.Context, R) (E, error)",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Adds the task to the priority queue. Task is configured not to be re-queued if `CheckHealth` indicates unhealthy. Returns immediately. Result/error are sent to provided channels.",
      "returnValue": "<-chan E (result channel), <-chan error (error channel)",
      "exceptions": [],
      "availability": "async",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Task"
        ],
        "patterns": [],
        "errors": [
          "error:task manager is shutting down"
        ]
      }
    },
    "Stop": {
      "id": "method:Stop",
      "useCase": "To gracefully shut down the `TaskManager`. This involves stopping new task submissions, allowing existing queued tasks to complete, and then releasing all resources.",
      "signature": "Stop() error",
      "parameters": "None",
      "prerequisites": "The `TaskManager` must be in a running state.",
      "sideEffects": "Transitions manager state to 'stopping'. Cancels the main context (signaling workers to drain queues). Stops the burst manager. Waits for all worker goroutines to complete. Drains and destroys all resources in the pool. Prevents new tasks from being queued.",
      "returnValue": "error (nil on successful graceful shutdown; `errors.New(\"task manager already stopping or killed\")` if already in a shutdown state)",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [],
        "patterns": [
          "pattern:Graceful_shutdown"
        ],
        "errors": [
          "error:task manager already stopping or killed"
        ]
      }
    },
    "Kill": {
      "id": "method:Kill",
      "useCase": "To immediately terminate the `TaskManager` without waiting for queued tasks to complete. All running tasks are cancelled, and resources are released abruptly. Use for urgent shutdowns.",
      "signature": "Kill() error",
      "parameters": "None",
      "prerequisites": "The `TaskManager` must be in a running state or stopping state.",
      "sideEffects": "Transitions manager state to 'killed'. Cancels the main context (signaling workers to exit immediately). Stops the burst manager. Waits for all worker goroutines to terminate. Drains and destroys all resources. Drops any tasks still in queues. Prevents new tasks from being queued.",
      "returnValue": "error (nil on successful immediate shutdown; `errors.New(\"task manager already killed\")` if already in 'killed' state)",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [],
        "patterns": [
          "pattern:Immediate_shutdown"
        ],
        "errors": [
          "error:task manager already killed"
        ]
      }
    },
    "Stats": {
      "id": "method:Stats",
      "useCase": "To retrieve a real-time snapshot of the `TaskManager`'s current operational state, including worker counts and queue sizes.",
      "signature": "Stats() TaskStats",
      "parameters": "None",
      "prerequisites": "None.",
      "sideEffects": "None.",
      "returnValue": "TaskStats (a struct containing current worker counts, queued tasks, and available resources)",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:TaskStats"
        ],
        "patterns": [
          "pattern:Get_live_stats"
        ]
      }
    },
    "Metrics": {
      "id": "method:Metrics",
      "useCase": "To retrieve comprehensive performance metrics collected by the `TaskManager`, such as task rates, execution times, and success/failure rates.",
      "signature": "Metrics() TaskMetrics",
      "parameters": "None",
      "prerequisites": "None.",
      "sideEffects": "None.",
      "returnValue": "TaskMetrics (a struct containing aggregated performance metrics)",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:TaskMetrics"
        ],
        "patterns": [
          "pattern:Get_performance_metrics"
        ]
      }
    },
    "NewCollector": {
      "id": "method:NewCollector",
      "useCase": "To create and initialize a new default `MetricsCollector` instance. Typically used internally or when providing a default collector.",
      "signature": "NewCollector() MetricsCollector",
      "parameters": "None",
      "prerequisites": "None.",
      "sideEffects": "Initializes internal state for metric aggregation.",
      "returnValue": "MetricsCollector (a new instance of the default metrics collector)",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "types": [],
        "patterns": []
      }
    },
    "RecordArrival": {
      "id": "method:RecordArrival",
      "useCase": "Records that a new task has arrived in the system (queued). Used internally by `TaskManager`.",
      "signature": "RecordArrival()",
      "parameters": "None",
      "prerequisites": "None.",
      "sideEffects": "Increments the total tasks arrived counter.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "types": [],
        "patterns": []
      }
    },
    "RecordCompletion": {
      "id": "method:RecordCompletion",
      "useCase": "Records that a task has completed successfully. Used internally by `TaskManager`.",
      "signature": "RecordCompletion(stamps TaskLifecycleTimestamps)",
      "parameters": "stamps: TaskLifecycleTimestamps (timing information for the completed task)",
      "prerequisites": "None.",
      "sideEffects": "Increments total tasks completed, updates total/min/max/percentile execution times and total wait time.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "types": [
          "type:TaskLifecycleTimestamps"
        ],
        "patterns": []
      }
    },
    "RecordFailure": {
      "id": "method:RecordFailure",
      "useCase": "Records that a task has failed permanently (after retries). Used internally by `TaskManager`.",
      "signature": "RecordFailure(stamps TaskLifecycleTimestamps)",
      "parameters": "stamps: TaskLifecycleTimestamps (timing information for the failed task)",
      "prerequisites": "None.",
      "sideEffects": "Increments the total tasks failed counter.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "types": [
          "type:TaskLifecycleTimestamps"
        ],
        "patterns": []
      }
    },
    "RecordRetry": {
      "id": "method:RecordRetry",
      "useCase": "Records that a task has been re-queued for a retry. Used internally by `TaskManager`.",
      "signature": "RecordRetry()",
      "parameters": "None",
      "prerequisites": "None.",
      "sideEffects": "Increments the total tasks retried counter.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:MetricsCollector"
        ],
        "types": [],
        "patterns": []
      }
    },
    "Debugf": {
      "id": "method:Debugf",
      "useCase": "Logs a message at the debug level. Part of the `Logger` interface.",
      "signature": "Debugf(format string, args ...any)",
      "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
      "prerequisites": "None.",
      "sideEffects": "Emits a log message.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:Logger"
        ],
        "types": [],
        "patterns": []
      }
    },
    "Infof": {
      "id": "method:Infof",
      "useCase": "Logs a message at the info level. Part of the `Logger` interface.",
      "signature": "Infof(format string, args ...any)",
      "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
      "prerequisites": "None.",
      "sideEffects": "Emits a log message.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:Logger"
        ],
        "types": [],
        "patterns": []
      }
    },
    "Warnf": {
      "id": "method:Warnf",
      "useCase": "Logs a message at the warning level. Part of the `Logger` interface.",
      "signature": "Warnf(format string, args ...any)",
      "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
      "prerequisites": "None.",
      "sideEffects": "Emits a log message.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:Logger"
        ],
        "types": [],
        "patterns": []
      }
    },
    "Errorf": {
      "id": "method:Errorf",
      "useCase": "Logs a message at the error level. Part of the `Logger` interface.",
      "signature": "Errorf(format string, args ...any)",
      "parameters": "format: string (printf-style format string); args: ...any (arguments for format string)",
      "prerequisites": "None.",
      "sideEffects": "Emits a log message.",
      "returnValue": "None",
      "exceptions": [],
      "availability": "sync",
      "status": "active",
      "related": {
        "interfaces": [
          "interface:Logger"
        ],
        "types": [],
        "patterns": []
      }
    }
  },
  "decisionTrees": {
    "ChooseTaskSubmissionMethod": {
      "id": "decisionTree:ChooseTaskSubmissionMethod",
      "question": "How should I submit a task?",
      "logic": "IF result_needed_immediately_and_blocking_ok THEN IF resource_pool_is_sufficient THEN RunTask ELSE RunTask (will create temporary resource); ELSE IF high_priority_required THEN IF blocking_ok THEN QueueTaskWithPriority ELSE (IF callback_needed THEN QueueTaskWithPriorityWithCallback ELSE QueueTaskWithPriorityAsync); ELSE IF blocking_ok THEN QueueTask ELSE (IF callback_needed THEN QueueTaskWithCallback ELSE QueueTaskAsync);",
      "validationMethod": "Observe call blocking behavior and method used (e.g., `RunTask` for immediate execution logs).",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:QueueTask",
          "method:QueueTaskWithCallback",
          "method:QueueTaskAsync",
          "method:RunTask",
          "method:QueueTaskWithPriority",
          "method:QueueTaskWithPriorityWithCallback",
          "method:QueueTaskWithPriorityAsync"
        ],
        "patterns": []
      }
    },
    "HandleUnhealthyResources": {
      "id": "decisionTree:HandleUnhealthyResources",
      "question": "How should I configure health checks and retries for my tasks?",
      "logic": "IF task_failure_implies_unhealthy_worker_or_resource THEN IMPLEMENT CheckHealth TO RETURN false FOR_THAT_ERROR_TYPE; ELSE CheckHealth SHOULD_RETURN true_OR_BE_NIL; IF transient_unhealthy_failures_should_be_retried THEN SET MaxRetries > 0; ELSE MaxRetries = 0;",
      "validationMethod": "Simulate unhealthy errors and observe worker replacement and task re-queuing (or lack thereof).",
      "related": {
        "interfaces": [],
        "methods": [],
        "patterns": [
          "pattern:Custom_health_check"
        ],
        "errors": [
          "error:processor_crash"
        ]
      }
    },
    "GracefulVsImmediateShutdown": {
      "id": "decisionTree:GracefulVsImmediateShutdown",
      "question": "Should I perform a graceful or immediate shutdown?",
      "logic": "IF all_in_flight_and_queued_tasks_must_complete THEN CALL Stop(); ELSE IF immediate_termination_is_critical_regardless_of_lost_tasks THEN CALL Kill();",
      "validationMethod": "Observe if queued tasks complete after `Stop()` or if they are dropped after `Kill()`. Check `Stats().QueuedTasks` and `Stats().ActiveWorkers` post-shutdown.",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:Stop",
          "method:Kill"
        ],
        "patterns": [
          "pattern:Graceful_shutdown",
          "pattern:Immediate_shutdown"
        ]
      }
    }
  },
  "patterns": {
    "Basic_tasker_setup": {
      "id": "pattern:Basic_tasker_setup",
      "description": "Demonstrates the fundamental setup for `tasker` using a simple `CalculatorResource` with two base workers.",
      "example": {
        "code": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\n// CalculatorResource represents a simple resource,\n// in this case, just a placeholder.\ntype CalculatorResource struct{}\n\n// onCreate for CalculatorResource - no actual setup needed\nfunc createCalcResource() (*CalculatorResource, error) {\n\tfmt.Println(\"INFO: Creating CalculatorResource\")\n\treturn &CalculatorResource{}, nil\n}\n\n// onDestroy for CalculatorResource - no actual cleanup needed\nfunc destroyCalcResource(r *CalculatorResource) error {\n\tfmt.Println(\"INFO: Destroying CalculatorResource\")\n\treturn nil\n}\n\nfunc main() {\n\tfmt.Println(\"--- Basic Usage: Simple Calculator ---\")\n\n\tctx := context.Background()\n\n\t// Configure the tasker for our CalculatorResource\n\tconfig := tasker.Config[*CalculatorResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 2, // Two base workers\n\t\tCtx:         ctx,\n\t\t// No specific health check or burst settings for this basic example\n\t}\n\n\t// Create a new task manager\n\tmanager, err := tasker.NewTaskManager[*CalculatorResource, int](config) // Tasks will return an int result\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop() // Ensure the manager is stopped gracefully\n\n\tfmt.Println(\"Queuing a simple Multiplication task...\")\n\ttask1Start := time.Now()\n\t// Queue a task to perform addition\n\tgo func() {\n\t\tsum, err := manager.QueueTask(func(ctx context.Context, r *CalculatorResource) (int, error) {\n\t\t\t// In a real scenario, 'r' could be a connection to a math service\n\t\t\ttime.Sleep(50 * time.Millisecond) // Simulate some work\n\t\t\ta, b := 10, 25\n\t\t\tfmt.Printf(\"Worker processing: %d * %d\\n\", a, b)\n\t\t\treturn a * b, nil\n\t\t})\n\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Task 1 failed: %v\\n\", err)\n\t\t} else {\n\t\t\tfmt.Printf(\"Task 1 (Multiplication) Result: %d (took %s)\\n\", sum, time.Since(task1Start))\n\t\t}\n\t}()\n\n\tfmt.Println(\"Queuing another addition task...\")\n\ttask2Start := time.Now()\n\tmanager.QueueTaskWithCallback(\n\t\tfunc(ctx context.Context, r *CalculatorResource) (int, error) {\n\t\t\ttime.Sleep(50 * time.Millisecond) // Simulate some work\n\t\t\ta, b := 10, 25\n\t\t\tfmt.Printf(\"Worker processing: %d + %d\\n\", a, b)\n\t\t\treturn a + b, nil\n\t\t},\n\t\tfunc(sum int, err error) { // do something with the results\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Task 2 failed: %v\\n\", err)\n\t\t\t} else {\n\t\t\t\tfmt.Printf(\"Task 2 (Addition) Result: %d (took %s)\\n\", sum, time.Since(task2Start))\n\t\t\t}\n\t\t},\n\t)\n\n\tfmt.Println(\"Queuing another subtraction task...\")\n\n\ttask3Start := time.Now()\n\tdifferencech, errch := manager.QueueTaskAsync(func(ctx context.Context, r *CalculatorResource) (int, error) {\n\t\ttime.Sleep(70 * time.Millisecond) // Simulate some work\n\t\ta, b := 100, 40\n\t\tfmt.Printf(\"Worker processing: %d - %d\\n\", a, b)\n\t\treturn a - b, nil\n\t})\n\n\tdifference := <-differencech\n\terr = <-errch\n\n\tif err != nil {\n\t\tfmt.Printf(\"Task 3 failed: %v\\n\", err)\n\t} else {\n\t\tfmt.Printf(\"Task 3 (Subtraction) Result: %d (took %s)\\n\", difference, time.Since(task3Start))\n\t}\n\n\t// Allow some time for tasks to complete\n\ttime.Sleep(500 * time.Millisecond)\n\n\tstats := manager.Stats()\n\tfmt.Printf(\"\\n--- Current Stats ---\\n\")\n\tfmt.Printf(\"Active Workers: %d\\n\", stats.ActiveWorkers)\n\tfmt.Printf(\"Queued Tasks: %d\\n\", stats.QueuedTasks)\n\tfmt.Printf(\"Available Resources: %d\\n\", stats.AvailableResources)\n\tfmt.Println(\"----------------------\")\n\n\tfmt.Println(\"Basic usage example finished.\")\n}\n",
        "validation": "Output log shows 'Creating CalculatorResource' twice, tasks being processed, and correct calculation results. Final stats show 2 active workers and 0 queued tasks."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:NewTaskManager",
          "method:QueueTask",
          "method:QueueTaskWithCallback",
          "method:QueueTaskAsync",
          "method:Stats",
          "method:Stop"
        ],
        "errors": []
      }
    },
    "Custom_health_check": {
      "id": "pattern:Custom_health_check",
      "description": "Demonstrates defining a custom health check function to identify unhealthy worker/resource states and trigger worker replacement and task retries.",
      "example": {
        "code": "package main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\n// ImageProcessor represents a resource for image manipulation.\ntype ImageProcessor struct {\n\tID        int\n\tIsHealthy bool\n}\n\n// onCreate for ImageProcessor: simulates creating a connection to an image processing service.\nfunc createImageProcessor() (*ImageProcessor, error) {\n\tid := rand.Intn(1000)\n\tfmt.Printf(\"INFO: Creating ImageProcessor %d\\n\", id)\n\treturn &ImageProcessor{ID: id, IsHealthy: true}, nil\n}\n\n// onDestroy for ImageProcessor: simulates closing the connection.\nfunc destroyImageProcessor(p *ImageProcessor) error {\n\tfmt.Printf(\"INFO: Destroying ImageProcessor %d\\n\", p.ID)\n\treturn nil\n}\n\n// checkImageProcessorHealth: Custom health check.\n// If the error is \"processor_crash\", consider the worker/resource unhealthy.\nfunc checkImageProcessorHealth(err error) bool {\n\tif err != nil && err.Error() == \"processor_crash\" {\n\t\tfmt.Printf(\"WARN: Detected unhealthy error: %v. Worker will be replaced.\\n\", err)\n\t\treturn false // This error indicates an unhealthy state\n\t}\n\treturn true // Other errors are just task failures, not worker health issues\n}\n\nfunc main() {\n\tfmt.Println(\"\\n--- Intermediate Usage: Image Processing ---\")\n\n\tctx := context.Background()\n\n\tconfig := tasker.Config[*ImageProcessor]{\n\t\tOnCreate:         createImageProcessor,\n\t\tOnDestroy:        destroyImageProcessor,\n\t\tWorkerCount:      2,\n\t\tCtx:              ctx,\n\t\tCheckHealth:      checkImageProcessorHealth, // Use custom health check\n\t\tMaxRetries:       1,\n\t\tResourcePoolSize: 1,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*ImageProcessor, string](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\tfmt.Println(\"Queueing a task that might crash a worker (unhealthy error)...\")\n\tgo func() {\n\t\tresult, err := manager.QueueTask(func(ctx context.Context, proc *ImageProcessor) (string, error) {\n\t\t\tfmt.Printf(\"Worker %d processing problematic image (might crash)\\n\", proc.ID)\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\tif rand.Intn(2) == 0 {\n\t\t\t\treturn \"\", errors.New(\"processor_crash\") // This triggers CheckHealth to return false\n\t\t\t}\n\t\t\treturn \"problematic_image_processed.jpg\", nil\n\t\t})\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Problematic Image Task Failed: %v\\n\", err)\n\t\t} else {\n\t\t\tfmt.Printf(\"Problematic Image Task Completed: %s\\n\", result)\n\t\t}\n\t}()\n\n\ttime.Sleep(500 * time.Millisecond)\n}\n",
        "validation": "When a task returns \"processor_crash\" error, the log should show 'WARN: Detected unhealthy error: processor_crash. Worker will be replaced.' followed by 'INFO: Destroying ImageProcessor [ID]' and 'INFO: Creating ImageProcessor [New ID]', indicating worker replacement. The task may be retried or fail with 'max retries exceeded'."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:QueueTask"
        ],
        "errors": [
          "error:processor_crash",
          "error:max retries exceeded"
        ]
      }
    },
    "Graceful_shutdown": {
      "id": "pattern:Graceful_shutdown",
      "description": "Demonstrates initiating a graceful shutdown of the `TaskManager` using `Stop()`, ensuring all pending tasks are completed before resources are released.",
      "example": {
        "code": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\ntype HeavyComputeResource struct{ ID int }\nfunc createComputeResource() (*HeavyComputeResource, error) { fmt.Printf(\"INFO: Creating ComputeResource\\n\"); return &HeavyComputeResource{ID: 1}, nil }\nfunc destroyComputeResource(r *HeavyComputeResource) error { fmt.Printf(\"INFO: Destroying ComputeResource\\n\"); return nil }\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tconfig := tasker.Config[*HeavyComputeResource]{\n\t\tOnCreate:    createComputeResource,\n\t\tOnDestroy:   destroyComputeResource,\n\t\tWorkerCount: 2,\n\t\tCtx:         ctx,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*HeavyComputeResource, string](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\n\t// Queue some tasks\n\tfor i := 0; i < 5; i++ {\n\t\ttaskID := i\n\t\tgo func() {\n\t\t\t_, _ = manager.QueueTask(func(ctx context.Context, res *HeavyComputeResource) (string, error) {\n\t\t\t\tfmt.Printf(\"Worker %d processing Task %d\\n\", res.ID, taskID)\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t\treturn fmt.Sprintf(\"Task %d completed\", taskID), nil\n\t\t\t})\n\t\t}()\n\t}\n\n\t// Allow some tasks to start, then initiate graceful shutdown\n\ttime.Sleep(200 * time.Millisecond)\n\tfmt.Println(\"\\nInitiating graceful shutdown...\")\n\terr = manager.Stop()\n\tif err != nil {\n\t\tfmt.Printf(\"Error during graceful shutdown: %v\\n\", err)\n\t} else {\n\t\tfmt.Println(\"Task manager gracefully shut down.\")\n\t}\n\tfmt.Printf(\"Final Active Workers: %d\\n\", manager.Stats().ActiveWorkers)\n}\n",
        "validation": "All 5 tasks submitted are eventually reported as 'completed'. Logs indicate workers are destroyed only after tasks finish. Final 'Active Workers' should be 0. `Task manager gracefully shut down.` should be printed."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:Stop"
        ],
        "errors": [
          "error:task manager already stopping or killed"
        ]
      }
    },
    "Immediate_shutdown": {
      "id": "pattern:Immediate_shutdown",
      "description": "Demonstrates initiating an immediate shutdown of the `TaskManager` using `Kill()`, which cancels active tasks and drops queued ones without waiting.",
      "example": {
        "code": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\ntype HeavyComputeResource struct{ ID int }\nfunc createComputeResource() (*HeavyComputeResource, error) { fmt.Printf(\"INFO: Creating ComputeResource\\n\"); return &HeavyComputeResource{ID: 1}, nil }\nfunc destroyComputeResource(r *HeavyComputeResource) error { fmt.Printf(\"INFO: Destroying ComputeResource\\n\"); return nil }\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tconfig := tasker.Config[*HeavyComputeResource]{\n\t\tOnCreate:    createComputeResource,\n\t\tOnDestroy:   destroyComputeResource,\n\t\tWorkerCount: 2,\n\t\tCtx:         ctx,\n\t}\n\n\tmanager, err := tasker.NewTaskManager[*HeavyComputeResource, string](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating task manager: %v\", err)\n\t}\n\n\t// Queue a long-running task\n\tgo func() {\n\t\t_, _ = manager.QueueTask(func(ctx context.Context, res *HeavyComputeResource) (string, error) {\n\t\t\tfmt.Printf(\"Worker %d processing long Task...\\n\", res.ID)\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tfmt.Printf(\"Task on Worker %d cancelled due to shutdown.\\n\", res.ID)\n\t\t\t\treturn \"\", ctx.Err()\n\t\t\tcase <-time.After(5 * time.Second): // Simulate long work\n\t\t\t\treturn \"long task completed\", nil\n\t\t\t}\n\t\t})\n\t}()\n\n\t// Immediately kill the manager\n\ttime.Sleep(50 * time.Millisecond)\n\tfmt.Println(\"\\nInitiating immediate shutdown (Kill)...\")\n\terr = manager.Kill()\n\tif err != nil {\n\t\tfmt.Printf(\"Error during immediate shutdown: %v\\n\", err)\n\t} else {\n\t\tfmt.Println(\"Task manager immediately shut down.\")\n\t}\n\tfmt.Printf(\"Final Active Workers: %d\\n\", manager.Stats().ActiveWorkers)\n}\n",
        "validation": "The long-running task should be reported as 'cancelled'. Logs should show workers being destroyed immediately. Final 'Active Workers' should be 0. `Task manager immediately shut down.` should be printed."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:Kill"
        ],
        "errors": [
          "error:task manager already killed"
        ]
      }
    },
    "Get_live_stats": {
      "id": "pattern:Get_live_stats",
      "description": "Demonstrates how to retrieve real-time operational statistics of the `TaskManager`.",
      "example": {
        "code": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\ntype CalcResource struct{}\nfunc createCalcResource() (*CalcResource, error) { return &CalcResource{}, nil }\nfunc destroyCalcResource(r *CalcResource) error { return nil }\n\nfunc main() {\n\tctx := context.Background()\n\tconfig := tasker.Config[*CalcResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 1,\n\t\tCtx:         ctx,\n\t}\n\tmanager, err := tasker.NewTaskManager[*CalcResource, int](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\t// Queue a task to keep a worker busy\n\tgo func() {\n\t\t_, _ = manager.QueueTask(func(ctx context.Context, r *CalcResource) (int, error) {\n\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\treturn 0, nil\n\t\t})\n\t}()\n\n\ttime.Sleep(50 * time.Millisecond) // Allow worker to pick up task\n\tstats := manager.Stats()\n\tfmt.Printf(\"Current Stats: Active Workers: %d, Queued Tasks: %d, Available Resources: %d\\n\",\n\t\tstats.ActiveWorkers, stats.QueuedTasks, stats.AvailableResources)\n\n\ttime.Sleep(200 * time.Millisecond) // Wait for task to complete\n\tstats = manager.Stats()\n\tfmt.Printf(\"Stats after task completion: Active Workers: %d, Queued Tasks: %d, Available Resources: %d\\n\",\n\t\tstats.ActiveWorkers, stats.QueuedTasks, stats.AvailableResources)\n}\n",
        "validation": "Output shows 'Active Workers: 1' and 'Queued Tasks: 0' initially (if task immediately picked up), and then 'Active Workers: 1' and 'Queued Tasks: 0' after task completion. The 'Available Resources' should reflect pool state."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:Stats"
        ],
        "errors": []
      }
    },
    "Get_performance_metrics": {
      "id": "pattern:Get_performance_metrics",
      "description": "Demonstrates how to retrieve comprehensive performance metrics from the `TaskManager`.",
      "example": {
        "code": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"github.com/asaidimu/tasker/v2\"\n)\n\ntype CalcResource struct{}\nfunc createCalcResource() (*CalcResource, error) { return &CalcResource{}, nil }\nfunc destroyCalcResource(r *CalcResource) error { return nil }\n\nfunc main() {\n\tctx := context.Background()\n\tconfig := tasker.Config[*CalcResource]{\n\t\tOnCreate:    createCalcResource,\n\t\tOnDestroy:   destroyCalcResource,\n\t\tWorkerCount: 1,\n\t\tCtx:         ctx,\n\t}\n\tmanager, err := tasker.NewTaskManager[*CalcResource, int](config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error creating manager: %v\", err)\n\t}\n\tdefer manager.Stop()\n\n\t// Queue multiple tasks to generate metrics data\n\tfor i := 0; i < 10; i++ {\n\t\tgo func() {\n\t\t\t_, _ = manager.QueueTask(func(ctx context.Context, r *CalcResource) (int, error) {\n\t\t\t\ttime.Sleep(time.Duration(10 + i*5) * time.Millisecond) // Varying work\n\t\t\t\treturn 0, nil\n\t\t\t})\n\t\t}()\n\t}\n\n\ttime.Sleep(500 * time.Millisecond) // Allow tasks to process\n\n\tmetrics := manager.Metrics()\n\tfmt.Printf(\"\\n--- Performance Metrics ---\\n\")\n\tfmt.Printf(\"Total Tasks Completed: %d\\n\", metrics.TotalTasksCompleted)\n\tfmt.Printf(\"Average Execution Time: %v\\n\", metrics.AverageExecutionTime)\n\tfmt.Printf(\"P95 Execution Time: %v\\n\", metrics.P95ExecutionTime)\n\tfmt.Printf(\"Task Completion Rate: %.2f/sec\\n\", metrics.TaskCompletionRate)\n\tfmt.Printf(\"Success Rate: %.2f\\n\", metrics.SuccessRate)\n\tfmt.Println(\"-----------------------------\")\n}\n",
        "validation": "Output shows various metrics like `Total Tasks Completed`, `Average Execution Time`, `P95 Execution Time`, `Task Completion Rate`, and `Success Rate`. Values should be non-zero after tasks have completed."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:Metrics"
        ],
        "errors": []
      }
    },
    "Queue_and_wait": {
      "id": "pattern:Queue_and_wait",
      "description": "Template for queuing a task to the main queue and blocking the caller until its result is available.",
      "example": {
        "code": "result, err := manager.QueueTask(func(ctx context.Context, res *MyResource) (string, error) {\n    // Task logic here\n    return \"task_done\", nil\n})\nif err != nil {\n    fmt.Printf(\"Task failed: %v\\n\", err)\n} else {\n    fmt.Printf(\"Task completed with result: %s\\n\", result)\n}\n",
        "validation": "The `result` variable holds the correct task output, and `err` is nil if the task was successful. The `fmt.Printf` will execute after the task is truly finished."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:QueueTask"
        ],
        "errors": []
      }
    },
    "Queue_with_callback": {
      "id": "pattern:Queue_with_callback",
      "description": "Template for queuing a task to the main queue and receiving its result asynchronously via a callback function.",
      "example": {
        "code": "manager.QueueTaskWithCallback(func(ctx context.Context, res *MyResource) (string, error) {\n    // Task logic here\n    return \"callback_task_done\", nil\n}, func(result string, err error) {\n    if err != nil {\n        fmt.Printf(\"Callback task failed: %v\\n\", err)\n    } else {\n        fmt.Printf(\"Callback task completed with result: %s\\n\", result)\n    }\n})\nfmt.Println(\"Submitted callback task, not blocking.\")\n",
        "validation": "The line 'Submitted callback task, not blocking.' prints immediately. The callback function prints its result message later, once the task is processed."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:QueueTaskWithCallback"
        ],
        "errors": []
      }
    },
    "Queue_asynchronously_with_channels": {
      "id": "pattern:Queue_asynchronously_with_channels",
      "description": "Template for queuing a task to the main queue and receiving its result and error via Go channels, allowing non-blocking submission and flexible result consumption.",
      "example": {
        "code": "resultChan, errChan := manager.QueueTaskAsync(func(ctx context.Context, res *MyResource) (string, error) {\n    // Task logic here\n    return \"channel_task_done\", nil\n})\nfmt.Println(\"Submitted async channel task, not blocking.\")\n\ngo func() {\n    result := <-resultChan\n    err := <-errChan\n    if err != nil {\n        fmt.Printf(\"Async channel task failed: %v\\n\", err)\n    } else {\n        fmt.Printf(\"Async channel task completed with result: %s\\n\", result)\n    }\n}()\n",
        "validation": "The line 'Submitted async channel task, not blocking.' prints immediately. The result/error message from the goroutine receiving from channels prints later, once the task is processed."
      },
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:QueueTaskAsync"
        ],
        "errors": []
      }
    }
  },
  "errors": {
    "task manager is shutting down": {
      "id": "error:task manager is shutting down",
      "type": "Go `error` (specific string match)",
      "symptoms": "Calling `QueueTask`, `RunTask`, or other submission methods returns an error with the message \"task manager is shutting down\" or \"task manager is shutting down (context done)\".",
      "properties": "None (standard `error` interface, typically `errors.New` or wrapped by `fmt.Errorf`)",
      "scenarios": [
        {
          "trigger": "Attempting to submit a new task after `manager.Stop()` or `manager.Kill()` has been called.",
          "example": "```go\nmanager.Stop() // Initiates shutdown\n_, err := manager.QueueTask(...) // This will likely return 'task manager is shutting down'\n```",
          "reason": "The TaskManager has transitioned to a non-running state (stopping or killed) and no longer accepts new tasks."
        },
        {
          "trigger": "Attempting to submit a new task after the context provided to `NewTaskManager` (`config.Ctx`) has been cancelled externally.",
          "example": "```go\nctx, cancel := context.WithCancel(context.Background())\nconfig := tasker.Config{Ctx: ctx, ...}\nmanager := tasker.NewTaskManager(config)\ncancel() // Cancels the main context\n_, err := manager.QueueTask(...) // This will likely return 'task manager is shutting down (context done)'\n```",
          "reason": "The primary control context for the manager has been cancelled, signaling it to begin shutdown."
        }
      ],
      "diagnosis": "Check application lifecycle management. Verify if `Stop()` or `Kill()` are being called prematurely. Ensure task submissions are coordinated with the manager's active state.",
      "resolution": "Only submit tasks when the `TaskManager` is in a running state. Add checks around submission calls (e.g., `if manager.isRunning() { ... }`). Ensure graceful shutdown routines are triggered only when no more new tasks are expected.",
      "prevention": "Implement robust application lifecycle hooks. Use `defer manager.Stop()` in `main` or main goroutine. Integrate `manager.Stop()` or `manager.Kill()` with OS signals (e.g., `SIGINT`, `SIGTERM`).",
      "handlingPatterns": "`if errors.Is(err, errors.New(\"task manager is shutting down\"))` (or contains substring) then discard/log task rather than retrying.",
      "propagationBehavior": "This error is returned directly to the caller of submission methods (`QueueTask`, `RunTask`, etc.) and propagated through result/error channels or callbacks for async methods.",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [],
        "patterns": [],
        "errors": []
      }
    },
    "processor_crash": {
      "id": "error:processor_crash",
      "type": "Go `error` (example custom error string)",
      "symptoms": "A task function explicitly returns an error indicating an underlying resource or worker failure (e.g., a connection drop, service outage). Logs may show `WARN: Detected unhealthy error: processor_crash. Worker will be replaced.`",
      "properties": "None (typically a simple `errors.New` or custom error type from the application's domain)",
      "scenarios": [
        {
          "trigger": "A task encounters a critical, unrecoverable error during its execution that indicates the worker's associated resource is faulty.",
          "example": "```go\nfunc myProblematicTask(ctx context.Context, proc *ImageProcessor) (string, error) {\n    if rand.Intn(2) == 0 { // 50% chance to simulate a crash\n        return \"\", errors.New(\"processor_crash\") // This triggers CheckHealth to return false\n    }\n    return \"processed\", nil\n}\n// In Config: CheckHealth: func(err error) bool { return err.Error() != \"processor_crash\" }\n```",
          "reason": "The `CheckHealth` function (provided in `tasker.Config`) detected this specific error string and returned `false`, signaling `tasker` that the worker or its resource is unhealthy."
        }
      ],
      "diagnosis": "Review the `CheckHealth` implementation in `tasker.Config`. Analyze task logs for the specific error returned by the task function. Check the health of external services that the resource (`R`) interacts with.",
      "resolution": "If the error genuinely means the resource is unusable, `tasker`'s retry mechanism (if `MaxRetries > 0`) will attempt to re-process the task on a new worker. Ensure `OnCreate` for the resource is robust to create new, healthy instances. If the external service is down, that requires external intervention.",
      "prevention": "Implement robust error handling within your task functions. Use circuit breakers or exponential backoff for external service calls. Monitor resource health externally to proactively replace faulty instances.",
      "handlingPatterns": "`tasker` automatically handles this by replacing the worker and potentially retrying the task. For the task caller, it's treated as a normal task failure, but with the possibility of being retried internally.",
      "propagationBehavior": "This error is returned by the task function. It is then passed to the `CheckHealth` function. If `CheckHealth` returns `false`, `tasker` handles the worker replacement and task retry internally. Eventually, if retries are exhausted, the error is returned to the original task submitter.",
      "related": {
        "interfaces": [],
        "interface": [
          "interface:TaskManager"
        ],
        "methods": [
          "method:QueueTask"
        ],
        "patterns": [
          "pattern:Custom_health_check"
        ],
        "errors": []
      }
    },
    "max retries exceeded": {
      "id": "error:max retries exceeded",
      "type": "Go `error` (specific string match, often wrapped)",
      "symptoms": "A task that previously triggered an unhealthy worker condition (i.e., `CheckHealth` returned `false`) is reported as failed with a message indicating retry exhaustion.",
      "properties": "None (standard `error` interface)",
      "scenarios": [
        {
          "trigger": "A task repeatedly fails with an error that `CheckHealth` identifies as unhealthy, and the number of retry attempts reaches `MaxRetries`.",
          "example": "```go\n// Config with MaxRetries: 1\n// task returns errors.New(\"processor_crash\") (unhealthy)\n// first attempt fails, worker replaced, task re-queued (retry 1/1)\n// second attempt fails with \"processor_crash\"\n// -> Result: Task Failed: max retries exceeded: computation_error_task_X\n```",
          "reason": "The task manager exhausted all allowed retries for a task that consistently caused unhealthy worker conditions or was placed on an unhealthy worker, and could not complete successfully."
        }
      ],
      "diagnosis": "This error indicates a persistent problem with the task or the resources it's trying to use. Examine the underlying error that caused `CheckHealth` to return `false` repeatedly.",
      "resolution": "Investigate the root cause of the persistent unhealthy condition. This might involve debugging the task logic, checking external service health, or reviewing resource creation/destruction (`OnCreate`/`OnDestroy`). Consider increasing `MaxRetries` if the errors are truly transient but require more attempts.",
      "prevention": "Improve task robustness to handle transient errors internally without relying solely on `tasker`'s retry. Ensure external dependencies are stable. Set `MaxRetries` appropriately for the expected transience of errors.",
      "handlingPatterns": "Catch this error to log it as a critical task failure. Potentially escalate to an alert if many tasks hit this state. These tasks cannot be recovered by `tasker`'s internal mechanism.",
      "propagationBehavior": "This error is returned directly to the caller of the submission methods (`QueueTask`, `QueueTaskWithPriority`, etc.) or delivered via callbacks/channels after all retries are exhausted.",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [],
        "patterns": [],
        "errors": [
          "error:processor_crash"
        ]
      }
    },
    "failed to create temporary resource": {
      "id": "error:failed to create temporary resource",
      "type": "Go `error` (specific string match, wrapped with `%w`)",
      "symptoms": "A call to `RunTask` returns an error with this message, often wrapping another underlying error from `OnCreate`.",
      "properties": "None (standard `error` interface, contains underlying error via wrapping)",
      "scenarios": [
        {
          "trigger": "When `RunTask` is called and the `resourcePool` is empty (or full, causing it to bypass the pool), it attempts to create a new, temporary resource via `OnCreate`, but `OnCreate` returns an error.",
          "example": "```go\n// In Config: OnCreate: func() (*BadResource, error) { return nil, errors.New(\"connection_refused\") }\n_, err := manager.RunTask(...) // Will return 'failed to create temporary resource: connection_refused'\n```",
          "reason": "The `OnCreate` function, responsible for providing a resource to the task, failed during a `RunTask` call, preventing the task from executing."
        }
      ],
      "diagnosis": "Inspect the wrapped error within the returned `failed to create temporary resource` error. This wrapped error (`errors.Unwrap(err)`) will contain the specific reason `OnCreate` failed.",
      "resolution": "Debug the `OnCreate` function. Common causes include incorrect connection strings, unavailable external services, insufficient permissions, or resource exhaustion. Ensure `OnCreate` is robust and handles its own potential errors gracefully.",
      "prevention": "Pre-flight checks for external dependencies. Robust error handling within `OnCreate`. Consider implementing retries within `OnCreate` itself for transient resource creation issues.",
      "handlingPatterns": "Log the error, especially unwrapping the root cause. This typically indicates an environment or configuration issue that needs attention.",
      "propagationBehavior": "This error is returned directly to the caller of `RunTask`.",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [
          "type:Config"
        ],
        "patterns": [],
        "errors": []
      }
    },
    "priority queue full, task requeue failed": {
      "id": "error:priority queue full, task requeue failed",
      "type": "Go `error` (specific string match, wrapped with `%w`)",
      "symptoms": "A task that was intended to be re-queued (due to `CheckHealth` returning `false` and retries remaining) fails with this error, indicating it could not be re-added to the priority queue.",
      "properties": "None (standard `error` interface, contains underlying error via wrapping)",
      "scenarios": [
        {
          "trigger": "A task fails, `CheckHealth` indicates an unhealthy worker, retries are allowed, and `tasker` attempts to re-queue it to the priority queue, but the priority queue's buffer is full.",
          "example": "```go\n// Config: MaxRetries: 1, PriorityQueue buffer is small\n// Task fails with unhealthy error, manager tries to re-queue\n// Priority queue is full at that exact moment\n// -> Result: Task Failed: priority queue full, task requeue failed: [original_error]\n```",
          "reason": "The internal priority queue did not have capacity to accept a task that needed to be re-queued after an unhealthy worker condition. This usually happens under extreme load or if the priority queue's buffer size is too small relative to `MaxRetries` and task failure rate."
        }
      ],
      "diagnosis": "Check the `priorityQueue` buffer size (not directly configurable by user, but tied to `WorkerCount` in default implementation). Review overall system load and task failure rates. This is an internal `tasker` queue, so it usually points to overloaded conditions.",
      "resolution": "This indicates that even the priority retry mechanism is overloaded. Consider increasing `WorkerCount` or `MaxWorkerCount` to increase processing capacity, or reduce the rate of unhealthy errors. The task is permanently failed from `tasker`'s perspective.",
      "prevention": "Ensure adequate worker capacity (`WorkerCount`, `MaxWorkerCount`). Minimize unhealthy worker conditions through robust resources. For very high load, consider custom `MetricsCollector` to monitor queue depths and pre-scale.",
      "handlingPatterns": "Log this error. This task is definitively lost from `tasker`'s management. Manual intervention or external retry logic may be needed.",
      "propagationBehavior": "This error is returned to the original caller of the submission methods or delivered via callbacks/channels.",
      "related": {
        "interfaces": [
          "interface:TaskManager"
        ],
        "types": [],
        "patterns": [],
        "errors": [
          "error:max retries exceeded"
        ]
      }
    }
  }
}